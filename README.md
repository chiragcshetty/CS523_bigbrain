# Big Brain: Efficient Cluster Management for Deep Learning Jobs

With the rising popularity of large scale Deep Learning (DL) workload, GPU clusters have become a necessity. Yet, the tools to manage them are far from ideal. On one hand the deep learning engineers are burdened with task of estimating resource management usage of their models and routinely face Out-of-memory (OOM) errors. On the other hand, typ- ical GPU cluster utilization are low, often below 30%. But with recent developments in GPU virtualization and drawing from our learnings building Beachi - a fast model splitting system - we explore ways to improve GPU cluster manage- ment for DL. Model Parallelism (MP) techniques have so far been seen as a need, when the models are too big. However, we (plan to) demonstrate MP as an essential tool in building efficient DL workflows.




Detailed discussions can be found here: https://www.notion.so/CS523-Big-Brain-Model-Parallelism-over-distributed-shared-infrastructure-c9c71de6640847aa8a81e4f78910efcd


