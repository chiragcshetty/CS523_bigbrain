{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.pytorch_modified_inception as pytorch_modified_inception\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import simple_model as sm\n",
    "import dummyModels as dm\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For profiler #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "#########################################\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "## Defined in this round about way (instead of just directly assigning) to keep it compatibble with summarize.py\n",
    "class Args:\n",
    "     def __init__(self,itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch):\n",
    "         self.type = itype\n",
    "         self.prof_rounds = prof_rounds\n",
    "         self.prof_gpu_id = prof_gpu_id\n",
    "         self.batch_size = batch_size\n",
    "         self.gpu_num = gpu_num\n",
    "         self.sch = sch\n",
    "            \n",
    "itype       = 'forward'  # help: forward/all -> Conside forward path only or both\n",
    "prof_rounds = 20     # help: 'rounds for profiler'\n",
    "prof_gpu_id = 0      # help: 'which gpu to place the profiler'\n",
    "batch_size  = '128'   # help: 'batch_size'\n",
    "gpu_num     = 4      # help: 'number of gpu to use'\n",
    "sch         = 'sct'  # help: 'sct/etf/topo'\n",
    "\n",
    "args = Args(itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch)\n",
    "\n",
    "#############################################################################################\n",
    "\"\"\"\n",
    "    Function: placer_lib.create_device_graph\n",
    "    -> Creates a graph with devices as nodes and unit weight edges between them\n",
    "    -> Each node: graph.add_node(device_id,\n",
    "                                 id=device_id,\n",
    "                                 name=device_info[\"name\"],\n",
    "                                 size=0,\n",
    "                                 memory_limit=device_info[\"memory_size\"])\n",
    "\"\"\"\n",
    "DEVICE_GRAPH_SINGLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 17179869184, 'type': ''}})\n",
    "DEVICE_GRAPH_MULTIPLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             1: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:1', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             2: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:2', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             3: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:3', 'memory_size': 8000000000, 'type': ''}})\n",
    "\n",
    "#############################################################################################\n",
    "\"\"\"\n",
    "    we are going to use streams to allow parallel processing\n",
    "\"\"\"\n",
    "COMPUTE0 = torch.cuda.Stream(device=0)\n",
    "COMPUTE1 = torch.cuda.Stream(device=1)\n",
    "COMPUTE2 = torch.cuda.Stream(device=2)\n",
    "COMPUTE3 = torch.cuda.Stream(device=3)\n",
    "COMPUTE_STREAM = {0:COMPUTE0,1:COMPUTE1,2:COMPUTE2,3:COMPUTE3}\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")\n",
    "\n",
    "# print memory of given GPU. ex: gpu_no = 0\n",
    "def print_mem(gpu_id, cached=2, unit='GB'):\n",
    "    if unit=='GB':\n",
    "        mem_allocated = round(torch.cuda.memory_allocated(gpu_id)/1024**3,8)\n",
    "        mem_cached    = round(torch.cuda.memory_reserved(gpu_id)/1024**3,8)\n",
    "    else:\n",
    "        mem_allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "        mem_cached    = torch.cuda.memory_reserved(gpu_id)\n",
    "        \n",
    "    if cached>0:\n",
    "        print('Allocated:', mem_allocated , 'GB')\n",
    "    if cached>1:\n",
    "        print('Cached:   ', mem_cached    , 'GB')\n",
    "    return mem_allocated, mem_cached\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "#### Estimate size of the model (in GB or MB or )\n",
    "\n",
    "def estimate_model_size(model, unit='MB', to_print = True): \n",
    "    persistent_memory = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        persistent_memory += param.element_size() * param.nelement()\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(persistent_memory/1024**3,8)\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    elif unit == 'B':\n",
    "        gb_mem = persistent_memory\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"Bytes\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(persistent_memory/1024**2,8)\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "    \n",
    "def estimate_input_size(inp, unit='MB'):\n",
    "    input_size = 0\n",
    "    if isinstance(inp, torch.Tensor): \n",
    "        input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "    if isinstance(inp, list): \n",
    "        for sub_inp in inp:\n",
    "            if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "\n",
    "    input_size = input_size*torch.rand((1,1)).element_size() # multiply by 4\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(input_size/1024**3,8)\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    if unit == 'B':\n",
    "        gb_mem = input_size\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"B\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(input_size/1024**2,8)\n",
    "        print(\"Estimated Input Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = {}\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return {id(model): model}\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.update(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.update(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "def get_module_list(model):\n",
    "    module_list = []\n",
    "    def get_modules(module):\n",
    "        sub_modules = module.__dict__['_modules']\n",
    "        if len(sub_modules)>0:\n",
    "            for name, sub_module in sub_modules.items():\n",
    "                get_modules(sub_module)\n",
    "        else:\n",
    "            module_list.append(module)\n",
    "    get_modules(model)\n",
    "    return module_list\n",
    "\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments passed (usually from the command line)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Defined in this round about way (instead of just directly assigning) to keep it compatibble with summarize.py\n",
    "class Args:\n",
    "     def __init__(self,itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch):\n",
    "         self.type = itype\n",
    "         self.prof_rounds = prof_rounds\n",
    "         self.prof_gpu_id = prof_gpu_id\n",
    "         self.batch_size = batch_size\n",
    "         self.gpu_num = gpu_num\n",
    "         self.sch = sch\n",
    "            \n",
    "itype       = 'forward'  # help: forward/all -> Conside forward path only or both\n",
    "prof_rounds = 20     # help: 'rounds for profiler'\n",
    "prof_gpu_id = 0      # help: 'which gpu to place the profiler'\n",
    "batch_size  = '32'   # help: 'batch_size'\n",
    "gpu_num     = 4      # help: 'number of gpu to use'\n",
    "sch         = 'sct'  # help: 'sct/etf/topo'\n",
    "\n",
    "args = Args(itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device (4 GPU) Setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Function: placer_lib.create_device_graph\n",
    "    -> Creates a graph with devices as nodes and unit weight edges between them\n",
    "    -> Each node: graph.add_node(device_id,\n",
    "                                 id=device_id,\n",
    "                                 name=device_info[\"name\"],\n",
    "                                 size=0,\n",
    "                                 memory_limit=device_info[\"memory_size\"])\n",
    "\"\"\"\n",
    "DEVICE_GRAPH_SINGLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 17179869184, 'type': ''}})\n",
    "DEVICE_GRAPH_MULTIPLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             1: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:1', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             2: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:2', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             3: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:3', 'memory_size': 8000000000, 'type': ''}})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    we are going to use streams to allow parallel processing\n",
    "\"\"\"\n",
    "COMPUTE0 = torch.cuda.Stream(device=0)\n",
    "COMPUTE1 = torch.cuda.Stream(device=1)\n",
    "COMPUTE2 = torch.cuda.Stream(device=2)\n",
    "COMPUTE3 = torch.cuda.Stream(device=3)\n",
    "COMPUTE_STREAM = {0:COMPUTE0,1:COMPUTE1,2:COMPUTE2,3:COMPUTE3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")\n",
    "\n",
    "# print memory of given GPU. ex: gpu_no = 0\n",
    "def print_mem(gpu_id, cached=2, unit='GB'):\n",
    "    if unit=='GB':\n",
    "        mem_allocated = round(torch.cuda.memory_allocated(gpu_id)/1024**3,8)\n",
    "        mem_cached    = round(torch.cuda.memory_reserved(gpu_id)/1024**3,8)\n",
    "    else:\n",
    "        mem_allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "        mem_cached    = torch.cuda.memory_reserved(gpu_id)\n",
    "        \n",
    "    if cached>0:\n",
    "        print('Allocated:', mem_allocated , 'GB')\n",
    "    if cached>1:\n",
    "        print('Cached:   ', mem_cached    , 'GB')\n",
    "    return mem_allocated, mem_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Estimate size of the model (in GB or MB or )\n",
    "\n",
    "def estimate_model_size(model, unit='MB', to_print = True): \n",
    "    persistent_memory = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        persistent_memory += param.element_size() * param.nelement()\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(persistent_memory/1024**3,8)\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    elif unit == 'B':\n",
    "        gb_mem = persistent_memory\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"Bytes\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(persistent_memory/1024**2,8)\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "    \n",
    "def estimate_input_size(inp, unit='MB'):\n",
    "    input_size = 0\n",
    "    if isinstance(inp, torch.Tensor): \n",
    "        input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "    if isinstance(inp, list): \n",
    "        for sub_inp in inp:\n",
    "            if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "\n",
    "    input_size = input_size*torch.rand((1,1)).element_size() # multiply by 4\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(input_size/1024**3,8)\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    if unit == 'B':\n",
    "        gb_mem = input_size\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"B\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(input_size/1024**2,8)\n",
    "        print(\"Estimated Input Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the leaf operations in a model. model.modules() gives not just the leaves, bbut higher levels as well\n",
    "# Ref: https://stackoverflow.com/questions/54846905/pytorch-get-all-layers-of-model\n",
    "# More explanation: https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/4\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = {}\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return {id(model): model}\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.update(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.update(get_children(child))\n",
    "    return flatt_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A global variable can be directly called form inside a function\n",
    "## But to change it, use the 'global' keyword\n",
    "## Source: https://stackoverflow.com/questions/10588317/python-function-global-variables\n",
    "def del_all():\n",
    "    ## Clear the GPU\n",
    "    try:\n",
    "        global model\n",
    "        del model\n",
    "    except:\n",
    "        print(\"No model\")\n",
    "    try:\n",
    "        global inp\n",
    "        del inp\n",
    "    except:\n",
    "        print(\"No inp\")\n",
    "    try:\n",
    "        global labels\n",
    "        del labels\n",
    "    except:\n",
    "        print(\"No Labels\")\n",
    "    try:\n",
    "        global output\n",
    "        del output\n",
    "    except:\n",
    "        print(\"No Output\")\n",
    "    try:\n",
    "        global loss\n",
    "        del loss\n",
    "    except:\n",
    "        print(\"No Loss\")\n",
    "    try:\n",
    "        global optimizer\n",
    "        del optimizer\n",
    "    except:\n",
    "        print(\"No optimizer\")\n",
    "    print(\"Emptying cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print_mem(args.prof_gpu_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For plotting memory trace through a training run.\n",
    "\n",
    "## (Github: https://github.com/quentinf00/article-memory-log)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def pp(df, exp):\n",
    "    df_exp = df[df.exp == exp]\n",
    "    df_pprint = (\n",
    "        df_exp.assign(\n",
    "            open_layer=lambda ddf: ddf.hook_type.map(\n",
    "                lambda x: {\"pre\": 0, \"fwd\": 1, \"bwd\": 2}[x]).rolling(2).apply(lambda x: x[0] == 0 and x[1] == 0\n",
    "                                                                              )\n",
    "        )\n",
    "            .assign(\n",
    "            close_layer=lambda ddf: ddf.hook_type.map(\n",
    "                lambda x: {\"pre\": 0, \"fwd\": 1, \"bwd\": 2}[x]).rolling(2).apply(lambda x: x[0] == 1 and x[1] == 1)\n",
    "        )\n",
    "            .assign(indent_level=lambda ddf: (ddf.open_layer.cumsum() - ddf.close_layer.cumsum()).fillna(0).map(int))\n",
    "            .sort_values(by=\"call_idx\")\n",
    "            .assign(mem_diff=lambda ddf: ddf.mem_all.diff() // 2 ** 20)\n",
    "    )\n",
    "    pprint_lines = [\n",
    "        f\"{'    ' * row[1].indent_level}{row[1].layer_type} {row[1].hook_type}  {row[1].mem_diff or ''}\"\n",
    "        for row in df_pprint.iterrows()\n",
    "    ]\n",
    "    for x in pprint_lines:\n",
    "        print(x)\n",
    "\n",
    "\n",
    "def plot_mem(\n",
    "        df,\n",
    "        exps=None,\n",
    "        normalize_call_idx=True,\n",
    "        normalize_mem_all=True,\n",
    "        filter_fwd=False,\n",
    "        return_df=False,\n",
    "        output_file=None\n",
    "):\n",
    "    if exps is None:\n",
    "        exps = df.exp.drop_duplicates()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    for exp in exps:\n",
    "        df_ = df[df.exp == exp]\n",
    "\n",
    "        if normalize_call_idx:\n",
    "            df_.call_idx = df_.call_idx / df_.call_idx.max()\n",
    "\n",
    "        if normalize_mem_all:\n",
    "            df_.mem_all = df_.mem_all - df_[df_.call_idx == df_.call_idx.min()].mem_all.iloc[0]\n",
    "            df_.mem_all = df_.mem_all // 2 ** 20\n",
    "\n",
    "        if filter_fwd:\n",
    "            layer_idx = 0\n",
    "            callidx_stop = df_[(df_[\"layer_idx\"] == layer_idx) & (df_[\"hook_type\"] == \"fwd\")][\"call_idx\"].iloc[0]\n",
    "            df_ = df_[df_[\"call_idx\"] <= callidx_stop]\n",
    "            # df_ = df_[df_.call_idx < df_[df_.layer_idx=='bwd'].call_idx.min()]\n",
    "\n",
    "        plot = df_.plot(ax=ax, x='call_idx', y='mem_all', label=exp)\n",
    "        if output_file:\n",
    "            plot.get_figure().savefig(output_file)\n",
    "\n",
    "    if return_df:\n",
    "        return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Submodules of model as nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModuleNode:\n",
    "    \"\"\"\n",
    "    This class represents a submodel (ex. conv2d layer) in the given model (ex. inception_v3). \n",
    "    It is represented as a node in the return graph\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # store the entire submodel\n",
    "        self.module = None\n",
    "        # submodel name\n",
    "        self.name = None\n",
    "\n",
    "        # nodes that must finish processing before this node (direct dependencies)\n",
    "        self.parent = set()\n",
    "        # nodes that depends on this node\n",
    "        self.children = set()\n",
    "\n",
    "        # forward function's estimated runtime\n",
    "        self.weight_forward = 0\n",
    "        # backward function's estimated runtime\n",
    "        self.weight_backward = 0\n",
    "        # id represented by the model's location (python's id function)\n",
    "        self.id_hash = None\n",
    "        # sudo id used, for one model, this sudo id starts from 0 and add 1 for each new node\n",
    "        self.id = None\n",
    "        # storage used by submodel's parameters (weight, bias)\n",
    "        self.persistent_memory = 0\n",
    "        # submodel's input's size\n",
    "        self.input_memory = 0\n",
    "        # submodel's output's size\n",
    "        self.output_memory = 0\n",
    "        \n",
    "        # gpu assigned to the submodule\n",
    "        self.p = None\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Profilling Method\n",
    "\n",
    "Objectives: For a given model:\n",
    "1) Decompose the model to consituent modules (layers) \\\n",
    "2) Measure steady state forward pass time for each of them \\\n",
    "3) Measure steady state backward pass time for each of them \\\n",
    "4) Measure memory usuage for each of them (input, module parameters, output) \\\n",
    "\n",
    "\n",
    "Functions:\n",
    "\n",
    "```run```: runs a dry run of training \\\n",
    "&emsp;-```recur_function```: iteratively breaks the model into modules. For each leaf module (those with empty module.__dict__['_modules']), compute its forward completion time, add a forward hook and backward hook, create a SubModule Node and return it \\\n",
    "\n",
    "Problems:\n",
    "\n",
    "- Memory measurement is measured as the sum of number of parameters. Size factor of 4 (tensor.float32) is missing\n",
    "- Memory measurement is static. Peak memory usuage due to storing of activations in forward pass not accounted for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiling:\n",
    "    \"\"\"\n",
    "    This class produce the profile, this class referenced \"https://github.com/msr-fiddle/pipedream\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, gpu=0, rounds=20, inception=False):\n",
    "        \"\"\"\n",
    "        model: ex. inception_v3 model, alexnet model, etc\n",
    "        gpu: choose in between {0,1,2,3}\n",
    "        rounds: number of rounds to run the profiling\n",
    "        \"\"\"\n",
    "        self.gpu = gpu\n",
    "        self.model = model.to(self.gpu)\n",
    "\n",
    "        self.rounds = rounds\n",
    "        # first few rounds are inaccurate, so I choose to discard the results from the first 1/4 rounds\n",
    "        self.ignore_rounds = int(self.rounds/4)\n",
    "        # counting variable, runs from 0 - self.rounds\n",
    "        self.cur_round = 0\n",
    "\n",
    "        # used to calculate backward runtime for each submodule\n",
    "        self.back_record = []\n",
    "        # all submodules record of the form {id_hash: SubModuleNode}\n",
    "        self.sub_module_nodes = {}\n",
    "        # use id_hash to record the order of submodules's execution\n",
    "        self.submodule_order = []\n",
    "\n",
    "        # internal use only, record the original forward functions for submodules\n",
    "        self.forward_original_methods = {}\n",
    "        # internal use only, switch back to the original forward functions after profiling\n",
    "        self.detach_record = set()\n",
    "        # Indicates if it is an inception model\n",
    "        self.inception = inception\n",
    "\n",
    "    def recur_function(self, module):\n",
    "        \"\"\"\n",
    "        modify self.model: adding forward timing, backward timing, input output sizes, etc\n",
    "        :param module: the model to recursively add forward/backward wrappers to\n",
    "        \"\"\"\n",
    "        this_profiler = self\n",
    "        sub_modules = module.__dict__['_modules']\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            # sub modules of sub_module, if there are more than 1, we need further recursion\n",
    "            sub_sub_modules = sub_module.__dict__['_modules']\n",
    "            if len(sub_sub_modules) > 0:\n",
    "                self.recur_function(sub_module)\n",
    "                continue\n",
    "\n",
    "            def calculate_forward_time(function, *input):\n",
    "                \"\"\"\n",
    "                helper function in forward wrapper\n",
    "                calculate forward runtime, and submodule result\n",
    "                \"\"\"\n",
    "                torch.cuda.synchronize(self.gpu)\n",
    "                start_time = time.time()\n",
    "                result = function(*input)\n",
    "                torch.cuda.synchronize(self.gpu)\n",
    "                stop_time = time.time()\n",
    "                return (stop_time - start_time) * 1000, result\n",
    "\n",
    "            def forward_wrapper(cur_module, *input):\n",
    "                \"\"\"\n",
    "                use this wrapper to replace the original forward function in submodules\n",
    "                :param cur_module: the input submodule\n",
    "                \"\"\"\n",
    "                # original forward function\n",
    "                function = this_profiler.forward_original_methods[cur_module]\n",
    "                if this_profiler.cur_round < this_profiler.ignore_rounds:\n",
    "                    if this_profiler.cur_round == 0:\n",
    "                        # record submodule execution order only in the first round\n",
    "                        this_profiler.submodule_order.append(id(cur_module))\n",
    "                    # do not record first few rounds\n",
    "                    result = function(*input)\n",
    "                    return result\n",
    "                \n",
    "                # collect relevant information of cur module\n",
    "                forward_time, result = calculate_forward_time(function, *input)\n",
    "                input_size = 0; persistent_memory = 0; output_memory = 1\n",
    "                for inp in input:\n",
    "                    if isinstance(inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "                    if isinstance(inp, list): \n",
    "                        for sub_inp in inp:\n",
    "                            if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "                for name, param in cur_module.named_parameters():\n",
    "                    product = 1\n",
    "                    for i in param.size(): product *= i\n",
    "                    persistent_memory += product\n",
    "                for i in result.size(): output_memory *= i\n",
    "\n",
    "                # record a SubModuleNode for each model layer\n",
    "                if id(cur_module) not in this_profiler.sub_module_nodes:\n",
    "                    cur_node = SubModuleNode()\n",
    "                    cur_node.id_hash = id(cur_module)\n",
    "                    cur_node.module = cur_module\n",
    "                    cur_node.name = cur_module.__class__.__name__\n",
    "                    cur_node.persistent_memory = persistent_memory\n",
    "                    cur_node.output_memory = output_memory\n",
    "                    cur_node.input_memory = input_size\n",
    "                else:\n",
    "                    cur_node = this_profiler.sub_module_nodes[id(cur_module)]\n",
    "                # we want weight_forward as the average forward runtime of the relevent rounds\n",
    "                cur_node.weight_forward += forward_time / (this_profiler.rounds - this_profiler.ignore_rounds)\n",
    "                this_profiler.sub_module_nodes[id(cur_module)] = cur_node\n",
    "\n",
    "                return result\n",
    "\n",
    "            def hook(cur_module, inputs, output):\n",
    "                # this is for retriving the module inside make dot function\n",
    "                output.grad_fn.metadata['module'] = cur_module\n",
    "\n",
    "            def backward_post_hook(cur_module, input, output):\n",
    "                \"\"\"\n",
    "                add backward hook to record backward runtime\n",
    "                :param cur_module: the input submodule\n",
    "                \"\"\"\n",
    "                if this_profiler.cur_round < this_profiler.ignore_rounds:\n",
    "                    # do not record first few rounds\n",
    "                    return\n",
    "                torch.cuda.synchronize(0)\n",
    "                cur_time = time.time() * 1000\n",
    "                this_profiler.back_record.append((id(cur_module), cur_time))\n",
    "\n",
    "            if sub_module in self.forward_original_methods:\n",
    "                # only record the original forward functions once\n",
    "                continue\n",
    "\n",
    "            self.forward_original_methods[sub_module] = sub_module.forward\n",
    "            sub_module.forward = forward_wrapper.__get__(sub_module, sub_module.__class__)\n",
    "            sub_module.register_forward_hook(hook)\n",
    "            sub_module.register_backward_hook(backward_post_hook)\n",
    "            \n",
    "    def detach(self, module):\n",
    "        \"\"\"\n",
    "        use this helper function to detach all forward wrappers\n",
    "        \"\"\"\n",
    "        this_profiler = self\n",
    "        sub_modules = module.__dict__['_modules']\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            sub_sub_modules = sub_module.__dict__['_modules']\n",
    "            if len(sub_sub_modules) > 0:\n",
    "                self.detach(sub_module)\n",
    "                continue\n",
    "            if sub_module in self.detach_record:\n",
    "                continue\n",
    "\n",
    "            self.detach_record.add(sub_module)\n",
    "            sub_module.forward = self.forward_original_methods[sub_module]\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return: the model's output of the final round\n",
    "        \"\"\"\n",
    "        self.sub_module_nodes = {}\n",
    "        self.recur_function(self.model)\n",
    "\n",
    "        # create a fake dataset, we don't care about accuracy.\n",
    "        dataset = torchvision.datasets.FakeData(\n",
    "            size=self.rounds * int(args.batch_size),\n",
    "            image_size=(3, 299, 299),\n",
    "            num_classes=1000,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=int(args.batch_size))\n",
    "        # this is the output of the final round\n",
    "        last_output = None\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            self.cur_round = batch_idx\n",
    "            # clear the record list\n",
    "            self.back_record = []\n",
    "\n",
    "            inp = inp.to(self.gpu); inp.requires_grad = True\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr = 0.0001); optimizer.zero_grad()\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            torch.cuda.synchronize(self.gpu)\n",
    "            output = self.model(inp)\n",
    "            torch.cuda.synchronize(self.gpu)\n",
    "\n",
    "            ## the following two lines only apply to inception_v3 model\n",
    "            if (self.inception):\n",
    "                output = output.logits[0] + output.aux_logits[0]\n",
    "                loss = criterion(output, torch.randn(len(output)).to(self.gpu))\n",
    "            else: ## if the model is not inception_v3, use the following one line\n",
    "                loss = criterion(output, torch.randn(int(args.batch_size), len(output[0])).to(self.gpu))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # add the start time of backward \n",
    "            self.back_record.append(('start', time.time() * 1000))\n",
    "            if batch_idx == self.rounds - 1:\n",
    "                loss.backward(loss, retain_graph=True)\n",
    "                last_output = output\n",
    "            else:\n",
    "                loss.backward(loss)\n",
    "\n",
    "            if batch_idx < self.ignore_rounds:\n",
    "                continue\n",
    "            else:\n",
    "                # calculate the backward runtime for each layer by calculating the time differences between each timestamp\n",
    "                for i in range(len(self.back_record) - 1, 0, -1):\n",
    "                    now = self.back_record[i]\n",
    "                    prev = self.back_record[i - 1]\n",
    "                    cur_node = self.sub_module_nodes[now[0]]\n",
    "                    cur_node.weight_backward += (now[1] - prev[1]) / (self.rounds - self.ignore_rounds)\n",
    "                    self.sub_module_nodes[now[0]] = cur_node\n",
    "        self.detach(self.model)\n",
    "        return last_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*\\*\\*\\*\\*\\*Test - Profiling:**\\*\\*\\*\\***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Time Tests\n",
    "\n",
    "- Measure the time for forward pass through a module.\n",
    "- Verify that forward pass time through entire model is sum of individual forward passes\n",
    "- All tests done on single GPU. No communication costs involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD MODEL & INPUTS INTO THE GPU\n",
    "\n",
    "factorModel = 1  # Factor by which size of layers (i.eno of params) in the model is scaled\n",
    "inp_size = (32, 3, 299, 299)\n",
    "\n",
    "to_gpu = 1 # to use GPU or CPU?\n",
    "which_model = 'toyToyModel'\n",
    "\n",
    "if which_model == 'toyModel':\n",
    "    model = sm.toyModel(factor=factorModel)\n",
    "elif which_model == 'toyToyModel':\n",
    "    model = sm.toyToyModel(factor=factorModel)\n",
    "else:\n",
    "    print(\"Invalid model name\")\n",
    "    \n",
    "inp   = torch.rand(inp_size)\n",
    "\n",
    "if to_gpu:\n",
    "    model = model.to(args.prof_gpu_id)\n",
    "    inp   = inp.to(args.prof_gpu_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## python initialization on GPU seems to take up 1080 MB (Why???)\n",
    "## nvidia-smi shows 1378MB occupied\n",
    "1080 + estimate_model_size(model) + estimate_input_size(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_gpu = args.prof_gpu_id\n",
    "\n",
    "def _calculate_forward_time(function, *input):\n",
    "    \"\"\"\n",
    "    helper function in forward wrapper\n",
    "    calculate forward runtime, and submodule result\n",
    "    \"\"\"\n",
    "    torch.cuda.synchronize(prof_gpu)\n",
    "    start_time = time.time()\n",
    "    result = function(*input)\n",
    "    torch.cuda.synchronize(prof_gpu) # need to sync before measuring time\n",
    "    stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the toyModel for this part\n"
     ]
    }
   ],
   "source": [
    "## Check forward times for some layers individually\n",
    "if which_model == 'toyModel':\n",
    "    #Conv2D\n",
    "    sub_module = model.__dict__['_modules']['features'].__dict__['_modules']['0']\n",
    "    forward_time0, result0 = _calculate_forward_time(sub_module.forward,inp)\n",
    "\n",
    "    #ReLU\n",
    "    sub_module = model.__dict__['_modules']['features'].__dict__['_modules']['1']\n",
    "    forward_time1, result1 = _calculate_forward_time(sub_module.forward,inp)\n",
    "\n",
    "\n",
    "    print (\"Conv2d layer forward time:\", forward_time0)\n",
    "    print (\"ReLU layer forward time:\", forward_time1)\n",
    "    print()\n",
    "    \n",
    "# Total time for 'sequential'\n",
    "    forward_times =[]\n",
    "    result = inp\n",
    "    for name, sub_module in model.__dict__['_modules']['features'].__dict__['_modules'].items():\n",
    "        forward_time, result = _calculate_forward_time(sub_module.forward,result)\n",
    "        forward_times.append(forward_time)\n",
    "\n",
    "    print (\"Sum of individual forwards:\")\n",
    "    print (sum(forward_times)) \n",
    "\n",
    "\n",
    "    module = model.__dict__['_modules']['features']\n",
    "    forward_time, result = _calculate_forward_time(module.forward,inp)\n",
    "\n",
    "    print(\"Net forward\")\n",
    "    print (forward_time)\n",
    "\n",
    "    # Verfied _calculate_forward_time works\n",
    "else:\n",
    "    print(\"Use the toyModel for this part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of individual forwards:\n",
      "7.464885711669922\n",
      "Net forward\n",
      "2.548694610595703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "### Main TEST: Check sum of computed forward times is equal to net forward time\n",
    "### Ensures there are no hidden operations when layers are chained together\n",
    "forward_times =[]\n",
    "result = inp\n",
    "for sub_module_id, sub_module in get_children(model).items():\n",
    "    if isinstance(sub_module, nn.Linear):\n",
    "        result = torch.flatten(result, 1)\n",
    "    forward_time, result = _calculate_forward_time(sub_module.forward,result)\n",
    "    forward_times.append(forward_time)\n",
    "\n",
    "print (\"Sum of individual forwards:\")\n",
    "print (sum(forward_times)) \n",
    "\n",
    "\n",
    "forward_time, result = _calculate_forward_time(model.forward,inp)\n",
    "\n",
    "print(\"Net forward\")\n",
    "print (forward_time)\n",
    "\n",
    "# Verfied _calculate_forward_time works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.520654678344727,\n",
       " 0.5884170532226562,\n",
       " 1.0364055633544922,\n",
       " 0.2651214599609375,\n",
       " 0.6837844848632812,\n",
       " 0.2758502960205078,\n",
       " 0.09465217590332031]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'del_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d12379247b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdel_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'del_all' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del result0\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del result1\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del sub_module\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del result\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del module\n",
    "except:\n",
    "    pass\n",
    "del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Memory estimation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clear the GPU\n",
    "del_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute actual on-GPU memory of model and the input\n",
    "\n",
    "factorModel = 1\n",
    "inp_size = (1, 3, 2990, 2990)\n",
    "\n",
    "print(\"Before starting.......\\n\")\n",
    "mem_allocated0, mem_cached0 = print_mem(args.prof_gpu_id)\n",
    "\n",
    "print(\"\\nOn Moving model to GPU....\\n\")\n",
    "model = sm.toyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "mem_allocated1, mem_cached1 = print_mem(args.prof_gpu_id)\n",
    "\n",
    "print(\"\\nOn Moving Input to GPU....\\n\")\n",
    "inp = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "mem_allocated2, mem_cached2 = print_mem(args.prof_gpu_id)\n",
    "\n",
    "model_actual_memory = round(mem_allocated1 - mem_allocated0, 10)\n",
    "input_actual_memory = round(mem_allocated2 - mem_allocated1, 10)\n",
    "\n",
    "print()\n",
    "print(\"Actual model memory: \", model_actual_memory)\n",
    "print(\"Actual Input memory: \", input_actual_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current memory computation is off by a factor of 4 (i.e size tensor.float32)\n",
    "## Just multiply the number of parameters in the sub_module result by 4\n",
    "\n",
    "input_size = 0\n",
    "if isinstance(inp, torch.Tensor): \n",
    "    input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "if isinstance(inp, list): \n",
    "    for sub_inp in inp:\n",
    "        if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "\n",
    "input_size = input_size*torch.rand((1,1)).element_size() # multiply by 4\n",
    "\n",
    "print(\"Estimated Input memory: \", round(input_size/1024**3,4), \"GB\")\n",
    "print(\"Actual Input memory   : \", input_actual_memory, \"GB\", \"\\n\")\n",
    "\n",
    "\n",
    "## Correct way - Model memory\n",
    "persistent_memory = 0\n",
    "for name, param in model.named_parameters():\n",
    "    persistent_memory += param.element_size() * param.nelement()\n",
    "\n",
    "print(\"Estimated Model Memory:\",round(persistent_memory/1024**3,4), \"GB\")\n",
    "print(\"Actual model memory   :\",model_actual_memory, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clear the GPU\n",
    "print_gpu_memory()\n",
    "del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory changes in forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step through the training process and note the memory changes at each step\n",
    "\n",
    "##----------------STEPS TO RUN---------------------------------\n",
    "## steps_to_run options\n",
    "## 0 -> Load model and input\n",
    "## 1 -> Run Forward\n",
    "## 2 -> Move labels and Compute loss\n",
    "## 3 -> Backward Run\n",
    "## 4 -> Run Update Step\n",
    "## 5 -> Delete all intermediates\n",
    "steps_to_run = 4\n",
    "reload_model = 0 #indicates to not reload model and inputs if already loaded\n",
    "##--------------------------------------------------------------\n",
    "\n",
    "loaded = 0       # flag to indicate if model and inputs are already loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n",
      "Loading the Model and input\n",
      "Allocated: 0.11900806 GB\n",
      "Cached:    0.1328125 GB\n",
      "Difference:  0.11900806 GB \n",
      "\n",
      "Loading the optimizer\n",
      "Allocated: 0.11900806 GB\n",
      "Cached:    0.1328125 GB\n",
      "Difference:  0.0 GB \n",
      "\n",
      "Net setup memory cost\n",
      "Allocated: 0.11900806 GB\n",
      "Cached:    0.1328125 GB\n",
      "Difference:  0.11900806 GB\n",
      "\n",
      "Estimated Model Memory: 0.08666888 GB\n",
      "Estimated Input Memory: 0.03197229 GB\n",
      "Estimated net memory: 0.11864117\n",
      "**************************************************************** \n",
      "\n",
      "1) Forward Run: Output generated\n",
      "Estimated Output Size:  0.0001192093 GB\n",
      "Allocated: 0.23424721 GB\n",
      "Cached:    0.24023438 GB\n",
      "Difference:  0.11523915000000001 GB\n",
      "**************************************************************** \n",
      "\n",
      "2) Labels moved:\n",
      "Allocated: 0.23436642 GB\n",
      "Cached:    0.24023438 GB\n",
      "Difference:  0.00011920999999998072 GB\n",
      "**************************************************************** \n",
      "\n",
      "3) Loss Computed:\n",
      "Allocated: 0.23436689 GB\n",
      "Cached:    0.24023438 GB\n",
      "Difference:  4.7000000000241293e-07 GB\n",
      "**************************************************************** \n",
      "\n",
      "4) Backward Run:\n",
      "Allocated: 0.20628262 GB\n",
      "Cached:    0.3828125 GB\n",
      "Difference:  -0.028084269999999995 GB\n",
      "**************************************************************** \n",
      "\n",
      "5) Step & Clear Gradients:\n",
      "Allocated: 0.11924696 GB\n",
      "Cached:    0.3828125 GB\n",
      "Difference:  -0.08703566 GB\n",
      "**************************************************************** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Initial State:\")\n",
    "mem_allocated, mem_cached = print_mem(args.prof_gpu_id)\n",
    "print()\n",
    "\n",
    "# Load Model and Input\n",
    "if (steps_to_run >= 0) and (loaded==0):\n",
    "    print(\"Loading the Model and input\")\n",
    "    factorModel = 1  # Factor by which size of layers (i.eno of params) in the model is scaled\n",
    "    \n",
    "    ################################################################################\n",
    "    inp_size = (32, 3, 299, 299)\n",
    "    model = sm.toyToyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "    #model = sm.toyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "    \n",
    "    #inp_size = (1, 10000)\n",
    "    #model = sm.linearModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "    ################################################################################\n",
    "    \n",
    "    inp   = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "    \n",
    "    inp.requires_grad = False # Usually you don’t need gradients in your input. \n",
    "                              # However, gradients in the input might be needed for some \n",
    "                              # special use cases e.g. creating adversarial samples.\n",
    "    \n",
    "    \n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated0a, mem_cached0a = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated0a-mem_allocated, \"GB \\n\")\n",
    "\n",
    "    #--------------------TRAINING STEPS-----------------------------\n",
    "    print(\"Loading the optimizer\")\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated0b, mem_cached0b = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated0b-mem_allocated0a, \"GB \\n\") \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if reload_model == 0:\n",
    "        loaded = 1\n",
    "elif (loaded==1):\n",
    "    print(\"Model already loaded!\")\n",
    "    inp   = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "    \n",
    "print(\"Net setup memory cost\")\n",
    "torch.cuda.synchronize(args.prof_gpu_id)\n",
    "mem_allocated0, mem_cached0 = print_mem(args.prof_gpu_id)\n",
    "print(\"Difference: \", mem_allocated0-mem_allocated, \"GB\\n\")\n",
    "print(\"Estimated net memory:\", estimate_model_size(model, 'GB') + estimate_input_size(inp, 'GB')) \n",
    "print(\"**************************************************************** \\n\")\n",
    "\n",
    "##-----------------ACTUAL RUN----------------------------------\n",
    "if steps_to_run >= 1:\n",
    "    print(\"1) Forward Run: Output generated\")\n",
    "    #model.zero_grad(set_to_none=True)\n",
    "    output = model(inp)\n",
    "\n",
    "    output_size = 0\n",
    "    if isinstance(output, torch.Tensor): \n",
    "        output_size += float(torch.prod(torch.tensor(output.size())))\n",
    "    output_size = output_size*4\n",
    "    print(\"Estimated Output Size: \", round(output_size/1024**3,10), \"GB\" )\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated1, mem_cached1 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated1-mem_allocated0, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "if steps_to_run >= 2:\n",
    "    print(\"2) Labels moved:\")\n",
    "    labels = torch.randn(output.size()).to(args.prof_gpu_id)\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated2, mem_cached2 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated2-mem_allocated1, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "if steps_to_run >= 2:\n",
    "    print(\"3) Loss Computed:\")\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated3, mem_cached3 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated3-mem_allocated2, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "if steps_to_run >= 3:\n",
    "    print(\"4) Backward Run:\")\n",
    "    loss.backward(loss)\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated4, mem_cached4 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated4-mem_allocated3, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "if steps_to_run >= 4:\n",
    "    print(\"5) Step & Clear Gradients:\")\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.zero_grad(set_to_none=True)\n",
    "    ##****zero_grad is same as this:****\n",
    "    #for param in model.parameters():\n",
    "    #    param.grad = None\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated5, mem_cached5 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated5-mem_allocated4, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "if steps_to_run >= 5:\n",
    "    print(\"6) Delete Intermediates:\")\n",
    "    del loss\n",
    "    del output\n",
    "\n",
    "    torch.cuda.synchronize(args.prof_gpu_id)\n",
    "    mem_allocated6, mem_cached6 = print_mem(args.prof_gpu_id)\n",
    "    print(\"Difference: \", mem_allocated6-mem_allocated5, \"GB\")\n",
    "    print(\"**************************************************************** \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.11924696 GB\n",
      "Cached:    0.3828125 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "Emptying cache\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "## Clear the GPU\n",
    "### Note: If the optimizer is not defined, then del model, del inp will clear the GPU\n",
    "### However if optimizer is defined, the memory will still be occupied after del model, del inp\n",
    "### To clear the mem, you also need del optimizer (which is done inside del_all())\n",
    "\n",
    "print_gpu_memory()\n",
    "del_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.list_gpu_processes()\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### TEST MEMORY CONSUMPTION on FORWARD -  no_grad() and requires_grad##########\n",
    "\n",
    "if 1:\n",
    "    ############# Forward with no_grad() #######################\n",
    "    print(\"CASE: no_grad\")\n",
    "    ini_mem, _ = print_mem(0,0)\n",
    "\n",
    "    model = sm.toyModel(factor=1).to(args.prof_gpu_id)\n",
    "    inp   = torch.rand((1,3,600,600)).to(args.prof_gpu_id)\n",
    "\n",
    "    load_mem, _ = print_mem(0,0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(inp)\n",
    "    forward_mem, _ = print_mem(0,0)\n",
    "\n",
    "    print(\"Memory on loading model & input:\", load_mem-ini_mem)\n",
    "    print(\"Memory on running forward:\", forward_mem-ini_mem)\n",
    "    print(\"Difference - memory consumed on running forward: \", forward_mem - load_mem )\n",
    "\n",
    "    del_all(); print('*'*40, '\\n')\n",
    "\n",
    "if 1:\n",
    "    ########## Forward with requires_grad=False ############\n",
    "    print(\"CASE: requires_grad=False\")\n",
    "    ini_mem, _ = print_mem(0,0)\n",
    "\n",
    "    model = sm.toyModel(factor=1).to(args.prof_gpu_id)\n",
    "    inp   = torch.rand((1,3,600,600)).to(args.prof_gpu_id)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    load_mem, _ = print_mem(0,0)\n",
    "\n",
    "    out = model(inp)\n",
    "    forward_mem, _ = print_mem(0,0)\n",
    "\n",
    "    print(\"Memory on loading model & input:\", load_mem-ini_mem)\n",
    "    print(\"Memory on running forward:\", forward_mem-ini_mem)\n",
    "    print(\"Difference - memory consumed on running forward: \", forward_mem - load_mem )\n",
    "\n",
    "    del_all(); print('*'*40, '\\n')\n",
    "\n",
    "if 1:\n",
    "    ################### Normal forward ###########################\n",
    "    ## Intermediate Activations saved\n",
    "    print(\"CASE: Normal forward - Intermediate Activations saved\")\n",
    "    ini_mem, _ = print_mem(0,0)\n",
    "\n",
    "    model = sm.toyModel(factor=1).to(args.prof_gpu_id)\n",
    "    inp   = torch.rand((1,3,600,600)).to(args.prof_gpu_id)\n",
    "\n",
    "    load_mem, _ = print_mem(0,0)\n",
    "\n",
    "    out = model(inp)\n",
    "    forward_mem, _ = print_mem(0,0)\n",
    "\n",
    "    print(\"Memory on loading model & input:\", load_mem-ini_mem)\n",
    "    print(\"Memory on running forward:\", forward_mem-ini_mem)\n",
    "    print(\"Difference - memory consumed on running forward: \", forward_mem - load_mem )\n",
    "\n",
    "    ####################### Backward ########################\n",
    "    #labels = torch.randn(len(out)).to(args.prof_gpu_id)\n",
    "    #criterion = nn.MSELoss()\n",
    "    #loss = criterion(out, labels)\n",
    "    #loss.backward(loss)\n",
    "    ##########################################################\n",
    "\n",
    "    del_all(); print('*'*40)\n",
    "    ##########################################################\n",
    "\n",
    "'''\n",
    "    Note: no_grad() and 'requires_grad=False' both cause forward to not store\n",
    "    activations. no_grad() however is a context within which a trainable model\n",
    "    can be put in inference mode. 'requires_grad=False' on the other hand fixes\n",
    "    the parameters and makes them not trainable\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer-wise memory trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Using hooks. Example\n",
    "\n",
    "factorModel = 1\n",
    "batch_size = 1\n",
    "inp_size = (batch_size, 3, 299, 299)\n",
    "\n",
    "#model = sm.toyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "model = sm.toyToyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "\n",
    "inp = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "\n",
    "prof_gpu_id = args.prof_gpu_id\n",
    "\n",
    "def _generate_mem_hook():\n",
    "    def hook(self, *args):\n",
    "        print(type(self).__name__)\n",
    "        print_mem(prof_gpu_id)\n",
    "        print()\n",
    "    return hook\n",
    "\n",
    "children_list = get_children(model)\n",
    "for mod_id, mod in children_list.items():\n",
    "    mod.register_forward_hook(_generate_mem_hook())\n",
    "    mod.register_backward_hook(_generate_mem_hook())\n",
    "    \n",
    "#******************************************************************************    \n",
    "## for toyModel - Layer wise illustration\n",
    "# mod = model.__dict__['_modules']['features']\n",
    "# h = mod.register_forward_pre_hook(_generate_mem_hook())\n",
    "#\n",
    "# mod = model.__dict__['_modules']['features'].__dict__['_modules']['0']\n",
    "# h = mod.register_forward_pre_hook(_generate_mem_hook())\n",
    "#******************************************************************************\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001); optimizer.zero_grad()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"**************Forward Run**********************************\")\n",
    "output = model(inp)\n",
    "labels = torch.randn(len(output[0])).to(args.prof_gpu_id)\n",
    "\n",
    "print(\"**************BBackward Run**********************************\")\n",
    "loss = criterion(output, labels)\n",
    "loss.backward(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main: Tracking the memory through training\n",
    "## Modified from: https://www.sicara.ai/blog/2019-28-10-deep-learning-memory-usage-and-pytorch-optimization-tricks\n",
    "## (Github: https://github.com/quentinf00/article-memory-log)\n",
    "\n",
    "def _get_gpu_mem(synchronize=True, empty_cache=True):\n",
    "    return torch.cuda.memory_allocated(), torch.cuda.memory_reserved()\n",
    "\n",
    "\n",
    "def _generate_mem_hook(handle_ref, mem, idx, hook_type, exp):\n",
    "    def hook(self, *args):\n",
    "        if len(mem) == 0 or mem[-1][\"exp\"] != exp:\n",
    "            call_idx = 0\n",
    "        else:\n",
    "            call_idx = mem[-1][\"call_idx\"] + 1\n",
    "        mem_all, mem_cached = _get_gpu_mem()\n",
    "        torch.cuda.synchronize()\n",
    "        mem.append({\n",
    "            'layer_idx': idx,\n",
    "            'call_idx': call_idx,\n",
    "            'layer_type': type(self).__name__,\n",
    "            'exp': exp,\n",
    "            'hook_type': hook_type,\n",
    "            'mem_all': mem_all,\n",
    "            'mem_cached': mem_cached,\n",
    "        })\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def _add_memory_hooks(idx, mod, mem_log, exp, hr):\n",
    "\n",
    "    h = mod.register_forward_pre_hook(_generate_mem_hook(hr, mem_log, idx, 'pre', exp))\n",
    "    hr.append(h)\n",
    "\n",
    "    h = mod.register_forward_hook(_generate_mem_hook(hr, mem_log, idx, 'fwd', exp))\n",
    "    hr.append(h)\n",
    "\n",
    "    h = mod.register_backward_hook(_generate_mem_hook(hr, mem_log, idx, 'bwd', exp))\n",
    "    hr.append(h)\n",
    "\n",
    "def log_mem(model, inp, mem_log=None, exp=None):\n",
    "    mem_log = []\n",
    "    exp = exp or f'exp_{len(mem_log)}'\n",
    "    hr = []\n",
    "    \n",
    "    children_list = get_children(model)\n",
    "    idx = 0\n",
    "    for module_id, module in children_list.items():\n",
    "        idx = idx +1\n",
    "        _add_memory_hooks(idx, module, mem_log, exp, hr)\n",
    "\n",
    "    try:\n",
    "        #--------------------TRAINING STEPS-----------------------------\n",
    "        mem_allocated0, mem_cached0 = print_mem(args.prof_gpu_id)\n",
    "        print()\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(), lr = 0.0001); optimizer.zero_grad()\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        print(\"1) Forward Run: Output generated\")\n",
    "        output = model(inp)\n",
    "\n",
    "        torch.cuda.synchronize(args.prof_gpu_id)\n",
    "        mem_allocated1, mem_cached1 = print_mem(args.prof_gpu_id)\n",
    "        print(\"Difference: \", mem_allocated1-mem_allocated0, \"GB \\n\")\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        print(\"2) Labels moved:\")\n",
    "        labels = torch.randn(output.size()).to(args.prof_gpu_id)\n",
    "\n",
    "        torch.cuda.synchronize(args.prof_gpu_id)\n",
    "        mem_allocated2, mem_cached2 = print_mem(args.prof_gpu_id)\n",
    "        print(\"Difference: \", mem_allocated2-mem_allocated1, \"GB \\n\")\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        print(\"3) Loss Computed:\")\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        torch.cuda.synchronize(args.prof_gpu_id)\n",
    "        mem_allocated3, mem_cached3 = print_mem(args.prof_gpu_id)\n",
    "        print(\"Difference: \", mem_allocated3-mem_allocated2, \"GB \\n\")\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        print(\"4) Backward Run:\")\n",
    "        loss.backward(loss)\n",
    "\n",
    "        torch.cuda.synchronize(args.prof_gpu_id)\n",
    "        mem_allocated4, mem_cached4 = print_mem(args.prof_gpu_id)\n",
    "        print(\"Difference: \", mem_allocated4-mem_allocated3, \"GB \\n\")\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        print(\"5) Step:\")\n",
    "        optimizer.step()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        torch.cuda.synchronize(args.prof_gpu_id)\n",
    "        mem_allocated5, mem_cached5 = print_mem(args.prof_gpu_id)\n",
    "        print(\"Difference: \", mem_allocated5-mem_allocated4, \"GB \\n\")\n",
    "    finally:\n",
    "        [h.remove() for h in hr]\n",
    "\n",
    "        return mem_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.2164669 GB\n",
      "Cached:    0.22265625 GB\n",
      "\n",
      "1) Forward Run: Output generated\n",
      "Allocated: 0.41208172 GB\n",
      "Cached:    0.46875 GB\n",
      "Difference:  0.19561482 GB \n",
      "\n",
      "2) Labels moved:\n",
      "Allocated: 0.41220093 GB\n",
      "Cached:    0.46875 GB\n",
      "Difference:  0.00011921000000003623 GB \n",
      "\n",
      "3) Loss Computed:\n",
      "Allocated: 0.4122014 GB\n",
      "Cached:    0.46875 GB\n",
      "Difference:  4.6999999997465736e-07 GB \n",
      "\n",
      "4) Backward Run:\n",
      "Allocated: 0.40124607 GB\n",
      "Cached:    0.75976562 GB\n",
      "Difference:  -0.010955329999999985 GB \n",
      "\n",
      "5) Step:\n",
      "Allocated: 0.2167058 GB\n",
      "Cached:    0.75976562 GB\n",
      "Difference:  -0.18454027 GB \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# %% Analysis baseline\n",
    "\n",
    "factorModel = 1\n",
    "batch_size = 32\n",
    "inp_size = (batch_size, 3, 299, 299)\n",
    "#inp_size =(1, 3, 600, 600)\n",
    "\n",
    "###****************Choose the MODEL************************************\n",
    "#model = sm.toyToyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "model = sm.toyModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "\n",
    "\n",
    "#model = sm.linearModel(factor=factorModel).to(args.prof_gpu_id)\n",
    "#inp_size = (1,10000)\n",
    "###********************************************************************\n",
    "inp = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "\n",
    "#****************************WHY?\n",
    "# inp.requires_grad = True\n",
    "#******************************\n",
    "\n",
    "mem_log = []\n",
    "\n",
    "try:\n",
    "    mem_log.extend(log_mem(model, inp,  exp='baseline'))\n",
    "except Exception as e:\n",
    "    print(f'log_mem failed because of {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'layer_idx': 1,\n",
       "  'call_idx': 0,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 232429568,\n",
       "  'mem_cached': 239075328},\n",
       " {'layer_idx': 1,\n",
       "  'call_idx': 1,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 277288960,\n",
       "  'mem_cached': 285212672},\n",
       " {'layer_idx': 2,\n",
       "  'call_idx': 2,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 277288960,\n",
       "  'mem_cached': 285212672},\n",
       " {'layer_idx': 2,\n",
       "  'call_idx': 3,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 322148352,\n",
       "  'mem_cached': 331350016},\n",
       " {'layer_idx': 3,\n",
       "  'call_idx': 4,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 322148352,\n",
       "  'mem_cached': 331350016},\n",
       " {'layer_idx': 3,\n",
       "  'call_idx': 5,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 353998848,\n",
       "  'mem_cached': 367001600},\n",
       " {'layer_idx': 4,\n",
       "  'call_idx': 6,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 353998848,\n",
       "  'mem_cached': 367001600},\n",
       " {'layer_idx': 4,\n",
       "  'call_idx': 7,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 385849344,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 5,\n",
       "  'call_idx': 8,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 385849344,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 5,\n",
       "  'call_idx': 9,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 417699840,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 6,\n",
       "  'call_idx': 10,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 417699840,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 6,\n",
       "  'call_idx': 11,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 439007232,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 7,\n",
       "  'call_idx': 12,\n",
       "  'layer_type': 'AdaptiveAvgPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 439007232,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 7,\n",
       "  'call_idx': 13,\n",
       "  'layer_type': 'AdaptiveAvgPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 439891968,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 8,\n",
       "  'call_idx': 14,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 439891968,\n",
       "  'mem_cached': 499122176},\n",
       " {'layer_idx': 8,\n",
       "  'call_idx': 15,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 440997888,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 9,\n",
       "  'call_idx': 16,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 440997888,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 9,\n",
       "  'call_idx': 17,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 441522176,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 10,\n",
       "  'call_idx': 18,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 441522176,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 10,\n",
       "  'call_idx': 19,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 442046464,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 11,\n",
       "  'call_idx': 20,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 442046464,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 11,\n",
       "  'call_idx': 21,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 442701824,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 12,\n",
       "  'call_idx': 22,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 442177536,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 12,\n",
       "  'call_idx': 23,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 442701824,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 13,\n",
       "  'call_idx': 24,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 442701824,\n",
       "  'mem_cached': 501219328},\n",
       " {'layer_idx': 13,\n",
       "  'call_idx': 25,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 443226112,\n",
       "  'mem_cached': 503316480},\n",
       " {'layer_idx': 14,\n",
       "  'call_idx': 26,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'pre',\n",
       "  'mem_all': 443226112,\n",
       "  'mem_cached': 503316480},\n",
       " {'layer_idx': 14,\n",
       "  'call_idx': 27,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'fwd',\n",
       "  'mem_all': 443354112,\n",
       "  'mem_cached': 503316480},\n",
       " {'layer_idx': 14,\n",
       "  'call_idx': 28,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 459638272,\n",
       "  'mem_cached': 503316480},\n",
       " {'layer_idx': 13,\n",
       "  'call_idx': 29,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 459510272,\n",
       "  'mem_cached': 503316480},\n",
       " {'layer_idx': 12,\n",
       "  'call_idx': 30,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 526111232,\n",
       "  'mem_cached': 570425344},\n",
       " {'layer_idx': 11,\n",
       "  'call_idx': 31,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 525586944,\n",
       "  'mem_cached': 570425344},\n",
       " {'layer_idx': 10,\n",
       "  'call_idx': 32,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 525455872,\n",
       "  'mem_cached': 570425344},\n",
       " {'layer_idx': 9,\n",
       "  'call_idx': 33,\n",
       "  'layer_type': 'Linear',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 638554624,\n",
       "  'mem_cached': 683671552},\n",
       " {'layer_idx': 8,\n",
       "  'call_idx': 34,\n",
       "  'layer_type': 'Dropout',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 638030336,\n",
       "  'mem_cached': 683671552},\n",
       " {'layer_idx': 7,\n",
       "  'call_idx': 35,\n",
       "  'layer_type': 'AdaptiveAvgPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 644026880,\n",
       "  'mem_cached': 683671552},\n",
       " {'layer_idx': 6,\n",
       "  'call_idx': 36,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 667890176,\n",
       "  'mem_cached': 717225984},\n",
       " {'layer_idx': 5,\n",
       "  'call_idx': 37,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 646582784,\n",
       "  'mem_cached': 717225984},\n",
       " {'layer_idx': 4,\n",
       "  'call_idx': 38,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 582882816,\n",
       "  'mem_cached': 717225984},\n",
       " {'layer_idx': 3,\n",
       "  'call_idx': 39,\n",
       "  'layer_type': 'MaxPool2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 597169664,\n",
       "  'mem_cached': 815792128},\n",
       " {'layer_idx': 2,\n",
       "  'call_idx': 40,\n",
       "  'layer_type': 'ReLU',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 565319168,\n",
       "  'mem_cached': 815792128},\n",
       " {'layer_idx': 1,\n",
       "  'call_idx': 41,\n",
       "  'layer_type': 'Conv2d',\n",
       "  'exp': 'baseline',\n",
       "  'hook_type': 'bwd',\n",
       "  'mem_all': 475600896,\n",
       "  'mem_cached': 815792128}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(mem_log)\n",
    "plot_mem(df, exps=['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()\n",
    "del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Pytorch memory management example: Measuring peak GPU RAM memory usuage\n",
    "## Code from: https://discuss.pytorch.org/t/measuring-peak-memory-usage-tracemalloc-for-pytorch/34067/11\n",
    "## Docs at: https://pytorch.org/docs/stable/cuda.html#memory-management\n",
    "\n",
    "def consume_gpu_ram(n): return torch.ones((n, n)).cuda()\n",
    "def consume_gpu_ram_256mb(): return consume_gpu_ram(2**13)\n",
    "\n",
    "def b2mb(x): return int(x/2**20)\n",
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = b2mb(self.end-self.begin)\n",
    "        self.peaked = b2mb(self.peak-self.begin)\n",
    "        print(f\"delta used/peak {self.used:8d}/{self.peaked:8d}\")\n",
    "\n",
    "# push the process' peak gauge high up and then release all the memory\n",
    "# expecting 0 used / 1024 peaked\n",
    "with TorchTracemalloc() as tt:\n",
    "    z = [consume_gpu_ram_256mb() for i in range(4)] # 1GB\n",
    "    del z\n",
    "assert tt.used == 0 and tt.peaked == 1024\n",
    "\n",
    "# allocate, allocate, release half\n",
    "# expecting 256 used / 512 peaked\n",
    "with TorchTracemalloc() as tt:\n",
    "    # should be: 256 used, 512 peaked\n",
    "    c1 = consume_gpu_ram_256mb()\n",
    "    c2 = consume_gpu_ram_256mb()\n",
    "    del c1\n",
    "assert tt.used == 256 and tt.peaked == 512\n",
    "del c2 # reset for next test\n",
    "\n",
    "# allocate, allocate, release all\n",
    "# expecting 0 used / 512 peaked\n",
    "with TorchTracemalloc() as tt:\n",
    "    # should be: 0 used, 512 peaked\n",
    "    c1 = consume_gpu_ram_256mb()\n",
    "    c2 = consume_gpu_ram_256mb()\n",
    "    del c1, c2\n",
    "assert tt.used == 0 and tt.peaked == 512\n",
    "\n",
    "# allocate, don't release\n",
    "# expecting 1536 used / 1536 peaked\n",
    "with TorchTracemalloc() as tt:\n",
    "    z = [consume_gpu_ram_256mb() for i in range(6)]\n",
    "assert tt.used == 1536 and tt.peaked == 1536\n",
    "del z # reset for next test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = b2gb(self.end-self.begin)\n",
    "        self.peaked = b2gb(self.peak-self.begin)\n",
    "        print(f\"delta used/peak {self.used}/{self.peaked}\")\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "#with torch.no_grad():\n",
    "if 1:\n",
    "    initial_mem, _ =print_mem(0, 0)\n",
    "\n",
    "    batch_size = 30\n",
    "    inp_size = (batch_size, 3, 299, 299)\n",
    "    inp = torch.rand(inp_size).to(args.prof_gpu_id)\n",
    "    input_mem, _ = print_mem(0, 0)\n",
    "\n",
    "    convLayer1 = nn.Conv2d(3, 64*2, 5).to(args.prof_gpu_id)\n",
    "    convLayer2 = nn.Conv2d(64*2, 10, 3).to(args.prof_gpu_id)\n",
    "    convLayer1.requires_grad = True\n",
    "    convLayer2.requires_grad = True\n",
    "    mem_setup,_ = print_mem(0, 0)\n",
    "\n",
    "    print(\"Input mem size:\", input_mem-initial_mem)\n",
    "    print(\"Conv layer mem size:\", mem_setup-input_mem)\n",
    "    print(\"Net setup size:\", mem_setup)\n",
    "\n",
    "    with TorchTracemalloc() as tt:\n",
    "        out1 = convLayer1(inp)\n",
    "        out2 = convLayer2(out1)\n",
    "        #print(mem_setup + b2gb(sys.getsizeof(out.storage())))\n",
    "    print(tt.peaked - tt.used)\n",
    "\n",
    "    after_forward,_ = print_mem(0, 0)\n",
    "    print(\"After forward pass: \", after_forward)\n",
    "\n",
    "    print(\"Size of out:\", b2gb(sys.getsizeof(out1.storage())) + b2gb(sys.getsizeof(out2.storage())))\n",
    "\n",
    "    del out1\n",
    "    del out2\n",
    "    after_del_output,_ = print_mem(0, 0)\n",
    "    print(\"After deleteing output: \", after_del_output)\n",
    "\n",
    "\n",
    "    print(after_forward - after_del_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del convLayer1, convLayer2\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del out1, out2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "del_all()\n",
    "#print_mem(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU allocates memory in steps of 512 Bytes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del_all()\n",
    "print(torch.cuda.memory_allocated('cuda:0'))\n",
    "\n",
    "inp = torch.rand((1,1)).to(0)\n",
    "inp.dtype #torch.float32 => 4 bytes\n",
    "print(torch.cuda.memory_allocated('cuda:0')) #Increases by 512 bytes\n",
    "\n",
    "inp = torch.rand((1,128)).to(0)\n",
    "print(torch.cuda.memory_allocated('cuda:0')) # still 512 bytes\n",
    "\n",
    "inp = torch.rand((1,129)).to(0)\n",
    "print(torch.cuda.memory_allocated('cuda:0')) # increases to 1024\n",
    "\n",
    "# It takes 128 float32 (4 bytes) to fill 512 bytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak and used memory in forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "        print(f\"delta used/peak {self.used}/{self.peaked}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_temp():\n",
    "    try:\n",
    "        del fc1, fc2, fc3\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del inp\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del out\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del target\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del err\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure all variables have been deleted (as below) before running\n",
    "\n",
    "del_temp()\n",
    "## Note: All memory size values in output will be rounded to \n",
    "## nearest multiple of 512 as demoed in the example above\n",
    "\n",
    "print(\"Model Setup\")\n",
    "N   = 50\n",
    "with TorchTracemalloc() as tt:\n",
    "    fc1 = nn.Linear(N, 1000).to('cuda:0') # 1000*N*4 (4 is size of float32) =1000*50*4 = 200,000 \n",
    "    fc2 = nn.Linear(1000, 1000).to('cuda:0') #1000*1000*4 = 4,000,000\n",
    "    fc3 = nn.Linear(1000, 1000).to('cuda:0') #1000*1000*4 = 4,000,000\n",
    "\n",
    "print(\"\\nInput Setup\")\n",
    "inp_size = (1, N)\n",
    "with TorchTracemalloc() as tt:\n",
    "    inp = torch.rand(inp_size).to('cuda:0') #50*4 = 200\n",
    "\n",
    "print(\"\\nForward\")\n",
    "with TorchTracemalloc() as tt:\n",
    "    #with torch.no_grad():\n",
    "    if 1:\n",
    "        out = fc3(fc2(fc1(inp))) \n",
    "'''\n",
    "    Each stage:\n",
    "    fc1 output = 1000*4 = 4000\n",
    "    Same for fc2, fc3\n",
    "    \n",
    "    With grad i.e saving intermediates:\n",
    "    used_mem = peak_mem = 4000*3 = 12000\n",
    "    \n",
    "    Without grad\n",
    "    used_mem = 4000 (only final out)\n",
    "    peak_mem = 4000+4000 (Each stage computation involves 4000 at input and 4000 at ouput)\n",
    "    \n",
    "'''\n",
    "        \n",
    "print(\"\\nLabels Moved\")\n",
    "with TorchTracemalloc() as tt:\n",
    "    target = torch.rand((1,1000)).to('cuda:0') #4*1000=4000\n",
    "\n",
    "print(\"\\nLoss Computed\")\n",
    "with TorchTracemalloc() as tt:\n",
    "    loss_fn = nn.MSELoss()  \n",
    "    err = loss_fn(out, target)\n",
    "\n",
    "print(\"\\nBackward\")\n",
    "with TorchTracemalloc() as tt:\n",
    "    err.backward()  \n",
    "# Memory doubles \n",
    "# used_mem = peak_mem = size of all trainable paremeters\n",
    "\n",
    "print(\"\\nClear Gradients\")\n",
    "with TorchTracemalloc() as tt:\n",
    "    fc1.zero_grad(set_to_none=True)\n",
    "    fc2.zero_grad(set_to_none=True)\n",
    "    fc3.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "inp_size = (1, N)\n",
    "fc1 = nn.Linear(N, 1000).to('cuda:0') \n",
    "fc2 = nn.Linear(1000, 1000).to('cuda:0')\n",
    "fc3 = nn.Linear(1000, 1000).to('cuda:0')\n",
    "\n",
    "target = torch.rand((1,1000)).to('cuda:0')\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "forward_times =[]\n",
    "backward_times =[]\n",
    "\n",
    "for _ in range(20):\n",
    "    inp = torch.rand(inp_size).to('cuda:0')\n",
    "    out = fc3(fc2(fc1(inp)))\n",
    "    err = loss_fn(out, target)\n",
    "    err.backward()\n",
    "    \n",
    "for _ in range(100):\n",
    "    inp = torch.rand(inp_size).to('cuda:0')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    out = fc3(fc2(fc1(inp)))\n",
    "    stop_time = time.time()\n",
    "    forward_times.append(stop_time-start_time)\n",
    "    \n",
    "    err = loss_fn(out, target)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    err.backward()\n",
    "    stop_time = time.time()\n",
    "    backward_times.append(stop_time-start_time)\n",
    "    \n",
    "print(\"Forward times mean: \", np.mean(forward_times))\n",
    "print(\"Backward times mean: \", np.mean(backward_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Froward function modification to track time and memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "\n",
    "\n",
    "def _calculate_time(function, *input):\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    start_time = time.time()\n",
    "    result = function(*input)\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, result\n",
    "\n",
    "def _calculate_memory(function, *input):\n",
    "    with TorchTracemalloc() as tt:\n",
    "        torch.cuda.synchronize('cuda:0')\n",
    "        start_time = time.time()\n",
    "        result = function(*input)\n",
    "        torch.cuda.synchronize('cuda:0')\n",
    "        stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, tt.used, tt.peaked , result\n",
    "\n",
    "def fwd_wrapper(module, *input):\n",
    "    # dummy run\n",
    "    t, _ = _calculate_time(old_fwd, *input)\n",
    "    # Actual run\n",
    "    t_mem, used_mem, peak_mem, res = _calculate_memory(old_fwd, *input)\n",
    "    t, _ = _calculate_time(old_fwd, *input)\n",
    "    return t, t_mem, used_mem, peak_mem, res\n",
    "\n",
    "\n",
    "def _get_metrics(module, inp_size):\n",
    "    runtime =[]\n",
    "    mem_measurement_timeOverhead = []\n",
    "    # achieveing steady state\n",
    "    for _ in range(20):\n",
    "        inp = torch.rand(inp_size).to('cuda:0')\n",
    "        module(inp)\n",
    "    # Actual Runs\n",
    "    for _ in range(100):\n",
    "        inp = torch.rand(inp_size).to('cuda:0')\n",
    "        t, t_mem, used_mem, peak_mem, _ = module(inp)\n",
    "        runtime.append(t)\n",
    "        mem_measurement_timeOverhead.append(t_mem)\n",
    "\n",
    "   \n",
    "    print(\"***********************************\")\n",
    "    print(\"Mean Runtime:\", np.mean(runtime))\n",
    "    print(\"STD Runtime:\", np.std(runtime))\n",
    "    print(\"***********************************\")\n",
    "    print(\"Mean Runtime with memory measurement:\", np.mean(mem_measurement_timeOverhead))\n",
    "    print(\"Difference in percent:\", 100*(np.mean(mem_measurement_timeOverhead)-np.mean(runtime))/np.mean(runtime), \"%\")\n",
    "    print(\"***********************************\")\n",
    "    print(\"Used Memory:\",used_mem )\n",
    "    print(\"Peak Memory:\", peak_mem)\n",
    "\n",
    "# Linear Layer\n",
    "N = 50\n",
    "fc1 = nn.Linear(N, 1000).to('cuda:0')\n",
    "old_fwd = fc1.forward\n",
    "fc1.forward = fwd_wrapper.__get__(fc1, fc1.__class__)\n",
    "print(\"Linear layer:\")\n",
    "_get_metrics(fc1, (1,N))\n",
    "\n",
    "\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "old_fwd = conv1.forward\n",
    "conv1.forward = fwd_wrapper.__get__(conv1, conv1.__class__)\n",
    "print(\"\\n\\nConv layer:\")\n",
    "_get_metrics(conv1, (1, 3, 299, 299))\n",
    "\n",
    "\n",
    "'''\n",
    "    1.'Peak memory' maybe different than 'used memory' based on the layer\n",
    "      Eg: Linear layer they are same, but not in conv\n",
    "      \n",
    "    2. No significant overhead in measuring the memory usage \n",
    "       using torch.cuda.max_memory_allocated(). So time and memory\n",
    "       usuage can be measured in the same turn.\n",
    "       \n",
    "'''\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check 'used memory' is indeed output memory size for single layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Conv layer\n",
    "\n",
    "m0, _  = print_mem(0,0)\n",
    "conv2 = nn.Conv2d(3, 640, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "m1, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual model mem: \", m1-m0)\n",
    "estimate_model_size(conv2, 'GB\\n')\n",
    "\n",
    "inp = torch.rand((1, 3, 2990, 2990)).to('cuda:0')\n",
    "m2, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual input mem: \", m2-m1)\n",
    "estimate_input_size(inp, 'GB\\n')\n",
    "\n",
    "with TorchTracemalloc() as tt:\n",
    "    output = conv2(inp)\n",
    "print(tt.used, tt.peaked)\n",
    "out_size = estimate_input_size(output, 'B\\n')\n",
    "\n",
    "## Assert that output size is indeed \n",
    "assert (tt.used==512*np.ceil(out_size/512)) # since memory is allotted in blocks of 512B\n",
    "print(\"Peak and used difference:\", tt.peaked - tt.used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del output\n",
    "del conv2\n",
    "del inp\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For linear layer\n",
    "m0, _  = print_mem(0,0)\n",
    "N = 50000\n",
    "fc1 = nn.Linear(N, 1000).to('cuda:0')\n",
    "m1, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual model mem: \", m1-m0)\n",
    "estimate_model_size(fc1, 'GB\\n')\n",
    "\n",
    "inp = torch.rand((1,N)).to('cuda:0')\n",
    "m2, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual input mem: \", m2-m1)\n",
    "estimate_input_size(inp, 'GB\\n')\n",
    "\n",
    "with TorchTracemalloc() as tt:\n",
    "    output = fc1(inp)\n",
    "print(tt.used, tt.peaked)\n",
    "out_size = estimate_input_size(output, 'B\\n')\n",
    "assert (tt.used==512*np.ceil(out_size/512)) # since memory is allotted in blocks of 512B\n",
    "print(\"Peak and used difference:\", tt.peaked - tt.used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del output\n",
    "del fc1\n",
    "del inp\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using forward hooks vs forward wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Using forward hooks #########################\n",
    "\n",
    "## Setup\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "inp = torch.rand((1, 3, 299, 299)).to('cuda:0')\n",
    "\n",
    "start_time = 0;stop_time  = 0\n",
    "times = [start_time, stop_time]\n",
    "\n",
    "def print_pre(module, input):\n",
    "    #print(\"I am about to start the forward run!\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[0] = time.time()\n",
    "\n",
    "def print_post(module ,input, output):\n",
    "    #print(\"I am done with the forward\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[1] = time.time()\n",
    "\n",
    "## Register the hooks\n",
    "conv1.register_forward_pre_hook(print_pre)\n",
    "conv1.register_forward_hook(print_post)\n",
    "\n",
    "metrics = []\n",
    "for _ in range(500):\n",
    "    out1 = conv1(inp)\n",
    "    t = (times[1] - times[0])*1000.0\n",
    "    metrics.append(t)\n",
    "    \n",
    "print(\"*****************************************\")\n",
    "hook_mean = np.mean(metrics)\n",
    "print(\"Mean forward time with hooks:\", hook_mean)\n",
    "print(\"STD forward time with hooks:\",np.std(metrics))\n",
    "\n",
    "del conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Using forward Wrapper #################\n",
    "\n",
    "conv2 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "inp = torch.rand((1, 3, 299, 299)).to('cuda:0')\n",
    " \n",
    "start_time = 0\n",
    "stop_time  = 0\n",
    "times = [start_time, stop_time]\n",
    "\n",
    "## save original forward function\n",
    "original_forward = conv2.forward\n",
    "\n",
    "## define a wrapper around the original forward function\n",
    "def modified_forward(module, *input):\n",
    "    #print(\"I am about to start the forward run!\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[0] = time.time()\n",
    "    out = original_forward(module,*input)\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[1] = time.time()\n",
    "    #print(\"I am done with the forward\")\n",
    "    return out\n",
    "\n",
    "## set wrapper as the new forward \n",
    "conv2.forward = modified_forward\n",
    "\n",
    "## forward run\n",
    "metrics = []\n",
    "for _ in range(500):\n",
    "    out2 = conv2(inp)\n",
    "    t = (times[1] - times[0])*1000.0\n",
    "    #print(t)\n",
    "    metrics.append(t)\n",
    "print(\"*****************************************\")\n",
    "wrapper_mean = np.mean(metrics)\n",
    "print(\"Mean forward time with wrapper:\", wrapper_mean)\n",
    "print(\"STD forward time with wrapper:\",np.std(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent difference:\", 100*(wrapper_mean - hook_mean)/hook_mean, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Thus using forward hooks or foward wrapper seem to be equivalent.\n",
    "As confirmed here on pytorch forum: \n",
    "https://discuss.pytorch.org/t/using-forward-hooks-vs-modifying-the-forward-function/129923\n",
    "\n",
    "Using forward wrapper offers advantage of being able to use a \n",
    "tracing context (eg: TorchTracemalloc() defined above) to measure \n",
    "peak memory usage\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out1\n",
    "del inp\n",
    "del conv2\n",
    "del out2\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding '.grad' and gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gradient of parameters is accumulated in .grad attriute of parameters\n",
    "'''\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "conv2 = nn.Conv2d(64, 192, kernel_size=11, stride=4, padding=2)\n",
    "\n",
    "inp = torch.rand((1, 3, 299, 299), requires_grad=True)\n",
    "\n",
    "target  =  torch.rand((1, 192, 17, 17))\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_forward():\n",
    "    out1 = conv1(inp)\n",
    "    out2 = conv2(out1)\n",
    "    return out1, out2\n",
    "\n",
    "def go_backward(out):\n",
    "    err     = loss_fn(out, target)\n",
    "    err.backward()\n",
    "    \n",
    "def observe_a_grad():\n",
    "    param =  list(conv2.parameters())\n",
    "    pp = (param[0].grad)             ## .grad attribute accumulates the gradient\n",
    "    print(pp[0][0][0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient is accumulated in the .grad of the parameters with requires_grad=True\n",
    "out1, out2 = go_forward()\n",
    "go_backward(out2)\n",
    "observe_a_grad()\n",
    "\n",
    "out1, out2 = go_forward()\n",
    "go_backward(out2)\n",
    "observe_a_grad()  # grad becomes 2X\n",
    "\n",
    "out1, out2 = go_forward()\n",
    "go_backward(out2)\n",
    "observe_a_grad() # grad becomes 3X\n",
    "\n",
    "## Reset gradient\n",
    "conv2.zero_grad()\n",
    "\n",
    "out1, out2 = go_forward()\n",
    "go_backward(out2)\n",
    "observe_a_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: .grad is only accumulated for parameters and leaf tensors with requires_grad=True \n",
    "(like we have set of inp). \n",
    ".grad is not retained for intermediate tensors like out1, out2 - since they are not required\n",
    "to make updates (when optimizer.step will be called). If it needs to be retained you need\n",
    "to set retain_grad for the tensor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del conv1, conv2, out1, out2, target, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding '.grad_fn' and '.grad_fn.next_functions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To visualize the grad_fn and the computation graph, refer to:\n",
    "https://colab.research.google.com/github/szagoruyko/pytorchviz/blob/master/examples.ipynb#scrollTo=wNo1AxRfNmF1\n",
    "'''\n",
    "n1 = 2\n",
    "n2 = 3\n",
    "n3 = 2\n",
    "conv1 = nn.Conv2d(n1, n2, kernel_size=2)\n",
    "conv2 = nn.Conv2d(n2, n3, kernel_size=2)\n",
    "\n",
    "#inp = torch.rand((1, n1, 299, 299), requires_grad=True)\n",
    "inp = torch.rand((1, n1, 299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = conv1(inp)\n",
    "out2 = conv2(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(conv1.parameters())) \n",
    "# Contains n2 kernels of size (2*2)*n1 and bias vector of size 1*n2 (one per output channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out2.grad_fn) # Conv2 layer backward\n",
    "print('*'*20)\n",
    "print(out2.grad_fn.next_functions) # Conv1, accumulatedGrad of W of conv2, accumulatedGrad of bias of conv2\n",
    "print('*'*20)\n",
    "print(out1.grad_fn) # Conv1 layer backward\n",
    "print('*'*20)\n",
    "print(out1.grad_fn.next_functions) # Input (None if requires_grad is False for inp, else accumulatedgrad for inp), accumulatedGrad of W, bias of conv1\n",
    "print('*'*20)\n",
    "print(inp.grad_fn) # none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc  = nn.Linear(3, 3, bias=True).to('cuda:0')\n",
    "inp = torch.rand((1, 3)).to('cuda:0')\n",
    "out = fc(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.grad_fn) # Linear backward -> AddmmBackward\n",
    "print('*'*20)\n",
    "print(out.grad_fn.next_functions)\n",
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(fc.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recur functions to move layers to GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = sm.toyToyModel(factor=3)\n",
    "#inp_size = (3, 299, 299)\n",
    "\n",
    "#model = sm.linearModel(factor=3)\n",
    "#inp_size = (1, 1, 10000)\n",
    "\n",
    "model = sm.parallelToyModel(factor=3)\n",
    "inp_size = (1, 3, 299, 299)\n",
    "\n",
    "#model = sm.toyModel(factor=1)\n",
    "#inp_size = (1, 3, 299, 299)\n",
    "\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a randoom assigniment of gpus to layers for testing\n",
    "\n",
    "gpu_assignment = {}\n",
    "\n",
    "def _recur_assign_gpu_to_layers(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            _recur_assign_gpu_to_layers(sub_module)\n",
    "    else:\n",
    "        gpu_assignment[id(module)]= np.random.randint(3)\n",
    "        print(module, \" : \", gpu_assignment[id(module)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recur_assign_gpu_to_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test that alloting layers to GPUs actually ooccupies the memory in those GPUs\n",
    "## Note: Minor difference in layer size and actual memory occupied is due to\n",
    "## pytorch alloting memory in hucks of 512 Bytes as demonstrated earlier\n",
    "\n",
    "def test_recur_move_layers_to_gpus(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            test_recur_move_layers_to_gpus(sub_module)\n",
    "    else:\n",
    "        gpu_id = gpu_assignment[id(module)]\n",
    "        mem0, _ = print_mem(gpu_id, cached=0, unit='B')\n",
    "        module.to(gpu_id)\n",
    "        mem1, _ = print_mem(gpu_id, cached=0, unit='B')\n",
    "        print(\"Module:              \", module)\n",
    "        print(\"GPU:                 \", gpu_id)\n",
    "        print(\"Memory change:       \", mem1-mem0)\n",
    "        print(\"Layer size:          \", estimate_model_size(module, unit='B', to_print=False))\n",
    "        print(\"Net memory occupied: \", mem1)\n",
    "        print(\"*\"*50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recur_move_layers_to_gpus(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()\n",
    "del inp\n",
    "del model\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify forwards to match the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = sm.toyToyModel(factor=3)\n",
    "#inp_size = (3, 299, 299)\n",
    "\n",
    "#model = sm.linearModel(factor=3)\n",
    "#inp_size = (1, 1, 10000)\n",
    "\n",
    "model = sm.parallelToyModel(factor=3)\n",
    "inp_size = (1, 3, 299, 299)\n",
    "\n",
    "#model = sm.toyModel(factor=1)\n",
    "#inp_size = (1, 3, 299, 299)\n",
    "\n",
    "#model = sm.smallModel()\n",
    "#inp_size = (1, 3, 5, 5)\n",
    "\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Manual  device assignment. Skip next two steps if doing this\n",
    "## For sm.parallelToyModel\n",
    "gpu_assignment = {}\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['features'][0])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['features'][1])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['features'][2])] = 1\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['avgpool'])] = 0\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier1'][0])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier1'][1])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier1'][2])] = 1\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier2'][0])] = 2\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier2'][1])] = 2\n",
    "gpu_assignment[id(model.__dict__['_modules']['classifier2'][2])] = 2\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['concatenateFinal'])] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a randoom assigniment of gpus to layers for testing\n",
    "\n",
    "gpu_assignment = {}\n",
    "\n",
    "def _recur_assign_gpu_to_layers(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            _recur_assign_gpu_to_layers(sub_module)\n",
    "    else:\n",
    "        gpu_assignment[id(module)]= np.random.randint(3)\n",
    "        print(module, \" : \", gpu_assignment[id(module)])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recur_assign_gpu_to_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = id(model.__dict__['_modules']['features'][0])\n",
    "#id_model = id(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## TO DO\n",
    "Note: It is important to keep no reference to any temporary variable to prevent memory clogging\n",
    "eg: inp.\n",
    "So delete them\n",
    "'''\n",
    "\n",
    "original_forwards = {}\n",
    "\n",
    "def recur_move_layers_to_gpus(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            recur_move_layers_to_gpus(sub_module)\n",
    "    else:\n",
    "        \n",
    "        module_id = id(module)\n",
    "        gpu_id = gpu_assignment[module_id]\n",
    "        \n",
    "        ############################################################\n",
    "        ### Move layers to the allotted GPUs\n",
    "        module.to(gpu_id)\n",
    "        ## For Testing #############################################\n",
    "        #mem0, _ = print_mem(gpu_id, cached=0, unit='B')\n",
    "        #module.to(gpu_id)\n",
    "        #mem1, _ = print_mem(gpu_id, cached=0, unit='B')\n",
    "        #print(\"Module:              \", module)\n",
    "        #print(\"GPU:                 \", gpu_id)\n",
    "        #print(\"Memory change:       \", mem1-mem0)\n",
    "        #print(\"Layer size:          \", estimate_model_size(module, unit='B', to_print=False))\n",
    "        #print(\"Net memory occupied: \", mem1)\n",
    "        #print(\"*\"*50)\n",
    "        ##########################################################\n",
    "        \n",
    "        original_forwards[module_id] = module.forward\n",
    "        \n",
    "        def modified_forward(self, *inputs):\n",
    "            print(self)\n",
    "            input_list = list(inputs)\n",
    "            for i, inp in enumerate(input_list):\n",
    "                if isinstance(inp, torch.Tensor):\n",
    "                    input_list[i] = inp.to(gpu_id)\n",
    "                else:\n",
    "                    print(\"Input not a Tensor!\") ## Fix this\n",
    "            inputs = tuple(input_list)\n",
    "            output = original_forwards[module_id](*inputs)\n",
    "            return output\n",
    "        \n",
    "        module.forward =  modified_forward.__get__(module, module.__class__)  \n",
    "\n",
    "def reset_forward_functions(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            reset_forward_functions(sub_module)\n",
    "    else:\n",
    "        module_id = id(module)\n",
    "        module.forward = original_forwards[module_id]\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_move_layers_to_gpus(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Running entire model in single GPU.\n",
    "### First reset the forward functions, move all layers to gpu:0 using\n",
    "### recur_move_layers_to_gpus, and reset the forward functions \n",
    "### (since recur_move_layers_to_gpus would have modified it)\n",
    "\n",
    "reset_forward_functions(model)\n",
    "for module_id in gpu_assignment.keys():\n",
    "    gpu_assignment[module_id] = 0\n",
    "recur_move_layers_to_gpus(model)\n",
    "reset_forward_functions(model)\n",
    "\n",
    "\n",
    "inp = inp.to(0)\n",
    "output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory();print('*'*50)\n",
    "del inp\n",
    "del model\n",
    "del original_forwards\n",
    "del output\n",
    "\n",
    "#### pytorch should immediately garbage collect as memntioned here:\n",
    "#### https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277/2\n",
    "#### However, we observe that allotted memory is fully cleared only after using gc.collect\n",
    "print_gpu_memory();print('*'*50)\n",
    "gc.collect()\n",
    "print_gpu_memory();print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To check if an object id exists and w\n",
    "#print(ctypes.cast(id_test, ctypes.py_object).value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure overhead of moving inputs in modified_forward vs natively in model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forwards = {}\n",
    "\n",
    "def recur_move_layers_to_gpus(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            recur_move_layers_to_gpus(sub_module)\n",
    "    else:\n",
    "        \n",
    "        module_id = id(module)\n",
    "        gpu_id = gpu_assignment[module_id]\n",
    "        \n",
    "        ### Move layers to the allotted GPUs\n",
    "        module.to(gpu_id)\n",
    "\n",
    "        \n",
    "        original_forwards[module_id] = module.forward\n",
    "        \n",
    "        def modified_forward(self, *inputs):\n",
    "            #print(self)\n",
    "            input_list = list(inputs)\n",
    "            for i, inp in enumerate(input_list):\n",
    "                if isinstance(inp, torch.Tensor):\n",
    "                    input_list[i] = inp.to(gpu_id)\n",
    "                else:\n",
    "                    print(\"Input not a Tensor!\") ## Fix this\n",
    "            inputs = tuple(input_list)\n",
    "            output = original_forwards[module_id](*inputs)\n",
    "            return output\n",
    "        \n",
    "        module.forward =  modified_forward.__get__(module, module.__class__)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super(TwoLayerLinearModel, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.fc2 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc3 = nn.Linear(self.linear3N, self.linear4N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_split(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super(TwoLayerLinearModel_split, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to('cuda:0')\n",
    "        self.fc2 = nn.Linear(self.linear2N, self.linear3N).to('cuda:1')\n",
    "        self.fc3 = nn.Linear(self.linear3N, self.linear4N).to('cuda:2')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.fc1(x)\n",
    "        x = x.to('cuda:1')\n",
    "        x = self.fc2(x)\n",
    "        x = x.to('cuda:2')\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 5\n",
    "inp_size = (1, 512*factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using recur to distribute model and using modified_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerLinearModel(factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_assignment = {}\n",
    "original_forwards = {}\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc1'])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc2'])] = 2\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc3'])] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_move_layers_to_gpus(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = []\n",
    "\n",
    "for _ in range(50):\n",
    "    inp   = torch.rand(inp_size)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    #print(\"Time Taken: \", 1000*(end-start))\n",
    "    time1.append(1000*(end-start))\n",
    "\n",
    "print(\"Mean time taken:\", np.mean(time1))\n",
    "mean_modified_forward = np.mean(time1[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory();print('*'*50)\n",
    "del model\n",
    "del output\n",
    "del inp\n",
    "del original_forwards\n",
    "del gpu_assignment\n",
    "gc.collect()\n",
    "print_gpu_memory();print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natively split model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerLinearModel_split(factor=5)\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = []\n",
    "\n",
    "for _ in range(50):\n",
    "    inp   = torch.rand(inp_size)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    #inp = inp.to(0)\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    time2.append(1000*(end-start))\n",
    "\n",
    "print(\"Mean time taken:\", np.mean(time2))\n",
    "mean_native = np.mean(time2[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using modified forward to distribute inputs time taken is: \", mean_modified_forward)\n",
    "print(\"Using natively distributed model, time taken is: \", mean_native)\n",
    "print(\"Percent difference: \", 100*( mean_modified_forward - mean_native)/mean_native)\n",
    "\n",
    "'''\n",
    "The overhead is not much. So implementation of recur_move_layers_to_gpus is atleast as good as\n",
    "defining the gpu assignments natively in the model definition itself\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory();print('*'*50)\n",
    "del model\n",
    "del output\n",
    "del inp\n",
    "gc.collect()\n",
    "print_gpu_memory();print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerLinearModel(factor=5).to(0)\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = []\n",
    "\n",
    "for _ in range(50):\n",
    "    inp   = torch.rand(inp_size)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    inp = inp.to(0)\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    time2.append(1000*(end-start))\n",
    "\n",
    "print(\"Mean time taken:\", np.mean(time2))\n",
    "mean_native = np.mean(time2[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does setting parallel streams for moving the input and output give any advantage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forwards = {}\n",
    "\n",
    "def stream_move_layers_to_gpus(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    if len(sub_modules) > 0:\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            stream_move_layers_to_gpus(sub_module)\n",
    "    else:\n",
    "        \n",
    "        module_id = id(module)\n",
    "        gpu_id = gpu_assignment[module_id]\n",
    "        \n",
    "        ### Move layers to the allotted GPUs\n",
    "        module.to(gpu_id)\n",
    "        \n",
    "        original_forwards[module_id] = module.forward\n",
    "        \n",
    "        def modified_forward(self, *inputs):\n",
    "            #print(self)\n",
    "            with torch.cuda.stream(COMPUTE_STREAM[gpu_id]):\n",
    "                input_list = list(inputs)\n",
    "                for i, inp in enumerate(input_list):\n",
    "                    if isinstance(inp, torch.Tensor):\n",
    "                        input_list[i] = inp.to(gpu_id)\n",
    "                    else:\n",
    "                        print(\"Input not a Tensor!\") ## Fix this\n",
    "                inputs = tuple(input_list)\n",
    "            \n",
    "            with torch.cuda.stream(COMPUTE_STREAM[gpu_id]):\n",
    "                output = original_forwards[module_id](*inputs)\n",
    "            return output\n",
    "        \n",
    "        module.forward =  modified_forward.__get__(module, module.__class__)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerLinearModel(factor=1)\n",
    "\n",
    "gpu_assignment = {}\n",
    "original_forwards = {}\n",
    "\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc1'])] = 0\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc2'])] = 1\n",
    "gpu_assignment[id(model.__dict__['_modules']['fc3'])] = 2\n",
    "\n",
    "stream_move_layers_to_gpus(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "for _ in range(50):\n",
    "    inp   = torch.rand(inp_size)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    #print(\"Time Taken: \", 1000*(end-start))\n",
    "    times.append(1000*(end-start))\n",
    "\n",
    "print(\"Mean time taken:\", np.mean(times[10:]))\n",
    "\n",
    "'''\n",
    "Parallel streams doesn't seem to give any advantages\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory();print('*'*50)\n",
    "del model\n",
    "del output\n",
    "del inp\n",
    "gc.collect()\n",
    "print_gpu_memory();print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does .to(device) work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code : https://discuss.pytorch.org/t/python-and-c-source-code-for-moving-tensors-from-cpu-to-gpu/78702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reference: https://stackoverflow.com/questions/59560043/what-is-the-difference-between-model-todevice-and-model-model-todevice\n",
    "- For modules .to is in place.\n",
    "So in a = b.to(0), a and b will point to same model residing in 0\n",
    "- For tensors it is not,  it makes a copy\n",
    "In a = b.to(0), a and b are two seperate copies with b on cpu and a on gpu\n",
    "You need to do b = b.to(0) to replace the original\n",
    "'''\n",
    "\n",
    "inp_size = (1, 3, 1000, 1000)\n",
    "inp   = torch.rand(inp_size)\n",
    "\n",
    "print(\"Without Reassigning nothing happens\")\n",
    "inp.to(0)\n",
    "print(inp.get_device())\n",
    "print_gpu_memory();print(\"#\"*50)\n",
    "\n",
    "print(\"There will be two inp copies, one in cpu (inp) and one in gpu(inp1)\")\n",
    "inp1 = inp.to(0)\n",
    "print(inp.get_device())\n",
    "print(inp1.get_device())\n",
    "print_gpu_memory();print(\"#\"*50)\n",
    "del inp1\n",
    "\n",
    "print(\"With reassigning, inp resides in gpu. inp is deleted from cpu\")\n",
    "inp = inp.to(0)\n",
    "print(inp.get_device())\n",
    "print_gpu_memory();print(\"#\"*50)\n",
    "\n",
    "print(\"inp will be deleted from gpu 0 and it will appear in gpu1\")\n",
    "inp = inp.to(1)\n",
    "print(inp.get_device())\n",
    "print_gpu_memory();print(\"#\"*50)\n",
    "\n",
    "del inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo 2\n",
    "inp_size = (1, 3, 1000, 1000)\n",
    "inp   = torch.rand(inp_size)\n",
    "\n",
    "def mov_inp(inp):\n",
    "    print(inp.get_device())\n",
    "    inp = inp.to(3)\n",
    "    print(inp.get_device())\n",
    "    return inp\n",
    "\n",
    "mov_inp(inp)\n",
    "print(\"Without reassigning:\", inp.get_device())\n",
    "print(\"'inp' in the gpu goes out of scope when the function ends\")\n",
    "print()\n",
    "\n",
    "inp = mov_inp(inp)\n",
    "print(\"With reassigning:\", inp.get_device())\n",
    "\n",
    "del inp\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual model split working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTracemalloc():\n",
    "    def __init__(self, gpu_id):\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated(self.gpu_id)\n",
    "        torch.cuda.reset_max_memory_allocated(self.gpu_id) # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated(self.gpu_id)\n",
    "        self.peak = torch.cuda.max_memory_allocated(self.gpu_id)\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "        print(\"TorchTrace Summary:\")\n",
    "        print(\"GPU: \",self.gpu_id );print(\"Peak :\", self.peaked);print(\"Used :\", self.used);print(\"Diff :\", self.peaked-self.used);print('*'*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple tensor transfers to gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_ini, _ = print_mem(0,0,'B');\n",
    "print(\"GPU0 Initial:\", m0_ini);print('*'*50)\n",
    "\n",
    "with TorchTracemalloc(0) as tt:\n",
    "    inp1   = torch.rand((1,1000)).to('cuda:0')\n",
    "\n",
    "m0, _ = print_mem(0,0,'B'); \n",
    "print(\"GPU0:\", m0-m0_ini);print('*'*50)\n",
    "    \n",
    "with TorchTracemalloc(0) as tt:\n",
    "    inp2   = torch.rand((1,2000)).to('cuda:0')\n",
    "\n",
    "m0, _ = print_mem(0,0,'B'); \n",
    "print(\"GPU0 Final:\", m0);print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inp1\n",
    "del inp2\n",
    "print_mem(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model on one gpu vs  Splitting across GPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 512\n",
    "N2 = 2048\n",
    "N3 = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.net1 = torch.nn.Linear(N1, N2)\n",
    "        self.net2 = torch.nn.Linear(N2, N3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        m0_ini, _ = print_mem(0,0,'B'); \n",
    "        print(\"GPU0 Initial:\", m0_ini);print('*'*50)\n",
    "        \n",
    "        x = self.net1(x)\n",
    "        \n",
    "        print(\"Output of first layer\")\n",
    "        m0, _ = print_mem(0,0,'B');  \n",
    "        print(\"GPU0:\", m0-m0_ini);print('*'*50)\n",
    "        \n",
    "        x = self.net2(x)\n",
    "        \n",
    "        print(\"Output of layer 1 consumed. Output of second layer generated\")\n",
    "        m0, _ = print_mem(0,0,'B');\n",
    "        print(\"GPU0 overall change:\", m0-m0_ini);print('*'*50)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1,N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model transfer.\")\n",
    "with TorchTracemalloc(0) as tt:\n",
    "    modelT = TestModel().to('cuda:0')\n",
    "\n",
    "print(\"Input transfer\")\n",
    "with TorchTracemalloc(0) as tt:\n",
    "    inp   = torch.rand(inp_size).to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forward run. Without no_grad, used_mem becomes equal to peak, since intermediate outputs are stored\")\n",
    "with TorchTracemalloc(0) as tt:\n",
    "    with torch.no_grad():\n",
    "    #if 1:\n",
    "        out = modelT(inp)\n",
    "'''\n",
    "Peak memory (12288) is sum of mem of out1(8192) and out2(4096). Thus the input  to a layer(output of previous layer)\n",
    "and output of the layer coexist momentarily. Finally only out2 remains (as reflected in used_mem)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = estimate_input_size(inp, 'B')\n",
    "m2 = estimate_model_size(modelT, 'B')\n",
    "m3 = estimate_input_size(out, 'B')\n",
    "print(m1+m2+m3)\n",
    "_,_=print_mem(0,1,'B')\n",
    "'''\n",
    "The two are equal with no_grad (else need to add intermediate activations)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inp\n",
    "del modelT\n",
    "del out\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test split model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1,N1)\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer model, with layer 1 on gpu0 and layer 2 on gpu1\n",
    "class TestModel_split(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel_split, self).__init__()\n",
    "        \n",
    "        m0_ini, _ = print_mem(0,0,'B'); m1_ini, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0 Initial:\", m0_ini);print(\"GPU1 Initial:\", m1_ini);print('*'*50)\n",
    "        \n",
    "        self.net1 = torch.nn.Linear(N1, N2).to('cuda:0')\n",
    "        \n",
    "        print(\"Moved layer 1 to gpu0\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "        \n",
    "        self.net2 = torch.nn.Linear(N2, N3).to('cuda:1')\n",
    "        \n",
    "        print(\"Moved layer 2 to gpu1\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m0_ini, _ = print_mem(0,0,'B'); m1_ini, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0 Initial:\", m0_ini);print(\"GPU1 Initial:\", m1_ini);print('*'*50)\n",
    "        \n",
    "        x = x.to('cuda:0')\n",
    "        \n",
    "        print(\"Input moved to gpu0\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "        \n",
    "        x = self.net1(x)\n",
    "        \n",
    "        print(\"Layer 1 output generated on gpu0\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "        \n",
    "        x = x.to('cuda:1')\n",
    "        \n",
    "        print(\"Layer 1 output moved to gpu1\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "        \n",
    "        x = self.net2(x)\n",
    "        \n",
    "        print(\"Layer 1 output generated to gpu1\")\n",
    "        m0, _ = print_mem(0,0,'B'); m1, _ = print_mem(1,0,'B'); \n",
    "        print(\"GPU0:\", m0-m0_ini);print(\"GPU1:\", m1-m1_ini);print('*'*50)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Moving the model:\")\n",
    "with TorchTracemalloc(1) as tt1:\n",
    "    with TorchTracemalloc(0) as tt0:\n",
    "        modelT = TestModel_split()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forward run - With no_grad, no memory used in gpu0. In gpu1, memory is used_mem is that of new output generated\")\n",
    "with TorchTracemalloc(1) as tt1:\n",
    "    with TorchTracemalloc(0) as tt0:\n",
    "        with torch.no_grad():\n",
    "        #if 1:\n",
    "            #modelT = TestModel_split()\n",
    "            out = modelT(inp)\n",
    "        \n",
    "\n",
    "inp.get_device()\n",
    "'''\n",
    "Note: inp still resides in CPU. gpu0 has no used_mem because reference to it's inp\n",
    "is lost once the forward run is complete. And intermediate activations are not stored due to no_grad\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inp\n",
    "del modelT\n",
    "del out\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of CUDA Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base class\n",
    "\n",
    "class TwoLayerLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        #super(TwoLayerLinearModel, self).__init__()  # syntax in python2, works in python3\n",
    "        # Explained here: https://stackoverflow.com/questions/61288224/why-not-super-init-model-self-in-pytorch\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.fc2a = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze()\n",
    "        x = self.fc1(x)\n",
    "        xb = self.fc2b(x)\n",
    "        xa = self.fc2a(x)\n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inherited from TwoLayerLinearModel\n",
    "\n",
    "class TwoLayerLinearModel_singleGPU(TwoLayerLinearModel):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__(factor)\n",
    "\n",
    "        self.squeeze = self.squeeze.to('cuda:0')\n",
    "        self.fc1 = self.fc1.to('cuda:0')\n",
    "        self.fc2a = self.fc2a.to('cuda:0')\n",
    "        self.fc2b = self.fc2b.to('cuda:0')\n",
    "        self.concatenate = self.concatenate.to('cuda:0')\n",
    "        self.fc3 = self.fc3.to('cuda:0')\n",
    "        self.fc4 = self.fc4.to('cuda:0')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xb = self.fc2b(x)\n",
    "        xa = self.fc2a(x)\n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = 'CPU'\n",
    "test = 'singleGPU'\n",
    "#test = 'singleGPU_native' \n",
    "\n",
    "factor = 5\n",
    "inp_size = (128, 512*factor)\n",
    "\n",
    "if test == 'CPU'or test == 'singleGPU':\n",
    "    model = TwoLayerLinearModel(factor)\n",
    "\n",
    "if test == 'singleGPU':\n",
    "    model = model.to('cuda:0')\n",
    "    \n",
    "if test == 'singleGPU_native': \n",
    "    model = TwoLayerLinearModel_singleGPU(factor)\n",
    "\n",
    "\n",
    "times = []\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(500):\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        inp   = torch.rand(inp_size)\n",
    "        start = time.time()\n",
    "        if test == 'singleGPU' or test == 'singleGPU_native':\n",
    "            inp = inp.to('cuda:0')\n",
    "        output = model(inp)\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        end = time.time()\n",
    "        times.append(1000*(end-start))\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "print(\"Mean time taken:\", np.mean(times[10:]))\n",
    "print()\n",
    "\n",
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To view the json\n",
    "## SSHFS:\n",
    "#sshfs cshetty2@earhart.cs.illinois.edu:/home/cshetty2/sct/pytorch/basic_experiments remote\n",
    "\n",
    "\n",
    "## Now on chrome, type:\n",
    "## chrome://tracing/\n",
    "## Open from the local file connected to sshfs above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "\n",
    "Note: Reported numbers are specific to the GPU's used (GPU0 and GPU1)\n",
    "\n",
    "CPU test:  ~ 35sec  \\\n",
    "singleGPU: ~ 10.5 sec \\\n",
    "singleGPU_native: ~10.5sec \n",
    "\n",
    "Conclusions:\\\n",
    "For single GPU case, defining a model and transferring it as a whole to GPU is same as natively defining the layers to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_splitGPU(TwoLayerLinearModel):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__(factor)\n",
    "\n",
    "        self.squeeze = self.squeeze.to('cuda:0')\n",
    "        self.fc1 = self.fc1.to('cuda:0')\n",
    "        self.fc2a = self.fc2a.to('cuda:1')\n",
    "        self.fc2b = self.fc2b.to('cuda:0')\n",
    "        self.concatenate = self.concatenate.to('cuda:0')\n",
    "        self.fc3 = self.fc3.to('cuda:0')\n",
    "        self.fc4 = self.fc4.to('cuda:0')\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x1 = x.to('cuda:1')\n",
    "        xb = self.fc2b(x)\n",
    "        \n",
    "        xa = self.fc2a(x1)\n",
    "        xa = xa.to('cuda:0')\n",
    "        \n",
    "        \n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_splitGPU_2(TwoLayerLinearModel):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__(factor)\n",
    "\n",
    "        self.squeeze = self.squeeze.to('cuda:0')\n",
    "        self.fc1 = self.fc1.to('cuda:0')\n",
    "        self.fc2a = self.fc2a.to('cuda:1')\n",
    "        self.fc2b = self.fc2b.to('cuda:0')\n",
    "        self.concatenate = self.concatenate.to('cuda:1')\n",
    "        self.fc3 = self.fc3.to('cuda:1')\n",
    "        self.fc4 = self.fc4.to('cuda:1')\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x1 = x.to('cuda:1')\n",
    "        xb = self.fc2b(x)\n",
    "        \n",
    "        xa = self.fc2a(x1)\n",
    "        xb = xb.to('cuda:1')\n",
    "        \n",
    "        \n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_splitGPU_stream(TwoLayerLinearModel):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__(factor)\n",
    "\n",
    "        self.squeeze = self.squeeze.to('cuda:0')\n",
    "        self.fc1 = self.fc1.to('cuda:0')\n",
    "        self.fc2a = self.fc2a.to('cuda:1')\n",
    "        self.fc2b = self.fc2b.to('cuda:0')\n",
    "        self.concatenate = self.concatenate.to('cuda:0')\n",
    "        self.fc3 = self.fc3.to('cuda:0')\n",
    "        self.fc4 = self.fc4.to('cuda:0')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.to('cuda:0')\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "            x1 = x.to('cuda:1')\n",
    "        xb = self.fc2b(x)\n",
    "        \n",
    "        \n",
    "        xa = self.fc2a(x1)\n",
    "        \n",
    "        #if 1:\n",
    "        with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "            #if 1:\n",
    "            with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "                xa = xa.to('cuda:0')\n",
    "        \n",
    "       \n",
    "        \n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = 'singleGPU'\n",
    "#test = 'twoGPU'\n",
    "test = 'twoGPU-2'\n",
    "#test = 'twoGPU_stream'\n",
    "\n",
    "factor = 5\n",
    "inp_size = (128, 512*factor)\n",
    "\n",
    "if test == 'singleGPU':\n",
    "    model = TwoLayerLinearModel_singleGPU(factor)\n",
    "\n",
    "if test == 'twoGPU':\n",
    "    model = TwoLayerLinearModel_splitGPU(factor)\n",
    "\n",
    "if test == 'twoGPU-2':\n",
    "    model = TwoLayerLinearModel_splitGPU_2(factor)\n",
    "    \n",
    "if test == 'twoGPU_stream': \n",
    "    model = TwoLayerLinearModel_splitGPU_stream(factor)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "times = []\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(200):\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        inp   = torch.rand(inp_size)\n",
    "        start = time.time()\n",
    "        inp = inp.to('cuda:0')\n",
    "        output = model(inp)\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        end = time.time()\n",
    "        times.append(1000*(end-start))\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "print(\"Mean time taken:\", np.mean(times[10:]))\n",
    "print()\n",
    "\n",
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. In split GPU, forward fuction:\n",
    "        xb = self.fc2b(x)\n",
    "        x1 = x.to('cuda:1') ... ~12 msec\n",
    "        \n",
    "        x1 = x.to('cuda:1')\n",
    "        xb = self.fc2b(x) ... ~8.1msec\n",
    "[Not sure why changing order matters]\n",
    "\n",
    "But:\n",
    "        \n",
    "        xb = self.fc2b(x)\n",
    "        with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "            x1 = x.to('cuda:1') ... ~7.2ms  \n",
    "\n",
    "Similarly:\n",
    "        \n",
    "        x1 = x.to('cuda:1')\n",
    "        xa = self.fc2a(x1)\n",
    "        xa = xa.to('cuda:0')\n",
    "        xb = self.fc2b(x) \n",
    "        ... ~12msec\n",
    "TODO: Check profiler trace\n",
    "\n",
    "\n",
    "2. In splitGPU_stream, this set of changes give following improvements:\n",
    "\n",
    "        with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "            x1 = x.to('cuda:1')....~7.2 (from 8.1)\n",
    "        -> Note stream has to be on GPU0, the sending GPU\n",
    "        \n",
    "        with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "                xa = xa.to('cuda:0') ...~7.2(no change.same for stream0)\n",
    "                \n",
    "        BUT on combining the two streams:\n",
    "        \n",
    "        with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "            with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "                xa = xa.to('cuda:0') ... ~6.7 (from 7.2)\n",
    "                \n",
    "        [Note: same effect if stream0 is inside stream1's context]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "[Check the profiler and add explanation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Conflicts - Using streams safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = torch.rand((1000,1000), dtype = torch.double) # in cpu - No problem with this\n",
    "\n",
    "## Define l on device 0\n",
    "l = torch.rand((1000,1000), dtype = torch.double).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "## Transfe to 1\n",
    "b = l.to(1)\n",
    "    \n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "# No sum diff if this is here\n",
    "# torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "    stream_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(stream_sum)\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(stream_sum-default_sum)\n",
    "\n",
    "del l\n",
    "del b, stream_sum\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()   # Doesnt have any impact\n",
    "#torch.cuda.empty_cache() #IF this is there the  the sum diff is always negative\n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNEXPLAINED!\n",
    "\n",
    "l = torch.rand((1000,1000), dtype = torch.double).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "    with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "        b = l.to(1)\n",
    "    \n",
    "with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "    stream_sum = torch.sum(b, dtype = torch.double)\n",
    "    \n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(stream_sum)\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(stream_sum-default_sum)\n",
    "\n",
    "## Here stream sum is equal to actual sum!!!\n",
    "\n",
    "del l\n",
    "del b, stream_sum\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache() #IF this is there the  the sum diff is always negative\n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING EFFECT OF CACHE\n",
    "l = (n*torch.ones((10000,10000), dtype = torch.double)).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[0]):   # No probblem if there's only one of the two streams\n",
    "    with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "        b = l.to(1)\n",
    "\n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(default_sum-actual_sum.to(1))\n",
    "\n",
    "del l\n",
    "del b,\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache() \n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n",
    "n=1-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "When a tensor is being transferred for GPU0 to GPU1, the tensor is transfer doesnt happen at once. Instead memory for that tensor is reserved in GPU1, and only then the transfer starts. The reservation maybe done as one of this:\n",
    "\n",
    "\n",
    "1) If a tensor by that name existed on GPU1  before, then values from that are used if a parallel stream tries to access the variable before the transfer is complete (like the torch.sum sees stale values form previous verison of the 'b')\n",
    "\n",
    "\n",
    "2) If the tensor did not exist, then a zero tensor of that name is created and used\n",
    "\n",
    "Unexplained:\n",
    "\n",
    "- Why is both streams required to see the anamoly? Just having anyone works fine\n",
    "- Why transfers from CPU work fine? (Maybe CPU transfers do not involve streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe use of streams conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a fork, where output of layer L1 is needed by L2 and L3, the correct way to use stream is:\n",
    "\n",
    "\"To allow parallel L1->L2 and L1->L3 transfer, AFTER the forward of L1 has completed and BEFORE forwards on L2 and L3 start\"\n",
    "\n",
    "Thus transfer must be initiated from (modified) forward of L2 and L3, since they are triggered AFTER L1 is complete. And these modified forwards of L2, L3 must each create a new stream on L1's device to get the required input. The transfer should happen on the default stream at L2 and L3 since the transfer must be complete before the their forwards may start.\n",
    "\n",
    "Any other way can be errorneous. Eg1: Starting a stream on L1's devices to transfer out its output may start transferring partial output. \n",
    "Eg2: Having a stream on L2/L3's device to recieve the input may result in their forwards acting on incomplete inputs (it is unecessarly as well, since we need stream on the sender device during transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nrun = 11\n",
    "Nsize = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.3638687133789\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "label = torch.rand((Nsize,Nsize)).to(0)\n",
    "for _ in range(Nrun):\n",
    "\n",
    "    m01 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m02 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m11 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    m12 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1);\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        \n",
    "        start = time.time()\n",
    "        out1 = torch.matmul(m11,m12)\n",
    "\n",
    "        out0 = torch.matmul(m01,m02)\n",
    "        \n",
    "        out1 = out1.to(0)\n",
    "        \n",
    "        \n",
    "        out = torch.matmul(out1,out0 ) \n",
    "        out.backward(gradient=label)\n",
    "\n",
    "        end = time.time()\n",
    "    times.append(1000*(end-start))\n",
    "prof.export_chrome_trace(\"timeTest.json\")\n",
    "    \n",
    "print(np.mean(times[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.67961764335632\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "label = torch.rand((Nsize,Nsize)).to(0)\n",
    "for _ in range(Nrun):\n",
    "    \n",
    "    m01 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m02 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m11 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    m12 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    out0 = torch.matmul(m01,m02)\n",
    "    out1 = torch.matmul(m11,m12)\n",
    "    out1 = out1.to(0)\n",
    "    out = out1 + out0  \n",
    "    out.backward(gradient=label)\n",
    "    \n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1);\n",
    "    end = time.time()\n",
    "    times.append(1000*(end-start))\n",
    "    \n",
    "print(np.mean(times[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.02333641052246\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "label = torch.rand((Nsize,Nsize)).to(0)\n",
    "for _ in range(Nrun):\n",
    "    \n",
    "    m01 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m02 = torch.rand((Nsize,Nsize), requires_grad=True).to(0)\n",
    "    m11 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    m12 = torch.rand((Nsize,Nsize), requires_grad=True).to(1)\n",
    "    \n",
    "    start.record()\n",
    "    \n",
    "    out0 = torch.matmul(m01,m02)\n",
    "    out1 = torch.matmul(m11,m12)\n",
    "    out1 = out1.to(0)\n",
    "    out = out1 + out0  \n",
    "    out.backward(gradient=label)\n",
    "    \n",
    "    end.record()\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1);\n",
    "    times.append(start.elapsed_time(end))\n",
    "    \n",
    "print(np.mean(times[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making most of a GPU - using streams within a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a models memory and compute requirements can be met within one GPU, then there should be no case where distributing it over multiple nodes will benefit \\\n",
    "\n",
    "But for this to hold, the NN model must be written in such a way that max parallelism is exploited. For instance two parallel branches must be executed as two seperate streams. \n",
    "\n",
    "However in pytorch, this does not seem to give any advantage. This has been observed by others else where as well: Pythorch streams sont seem to execute concurrently\n",
    "\n",
    "eg: https://github.com/pytorch/pytorch/issues/48279\n",
    "\n",
    "On the other hand, Average GPU utilization seems very less, around 10pc: https://towardsdatascience.com/measuring-actual-gpu-usage-for-deep-learning-training-e2bf3654bcfd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run_gpu = 0\n",
    "stream1 = torch.cuda.Stream(device=0)\n",
    "stream2 = torch.cuda.Stream(device=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "class _addLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 51*self.factor\n",
    "        self.linear2N = 20*self.factor\n",
    "        self.linear3N = 51*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 51*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        \n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a3 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a4 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a5 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a6 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a7 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        \n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b3 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b4 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b5 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b6 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b7 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        \n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.add1 = _addLayer()\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa1 = self.fc2a2(xa1)\n",
    "        xa1 = self.fc2a3(xa1)\n",
    "        xa1 = self.fc2a4(xa1)\n",
    "        xa1 = self.fc2a5(xa1)\n",
    "        xa1 = self.fc2a6(xa1)\n",
    "        xa2 = self.fc2a7(xa1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb1 = self.fc2b2(xb1)\n",
    "        xb1 = self.fc2b3(xb1)\n",
    "        xb1 = self.fc2b4(xb1)\n",
    "        xb1 = self.fc2b5(xb1)\n",
    "        xb1 = self.fc2b6(xb1)\n",
    "        xb2 = self.fc2b7(xb1)\n",
    "        \n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.add1(y,xb2)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_stream(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 51*self.factor\n",
    "        self.linear2N = 20*self.factor\n",
    "        self.linear3N = 51*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 51*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        \n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a3 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a4 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a5 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a6 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2a7 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        \n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b3 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b4 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b5 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b6 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b7 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        \n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.add1 = _addLayer()\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.cuda.stream(stream1):\n",
    "            x = self.squeeze(x)\n",
    "            x = self.fc1(x)\n",
    "        torch.cuda.synchronize(0);\n",
    "        with torch.cuda.stream(stream2):\n",
    "            xb1 = self.fc2b1(x)\n",
    "            xb1 = self.fc2b2(xb1)\n",
    "            xb1 = self.fc2b3(xb1)\n",
    "            xb1 = self.fc2b4(xb1)\n",
    "            xb1 = self.fc2b5(xb1)\n",
    "            xb1 = self.fc2b6(xb1)\n",
    "            xb2 = self.fc2b7(xb1)\n",
    "        with torch.cuda.stream(stream1):\n",
    "            xa1 = self.fc2a1(x)\n",
    "            xa1 = self.fc2a2(xa1)\n",
    "            xa1 = self.fc2a3(xa1)\n",
    "            xa1 = self.fc2a4(xa1)\n",
    "            xa1 = self.fc2a5(xa1)\n",
    "            xa1 = self.fc2a6(xa1)\n",
    "            xa2 = self.fc2a7(xa1)\n",
    "        torch.cuda.synchronize(0);\n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.add1(y,xb2)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 5\n",
    "inp_size_single = (1, 51*factor)\n",
    "#model = TwoLayerLinearModel_stream(factor)\n",
    "model = TwoLayerLinearModel(factor)\n",
    "opt_size = 5120\n",
    "batch_size = '12800'\n",
    "\n",
    "inp_size = (int(batch_size),) + inp_size_single\n",
    "model = model.to(single_run_gpu)\n",
    "run_type = \"forward\"\n",
    "Nrun = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im done\n",
      "Im out\n",
      "Mean time taken: 1.7347246468186617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for nrun in range(Nrun):\n",
    "            #print(nrun)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu)\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "        print(\"Im done\")\n",
    "    print(\"Im out\")\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "class _addLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class allOnesModel_singleDevice(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.linear1N = 512\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear1N)\n",
    "        #torch.nn.init.constant_(self.fc1.weight,1/512)\n",
    "        torch.nn.init.zeros_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.linear1N, self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(self.linear1N, self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc3.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = (1, 512)\n",
    "model = allOnesModel_singleDevice()\n",
    "out = model(torch.ones(inp_size))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "\n",
    "class allOnesModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.linear1N = N\n",
    "        self.k = 0*1/N\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, 10*self.linear1N).to('cuda:0')\n",
    "        torch.nn.init.constant_(self.fc1.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2 = nn.Linear(10*self.linear1N, self.linear1N).to('cuda:0')\n",
    "        torch.nn.init.constant_(self.fc2.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(self.linear1N, self.linear1N).to('cuda:1')\n",
    "        torch.nn.init.constant_(self.fc3.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1, N)\n",
    "model = allOnesModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "GPU: 0\n",
      "GPU: 1\n"
     ]
    }
   ],
   "source": [
    "forward_func ={}\n",
    "stream1s ={}\n",
    "stream2s ={}\n",
    "for mod_name in model.__dict__['_modules']:\n",
    "    module = model.__dict__['_modules'][mod_name]\n",
    "    forward_func[mod_name] = module.forward\n",
    "    gpu_id = module.weight.get_device()\n",
    "    stream1s[mod_name] = torch.cuda.Stream(gpu_id)\n",
    "    stream2s[mod_name] = torch.cuda.Stream(gpu_id)\n",
    "    print(\"GPU:\", gpu_id)\n",
    "    \n",
    "    def modified_forward(self, *inputs):\n",
    "        input_list = list(inputs)\n",
    "        for i, inp in enumerate(input_list):\n",
    "            if isinstance(inp, torch.Tensor):\n",
    "                #if 1:\n",
    "                #with torch.cuda.stream(torch.cuda.Stream(device=inp.get_device())):  ## Stream must be setup on the sending node (see experiment.ipynb, section \"Use of CUDA Streams\")\n",
    "                with torch.cuda.stream(stream1s[mod_name]):\n",
    "                    input_list[i] = inp.to(gpu_id)\n",
    "            else:\n",
    "                print(\"Input not a Tensor!\")\n",
    "        inputs = tuple(input_list)\n",
    "        ########################################################\n",
    "        with torch.cuda.stream(stream2s[mod_name]):\n",
    "            output = forward_func[mod_name](*inputs) \n",
    "\n",
    "        return output\n",
    "\n",
    "    module.forward =  modified_forward.__get__(module, module.__class__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp   = torch.ones(inp_size)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    out   = model(inp)\n",
    "    torch.cuda.synchronize(0); torch.cuda.synchronize(1)\n",
    "prof.export_chrome_trace(\"allOnesModel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, N) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.linear1N = N\n",
    "        self.k = 1*1/N\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, 10*self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc1.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2 = nn.Linear(10*self.linear1N, 5*self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc2.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc3.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "        self.fc4 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "        torch.nn.init.constant_(self.fc4.weight, self.k)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        \n",
    "        self.fc5 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "\n",
    "        \n",
    "        self.fc6 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "\n",
    "        \n",
    "        self.fc7 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "\n",
    "        \n",
    "        self.fc8 = nn.Linear(5*self.linear1N, self.linear1N)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #start = time.time()\n",
    "\n",
    "        #x = x.to(self.layers[0])\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        #x = x.to(self.layers[1])\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #x = x.to(self.layers[2])\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        #x = x.to(self.layers[3])\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc8(x)\n",
    "        #print((time.time()-start)*1000)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 264\n",
    "inp_size = (10, N)\n",
    "model1 = dummyModel(N).to(0)\n",
    "model2 = dummyModel(N).to(0)\n",
    "stream1s = torch.cuda.Stream(device=0)\n",
    "stream2s = torch.cuda.Stream(device=0)\n",
    "inp1   = torch.rand(inp_size).to(0)\n",
    "inp2   = torch.rand(inp_size).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15524530410766602\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(20):\n",
    "        outa   = model1(inp1)\n",
    "        outb   = model2(inp2)\n",
    "torch.cuda.synchronize(0); \n",
    "end = time.time()\n",
    "print((end-start))\n",
    "prof.export_chrome_trace(\"dummyrun.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2467820644378662\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    with torch.cuda.stream(stream1s):\n",
    "        for _ in range(20):\n",
    "            outa   = model1(inp1)\n",
    "    for _ in range(20):\n",
    "        outb   = model2(inp2)\n",
    "torch.cuda.synchronize(0); \n",
    "end = time.time()\n",
    "print((end-start))\n",
    "prof.export_chrome_trace(\"dummyrun.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min = Optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_size(model, unit='MB', to_print = True): \n",
    "    \n",
    "    no_gpus = torch.cuda.device_count()\n",
    "    gpu_wise_mem = [0 for _ in range(no_gpus+1)] #Last is for cpu\n",
    "    layerwise_mem = []\n",
    "    \n",
    "    persistent_memory = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_mem = param.element_size() * param.nelement()\n",
    "        layerwise_mem.append(layer_mem)\n",
    "        gpu_wise_mem[param.get_device()] += layer_mem\n",
    "        \n",
    "    if unit == 'GB':\n",
    "        gb_mem = [round(x/1024**3,8) for x in gpu_wise_mem]\n",
    "        layer_mem = [round(x/1024**3,8) for x in layerwise_mem]\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem, layer_mem\n",
    "    elif unit == 'B':\n",
    "        gb_mem = gpu_wise_mem\n",
    "        layer_mem = layerwise_mem\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\",gb_mem, \"Bytes\")\n",
    "        return gb_mem, layer_mem\n",
    "    else:\n",
    "        mb_mem = [round(x/1024**2,8) for x in gpu_wise_mem]\n",
    "        layer_mem = [round(x/1024**2,8) for x in layerwise_mem]\n",
    "        if to_print:\n",
    "            print(\"Estimated Model Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem, layer_mem\n",
    "    \n",
    "def estimate_input_size(inp, unit='MB'):\n",
    "    input_size = 0\n",
    "    if isinstance(inp, torch.Tensor): \n",
    "        input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "    if isinstance(inp, list): \n",
    "        for sub_inp in inp:\n",
    "            if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "\n",
    "    input_size = input_size*torch.rand((1,1)).element_size() # multiply by 4\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(input_size/1024**3,8)\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    if unit == 'B':\n",
    "        gb_mem = input_size\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"B\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(input_size/1024**2,8)\n",
    "        print(\"Estimated Input Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "    \n",
    "def get_gpu_mem(gpu_id, unit = \"GB\", printOut = True):\n",
    "    total = torch.cuda.get_device_properties(gpu_id).total_memory - 1085000000 #1085MB python libs\n",
    "    reserved = torch.cuda.memory_reserved(gpu_id) # cache\n",
    "    allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "    if unit == \"GB\":\n",
    "        total = round(total/1024**3,8)\n",
    "        reserved = round(reserved/1024**3,8)\n",
    "        allocated = round(allocated/1024**3,8)\n",
    "    if printOut:\n",
    "        print(total, reserved, allocated)\n",
    "    return total, reserved, allocated\n",
    "\n",
    "def free_gpu_mem(unit = \"GB\"):\n",
    "    no_gpu = torch.cuda.device_count()\n",
    "    free_mem = [0 for _ in range(no_gpu)]\n",
    "    for gpu_id in range(no_gpu):\n",
    "        t,r,a = get_gpu_mem(gpu_id, unit, False)\n",
    "        free_mem[gpu_id] = t-a\n",
    "    return free_mem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "        #print(f\"delta used/peak {self.used}/{self.peaked}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size(), obj.get_device())\n",
    "            del obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, N, layers) -> None:\n",
    " \n",
    "        super().__init__() # python 3 syntaxa \n",
    "        \n",
    "        self.linear1N = N\n",
    "        self.k = 1*1/N\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.linear1N, 10*self.linear1N)\n",
    "        self.fc2 = nn.Linear(10*self.linear1N, 5*self.linear1N)\n",
    "        self.fc3 = nn.Linear(5*self.linear1N, 5*self.linear1N)\n",
    "        self.fc4 = nn.Linear(5*self.linear1N, self.linear1N)\n",
    "        \n",
    "        if layers:\n",
    "            self.fc1 = self.fc1.to(self.layers[0])\n",
    "            torch.nn.init.constant_(self.fc1.weight, self.k)\n",
    "            torch.nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "            self.fc2 = self.fc2.to(self.layers[1])\n",
    "            torch.nn.init.constant_(self.fc2.weight, self.k)\n",
    "            torch.nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "\n",
    "            self.fc3 = self.fc3.to(self.layers[2])\n",
    "            torch.nn.init.constant_(self.fc3.weight, self.k)\n",
    "            torch.nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "            self.fc4 = self.fc4.to(self.layers[3])\n",
    "            torch.nn.init.constant_(self.fc4.weight, self.k)\n",
    "            torch.nn.init.zeros_(self.fc4.bias)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.layers:\n",
    "            x = x.to(self.layers[0])\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        if self.layers:\n",
    "            x = x.to(self.layers[1])\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if self.layers:\n",
    "            x = x.to(self.layers[2])\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        if self.layers:\n",
    "            x = x.to(self.layers[3])\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 4200\n",
    "N2 = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mem estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Model Memory: [0.0, 0.0, 0.0, 0.0, 12.07040995] GB\n",
      "Net mem required:  12.07040996\n",
      "Layer-wise mem required: [1.3413280300000001, 6.7056343, 3.3528730299999996, 0.6705745999999999]\n",
      "Available Memory: [2.84165925, 4.81322354, 6.78474206, 6.78437585]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = dummyModel(N1, [0,0,1,1])\n",
    "\n",
    "model = dummyModel(N2, None)\n",
    "\n",
    "gpu_mem, layer_mem = estimate_model_size(model, 'GB')\n",
    "\n",
    "print(\"Net mem required: \", sum(layer_mem))\n",
    "print(\"Layer-wise mem required:\", [sum(x) for x in zip(layer_mem[0::2], layer_mem[1::2])] ) #layer mem gives weight and bias alternatingly\n",
    "print(\"Available Memory:\", free_gpu_mem())\n",
    "\n",
    "del model, model1\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0039,  0.0067, -0.0008,  ..., -0.0040,  0.0064,  0.0069],\n",
      "        [ 0.0040,  0.0116, -0.0038,  ..., -0.0005,  0.0020,  0.0086],\n",
      "        [ 0.0102,  0.0030, -0.0105,  ...,  0.0070, -0.0062, -0.0101],\n",
      "        ...,\n",
      "        [-0.0056, -0.0126,  0.0041,  ..., -0.0013, -0.0049,  0.0008],\n",
      "        [ 0.0068, -0.0011, -0.0034,  ...,  0.0076, -0.0043, -0.0029],\n",
      "        [ 0.0128, -0.0118,  0.0087,  ...,  0.0045, -0.0128, -0.0012]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0072,  0.0042,  0.0033,  ..., -0.0009,  0.0083, -0.0033],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.6967e-03,  4.4900e-04, -3.9076e-03,  ...,  4.9258e-04,\n",
      "         -6.8259e-04,  3.8209e-03],\n",
      "        [ 3.2340e-03,  4.6680e-04, -7.0850e-04,  ..., -2.8218e-03,\n",
      "         -1.0721e-03,  1.0080e-04],\n",
      "        [ 2.5294e-03, -3.4114e-03,  3.3583e-03,  ...,  1.3601e-03,\n",
      "         -3.2448e-03,  2.4895e-03],\n",
      "        ...,\n",
      "        [-3.9112e-03,  1.5714e-03,  1.9504e-03,  ..., -6.1351e-04,\n",
      "          2.9626e-03,  1.1507e-03],\n",
      "        [-1.0148e-03, -2.3356e-04, -1.9648e-03,  ...,  2.6283e-03,\n",
      "         -1.7302e-03,  1.2251e-03],\n",
      "        [ 2.0970e-03, -1.4134e-03,  1.0144e-05,  ...,  3.4123e-03,\n",
      "          3.4010e-03,  1.9401e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0020, -0.0038,  0.0036,  ...,  0.0024, -0.0022,  0.0026],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0003,  0.0040,  0.0013,  ...,  0.0036, -0.0032, -0.0024],\n",
      "        [ 0.0057,  0.0020, -0.0047,  ...,  0.0039,  0.0036, -0.0007],\n",
      "        [ 0.0028, -0.0016, -0.0048,  ...,  0.0056,  0.0030,  0.0002],\n",
      "        ...,\n",
      "        [-0.0045,  0.0015, -0.0050,  ..., -0.0036,  0.0003,  0.0046],\n",
      "        [ 0.0045, -0.0031, -0.0043,  ..., -0.0023,  0.0014, -0.0051],\n",
      "        [-0.0007,  0.0025, -0.0010,  ..., -0.0019, -0.0018, -0.0025]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0034,  0.0056, -0.0025,  ...,  0.0045, -0.0051, -0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.7599e-03, -1.0481e-03, -4.8413e-03,  ...,  3.3393e-03,\n",
      "          2.4927e-03,  1.5600e-04],\n",
      "        [-3.7489e-04, -1.5965e-03, -2.4753e-04,  ..., -2.9700e-03,\n",
      "         -1.9052e-03,  2.8029e-04],\n",
      "        [ 5.7567e-03,  3.2385e-03, -3.4996e-03,  ..., -3.7319e-03,\n",
      "          3.3225e-03, -3.5825e-03],\n",
      "        ...,\n",
      "        [ 1.2659e-03, -4.7326e-03, -3.3014e-03,  ..., -3.9948e-03,\n",
      "          4.2198e-03,  5.5732e-03],\n",
      "        [-3.4920e-03,  1.3374e-03,  1.0313e-03,  ...,  2.4555e-03,\n",
      "         -4.0182e-03,  8.2550e-04],\n",
      "        [ 3.1620e-03, -6.7382e-04, -7.5499e-05,  ...,  1.0500e-04,\n",
      "         -2.1701e-03, -1.0193e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0043, -0.0004, -0.0015,  ...,  0.0032,  0.0032,  0.0041],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = dummyModel(N2, None)\n",
    "for name, param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size1 = (256, N1)\n",
    "inp_size2 = (256, N2)\n",
    "\n",
    "s11=(torch.cuda.Stream(device=0))\n",
    "s12=(torch.cuda.Stream(device=1))\n",
    "\n",
    "s21=(torch.cuda.Stream(device=0))\n",
    "s22=(torch.cuda.Stream(device=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.71 GiB (GPU 1; 7.80 GiB total capacity; 1.34 GiB already allocated; 5.39 GiB free; 1.34 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dd4fa115dbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6d22e30c79e6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N, layers)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.71 GiB (GPU 1; 7.80 GiB total capacity; 1.34 GiB already allocated; 5.39 GiB free; 1.34 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model1 = dummyModel(N1, [0,0,0,0])\n",
    "model2 = dummyModel(N2, [1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081.328392028809\n",
      "165.1151180267334\n",
      "170.52316665649414\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    \n",
    "    inp1   = torch.rand(inp_size1)\n",
    "    inp2   = torch.rand(inp_size2)\n",
    "    \n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    #with TorchTracemalloc() as tt:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.stream(s11):\n",
    "                with torch.cuda.stream(s12):\n",
    "                    out1 = model1(inp1)\n",
    "            with torch.cuda.stream(s21):\n",
    "                with torch.cuda.stream(s22):\n",
    "                    out2 = model2(inp2)\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "\n",
    "    print((end-start)*1000)\n",
    "    prof.export_chrome_trace(\"simultaneous.json\")\n",
    "\n",
    "#print(b2gb(tt.used))\n",
    "#print(b2gb(tt.peaked))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n = 5\n",
    "for _ in range(3):\n",
    "    \n",
    "    inp1   = torch.rand((n, inp_size1[0], inp_size1[1]))\n",
    "    inp2   = torch.rand((n, inp_size2[0], inp_size2[1]))\n",
    "    \n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    #with TorchTracemalloc() as tt:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.stream(s11):\n",
    "                with torch.cuda.stream(s12):\n",
    "                    for k1 in range(n):\n",
    "                        out1 = model1(inp1[k1])\n",
    "            with torch.cuda.stream(s21):\n",
    "                with torch.cuda.stream(s22):\n",
    "                    for k2 in range(n):\n",
    "                        out2 = model2(inp2[k2])\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "\n",
    "    print((end-start)*1000)\n",
    "    prof.export_chrome_trace(\"simultaneous.json\")\n",
    "\n",
    "#print(b2gb(tt.used))\n",
    "#print(b2gb(tt.peaked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([45000, 4500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([45000]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500, 45000]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500, 22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4500, 22500]) -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:151: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "source": [
    "del model1, model2\n",
    "del inp1,inp2\n",
    "del out1, out2\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size1 = (256, N1)\n",
    "inp_size2 = (256, N2)\n",
    "\n",
    "s11=(torch.cuda.Stream(device=0))\n",
    "s12=(torch.cuda.Stream(device=1))\n",
    "\n",
    "s21=(torch.cuda.Stream(device=0))\n",
    "s22=(torch.cuda.Stream(device=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3101.433038711548\n",
      "320.71542739868164\n",
      "310.26554107666016\n"
     ]
    }
   ],
   "source": [
    "model1 = dummyModel(N1, [0,0,0,0])\n",
    "model2 = dummyModel(N2, [1,1,1,0])\n",
    "\n",
    "for _ in range(3):\n",
    "    inp1   = torch.ones(inp_size1)\n",
    "    inp2   = torch.ones(inp_size2)\n",
    "    \n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    #with TorchTracemalloc() as tt:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.stream(s11):\n",
    "                with torch.cuda.stream(s12):\n",
    "                    out1 = model1(inp1)\n",
    "            #del model1,inp1,out1\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "            #get_gpu_mem(0);get_gpu_mem(1)\n",
    "            with torch.cuda.stream(s21):\n",
    "                with torch.cuda.stream(s22):\n",
    "                    out2 = model2(inp2)\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    #del model2,inp2,out2\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    #get_gpu_mem(0);get_gpu_mem(1)\n",
    "    \n",
    "\n",
    "    print((end-start)*1000)\n",
    "    prof.export_chrome_trace(\"separate.json\")\n",
    "\n",
    "    #print(b2gb(tt.used))\n",
    "    #print(b2gb(tt.peaked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4264.286518096924\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 674.00 MiB (GPU 0; 7.80 GiB total capacity; 6.30 GiB already allocated; 270.31 MiB free; 6.34 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-866317c1f53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minp1\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_size1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_size1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minp2\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_size2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_size2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6d22e30c79e6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N, layers)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 674.00 MiB (GPU 0; 7.80 GiB total capacity; 6.30 GiB already allocated; 270.31 MiB free; 6.34 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "model1 = dummyModel(N1, [0,0,0,0])\n",
    "model2 = dummyModel(N2, [1,1,1,0])\n",
    "for _ in range(3):\n",
    "    inp1   = torch.rand((n, inp_size1[0], inp_size1[1]))\n",
    "    inp2   = torch.rand((n, inp_size2[0], inp_size2[1]))\n",
    "    \n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    start = time.time()\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    #with TorchTracemalloc() as tt:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.stream(s11):\n",
    "                with torch.cuda.stream(s12):\n",
    "                    for k1 in range(n):\n",
    "                        out1 = model1(inp1[k1])\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "            #del model1,inp1,out1\n",
    "            #get_gpu_mem(0);get_gpu_mem(1)\n",
    "            with torch.cuda.stream(s21):\n",
    "                with torch.cuda.stream(s22):\n",
    "                    for k2 in range(n):\n",
    "                        out2 = model2(inp2[k2])\n",
    "    torch.cuda.synchronize(0);torch.cuda.synchronize(1)\n",
    "    end = time.time()\n",
    "    #del model2,inp2,out2\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    #get_gpu_mem(0);get_gpu_mem(1)\n",
    "    \n",
    "\n",
    "    print((end-start)*1000)\n",
    "    prof.export_chrome_trace(\"separate.json\")\n",
    "\n",
    "    #print(b2gb(tt.used))\n",
    "    #print(b2gb(tt.peaked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1,inp1,out1\n",
    "del model2,inp2,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([45000, 4500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([45000]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500, 45000]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500, 22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([22500]) -1\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4500, 22500]) -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch nn Dimension bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update: It is a known problem that can bbe fixed by updating to pytorch 1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The faulty model\n",
    "class testModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    " \n",
    "        super().__init__() #Python3 syntax\n",
    "        \n",
    "        self.fc1 = nn.Linear(10, 10)\n",
    "        self.fc2 = nn.Linear(5,1)   # The fault\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 5x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7291849911d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b5083dac1aee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x1)"
     ]
    }
   ],
   "source": [
    "## This does not work as expected\n",
    "inp   = torch.ones((1,10))\n",
    "model = testModel()\n",
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works!\n",
    "inp   = torch.ones((1,10)).to('cuda:0')\n",
    "model = testModel().to('cuda:0')\n",
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2434]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(1000,3)\n",
    "embedding(torch.LongTensor([[999,4,5],[10,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.PAD_TOKEN = '<pad>'\n",
    "        self.UNK_TOKEN = '<unk>'\n",
    "        self.BOS_TOKEN = '<s>'\n",
    "        self.EOS_TOKEN = '<\\s>'\n",
    "\n",
    "        # special PAD, UNKNOWN, BEGIN-OF-STRING, END-OF-STRING tokens\n",
    "        self.PAD, self.UNK, self.BOS, self.EOS = [0, 1, 2, 3]\n",
    "\n",
    "        # path to the moses detokenizer, relative to the data directory\n",
    "        self.DETOKENIZER = 'mosesdecoder/scripts/tokenizer/detokenizer.perl'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_(lstm, init_weight=0.1):\n",
    "    \"\"\"\n",
    "    Initializes weights of LSTM layer.\n",
    "    Weights and biases are initialized with uniform(-init_weight, init_weight)\n",
    "    distribution.\n",
    "    :param lstm: instance of torch.nn.LSTM\n",
    "    :param init_weight: range for the uniform initializer\n",
    "    \"\"\"\n",
    "    # Initialize hidden-hidden weights\n",
    "    init.uniform_(lstm.weight_hh_l0.data, -init_weight, init_weight)\n",
    "    # Initialize input-hidden weights:\n",
    "    init.uniform_(lstm.weight_ih_l0.data, -init_weight, init_weight)\n",
    "\n",
    "    # Initialize bias. PyTorch LSTM has two biases, one for input-hidden GEMM\n",
    "    # and the other for hidden-hidden GEMM. Here input-hidden bias is\n",
    "    # initialized with uniform distribution and hidden-hidden bias is\n",
    "    # initialized with zeros.\n",
    "    init.uniform_(lstm.bias_ih_l0.data, -init_weight, init_weight)\n",
    "    init.zeros_(lstm.bias_hh_l0.data)\n",
    "\n",
    "    if lstm.bidirectional:\n",
    "        init.uniform_(lstm.weight_hh_l0_reverse.data, -init_weight, init_weight)\n",
    "        init.uniform_(lstm.weight_ih_l0_reverse.data, -init_weight, init_weight)\n",
    "\n",
    "        init.uniform_(lstm.bias_ih_l0_reverse.data, -init_weight, init_weight)\n",
    "        init.zeros_(lstm.bias_hh_l0_reverse.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualRecurrentEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder with Embedding, LSTM layers, residual connections and optional\n",
    "    dropout.\n",
    "    The first LSTM layer is bidirectional and uses variable sequence length\n",
    "    API, the remaining (num_layers-1) layers are unidirectional. Residual\n",
    "    connections are enabled after third LSTM layer, dropout is applied on\n",
    "    inputs to LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n",
    "                 batch_first=False, embedder=None, init_weight=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the ResidualRecurrentEncoder.\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param hidden_size: hidden size for LSTM layers\n",
    "        :param num_layers: number of LSTM layers, 1st layer is bidirectional\n",
    "        :param dropout: probability of dropout (on input to LSTM layers)\n",
    "        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n",
    "            if false the model uses (seq, batch, feature)\n",
    "        :param embedder: instance of nn.Embedding, if None constructor will\n",
    "            create new embedding layer\n",
    "        :param init_weight: range for the uniform initializer\n",
    "        \"\"\"\n",
    "        super(ResidualRecurrentEncoder, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        # 1st LSTM layer, bidirectional\n",
    "        self.rnn_layers.append(\n",
    "            nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True,\n",
    "                    batch_first=batch_first, bidirectional=True))\n",
    "\n",
    "        # 2nd LSTM layer, with 2x larger input_size\n",
    "        self.rnn_layers.append(\n",
    "            nn.LSTM((2 * hidden_size), hidden_size, num_layers=1, bias=True,\n",
    "                    batch_first=batch_first))\n",
    "\n",
    "        # Remaining LSTM layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.rnn_layers.append(\n",
    "                nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True,\n",
    "                        batch_first=batch_first))\n",
    "\n",
    "        for lstm in self.rnn_layers:\n",
    "            init_lstm_(lstm, init_weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "        else:\n",
    "            self.embedder = nn.Embedding(vocab_size, hidden_size,\n",
    "                                         padding_idx=config.PAD)\n",
    "            nn.init.uniform_(self.embedder.weight.data, -init_weight,\n",
    "                             init_weight)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        Execute the encoder.\n",
    "        :param inputs: tensor with indices from the vocabulary\n",
    "        :param lengths: vector with sequence lengths (excluding padding)\n",
    "        returns: tensor with encoded sequences\n",
    "        \"\"\"\n",
    "        x = self.embedder(inputs)\n",
    "\n",
    "        # bidirectional layer\n",
    "        x = self.dropout(x)\n",
    "        x = pack_padded_sequence(x, lengths.cpu().numpy(),\n",
    "                                 batch_first=self.batch_first)\n",
    "        x, _ = self.rnn_layers[0](x)\n",
    "        x, _ = pad_packed_sequence(x, batch_first=self.batch_first)\n",
    "\n",
    "        # 1st unidirectional layer\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.rnn_layers[1](x)\n",
    "\n",
    "        # the rest of unidirectional layers,\n",
    "        # with residual connections starting from 3rd layer\n",
    "        for i in range(2, len(self.rnn_layers)):\n",
    "            residual = x\n",
    "            x = self.dropout(x)\n",
    "            x, _ = self.rnn_layers[i](x)\n",
    "            x = x + residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualRecurrentEncoder(50000, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0717, -0.0024, -0.0480,  ...,  0.1015, -0.0585, -0.0175],\n",
       "         [ 0.0117,  0.0339, -0.0252,  ...,  0.2130, -0.0343,  0.0448],\n",
       "         [-0.0261,  0.0982,  0.1495,  ...,  0.2551,  0.0102,  0.0561]],\n",
       "\n",
       "        [[ 0.0418, -0.0378,  0.0030,  ...,  0.0964, -0.0223, -0.0040],\n",
       "         [ 0.0753, -0.0323,  0.0370,  ...,  0.0835, -0.0297,  0.0397],\n",
       "         [ 0.0809, -0.0238,  0.0371,  ...,  0.0864, -0.0449,  0.0608]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.LongTensor([[50000-1,4,5],[10,2,0]]), torch.tensor([3,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau Attention (https://arxiv.org/abs/1409.0473)\n",
    "    Implementation is very similar to tf.contrib.seq2seq.BahdanauAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, query_size, key_size, num_units, normalize=False,\n",
    "                 batch_first=False, init_weight=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the BahdanauAttention.\n",
    "        :param query_size: feature dimension for query\n",
    "        :param key_size: feature dimension for keys\n",
    "        :param num_units: internal feature dimension\n",
    "        :param normalize: whether to normalize energy term\n",
    "        :param batch_first: if True batch size is the 1st dimension, if False\n",
    "            the sequence is first and batch size is second\n",
    "        :param init_weight: range for uniform initializer used to initialize\n",
    "            Linear key and query transform layers and linear_att vector\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.batch_first = batch_first\n",
    "        self.num_units = num_units\n",
    "\n",
    "        self.linear_q = nn.Linear(query_size, num_units, bias=False)\n",
    "        self.linear_k = nn.Linear(key_size, num_units, bias=False)\n",
    "        nn.init.uniform_(self.linear_q.weight.data, -init_weight, init_weight)\n",
    "        nn.init.uniform_(self.linear_k.weight.data, -init_weight, init_weight)\n",
    "\n",
    "        self.linear_att = Parameter(torch.Tensor(num_units))\n",
    "\n",
    "        self.mask = None\n",
    "\n",
    "        if self.normalize:\n",
    "            self.normalize_scalar = Parameter(torch.Tensor(1))\n",
    "            self.normalize_bias = Parameter(torch.Tensor(num_units))\n",
    "        else:\n",
    "            self.register_parameter('normalize_scalar', None)\n",
    "            self.register_parameter('normalize_bias', None)\n",
    "\n",
    "        self.reset_parameters(init_weight)\n",
    "\n",
    "    def reset_parameters(self, init_weight):\n",
    "        \"\"\"\n",
    "        Sets initial random values for trainable parameters.\n",
    "        \"\"\"\n",
    "        stdv = 1. / np.sqrt(self.num_units)\n",
    "        self.linear_att.data.uniform_(-init_weight, init_weight)\n",
    "\n",
    "        if self.normalize:\n",
    "            self.normalize_scalar.data.fill_(stdv)\n",
    "            self.normalize_bias.data.zero_()\n",
    "\n",
    "    def set_mask(self, context_len, context):\n",
    "        \"\"\"\n",
    "        sets self.mask which is applied before softmax\n",
    "        ones for inactive context fields, zeros for active context fields\n",
    "        :param context_len: b\n",
    "        :param context: if batch_first: (b x t_k x n) else: (t_k x b x n)\n",
    "        self.mask: (b x t_k)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.batch_first:\n",
    "            max_len = context.size(1)\n",
    "        else:\n",
    "            max_len = context.size(0)\n",
    "\n",
    "        indices = torch.arange(0, max_len, dtype=torch.int64,\n",
    "                               device=context.device)\n",
    "        self.mask = indices >= (context_len.unsqueeze(1))\n",
    "\n",
    "    def calc_score(self, att_query, att_keys):\n",
    "        \"\"\"\n",
    "        Calculate Bahdanau score\n",
    "        :param att_query: b x t_q x n\n",
    "        :param att_keys: b x t_k x n\n",
    "        returns: b x t_q x t_k scores\n",
    "        \"\"\"\n",
    "\n",
    "        b, t_k, n = att_keys.size()\n",
    "        t_q = att_query.size(1)\n",
    "\n",
    "        att_query = att_query.unsqueeze(2).expand(b, t_q, t_k, n)\n",
    "        att_keys = att_keys.unsqueeze(1).expand(b, t_q, t_k, n)\n",
    "        sum_qk = att_query + att_keys\n",
    "\n",
    "        if self.normalize:\n",
    "            sum_qk = sum_qk + self.normalize_bias\n",
    "            linear_att = self.linear_att / self.linear_att.norm()\n",
    "            linear_att = linear_att * self.normalize_scalar\n",
    "        else:\n",
    "            linear_att = self.linear_att\n",
    "\n",
    "        out = torch.tanh(sum_qk).matmul(linear_att)\n",
    "        return out\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        \"\"\"\n",
    "        :param query: if batch_first: (b x t_q x n) else: (t_q x b x n)\n",
    "        :param keys: if batch_first: (b x t_k x n) else (t_k x b x n)\n",
    "        :returns: (context, scores_normalized)\n",
    "        context: if batch_first: (b x t_q x n) else (t_q x b x n)\n",
    "        scores_normalized: if batch_first (b x t_q x t_k) else (t_q x b x t_k)\n",
    "        \"\"\"\n",
    "\n",
    "        # first dim of keys and query has to be 'batch', it's needed for bmm\n",
    "        if not self.batch_first:\n",
    "            keys = keys.transpose(0, 1)\n",
    "            if query.dim() == 3:\n",
    "                query = query.transpose(0, 1)\n",
    "\n",
    "        if query.dim() == 2:\n",
    "            single_query = True\n",
    "            query = query.unsqueeze(1)\n",
    "        else:\n",
    "            single_query = False\n",
    "\n",
    "        b = query.size(0)\n",
    "        t_k = keys.size(1)\n",
    "        t_q = query.size(1)\n",
    "\n",
    "        # FC layers to transform query and key\n",
    "        processed_query = self.linear_q(query)\n",
    "        processed_key = self.linear_k(keys)\n",
    "\n",
    "        # scores: (b x t_q x t_k)\n",
    "        scores = self.calc_score(processed_query, processed_key)\n",
    "\n",
    "        if self.mask is not None:\n",
    "            mask = self.mask.unsqueeze(1).expand(b, t_q, t_k)\n",
    "            # I can't use -INF because of overflow check in pytorch\n",
    "            scores.masked_fill_(mask, -65504.0)\n",
    "\n",
    "        # Normalize the scores, softmax over t_k\n",
    "        scores_normalized = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Calculate the weighted average of the attention inputs according to\n",
    "        # the scores\n",
    "        # context: (b x t_q x n)\n",
    "        context = torch.bmm(scores_normalized, keys)\n",
    "\n",
    "        if single_query:\n",
    "            context = context.squeeze(1)\n",
    "            scores_normalized = scores_normalized.squeeze(1)\n",
    "        elif not self.batch_first:\n",
    "            context = context.transpose(0, 1)\n",
    "            scores_normalized = scores_normalized.transpose(0, 1)\n",
    "\n",
    "        return context, scores_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BahdanauAttention(1024,1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BahdanauAttention(\n",
       "  (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM wrapped with an attention module.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1024, context_size=1024, hidden_size=1024,\n",
    "                 num_layers=1, batch_first=False, dropout=0.2,\n",
    "                 init_weight=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the RecurrentAttention.\n",
    "        :param input_size: number of features in input tensor\n",
    "        :param context_size: number of features in output from encoder\n",
    "        :param hidden_size: internal hidden size\n",
    "        :param num_layers: number of layers in LSTM\n",
    "        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n",
    "            if false the model uses (seq, batch, feature)\n",
    "        :param dropout: probability of dropout (on input to LSTM layer)\n",
    "        :param init_weight: range for the uniform initializer\n",
    "        \"\"\"\n",
    "\n",
    "        super(RecurrentAttention, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, bias=True,\n",
    "                           batch_first=batch_first)\n",
    "        init_lstm_(self.rnn, init_weight)\n",
    "\n",
    "        self.attn = BahdanauAttention(hidden_size, context_size, context_size,\n",
    "                                      normalize=True, batch_first=batch_first)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, context, context_len):\n",
    "        \"\"\"\n",
    "        Execute RecurrentAttention.\n",
    "        :param inputs: tensor with inputs\n",
    "        :param hidden: hidden state for LSTM layer\n",
    "        :param context: context tensor from encoder\n",
    "        :param context_len: vector of encoder sequence lengths\n",
    "        :returns (rnn_outputs, hidden, attn_output, attn_scores)\n",
    "        \"\"\"\n",
    "        # set attention mask, sequences have different lengths, this mask\n",
    "        # allows to include only valid elements of context in attention's\n",
    "        # softmax\n",
    "        self.attn.set_mask(context_len, context)\n",
    "\n",
    "        inputs = self.dropout(inputs)\n",
    "        rnn_outputs, hidden = self.rnn(inputs, hidden)\n",
    "        attn_outputs, scores = self.attn(rnn_outputs, context)\n",
    "\n",
    "        return rnn_outputs, hidden, attn_outputs, scores\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, init_weight=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the Classifier.\n",
    "        :param in_features: number of input features\n",
    "        :param out_features: number of output features (size of vocabulary)\n",
    "        :param init_weight: range for the uniform initializer\n",
    "        \"\"\"\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Linear(in_features, out_features)\n",
    "        nn.init.uniform_(self.classifier.weight.data, -init_weight, init_weight)\n",
    "        nn.init.uniform_(self.classifier.bias.data, -init_weight, init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Execute the classifier.\n",
    "        :param x: output from decoder\n",
    "        \"\"\"\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualRecurrentDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with Embedding, LSTM layers, attention, residual connections and\n",
    "    optinal dropout.\n",
    "    Attention implemented in this module is different than the attention\n",
    "    discussed in the GNMT arxiv paper. In this model the output from the first\n",
    "    LSTM layer of the decoder goes into the attention module, then the\n",
    "    re-weighted context is concatenated with inputs to all subsequent LSTM\n",
    "    layers in the decoder at the current timestep.\n",
    "    Residual connections are enabled after 3rd LSTM layer, dropout is applied\n",
    "    on inputs to LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n",
    "                 batch_first=False, embedder=None, init_weight=0.1):\n",
    "        \"\"\"\n",
    "        Constructor of the ResidualRecurrentDecoder.\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param hidden_size: hidden size for LSMT layers\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param dropout: probability of dropout (on input to LSTM layers)\n",
    "        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n",
    "            if false the model uses (seq, batch, feature)\n",
    "        :param embedder: instance of nn.Embedding, if None constructor will\n",
    "            create new embedding layer\n",
    "        :param init_weight: range for the uniform initializer\n",
    "        \"\"\"\n",
    "        super(ResidualRecurrentDecoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.att_rnn = RecurrentAttention(hidden_size, hidden_size,\n",
    "                                          hidden_size, num_layers=1,\n",
    "                                          batch_first=batch_first,\n",
    "                                          dropout=dropout)\n",
    "\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.rnn_layers.append(\n",
    "                nn.LSTM(2 * hidden_size, hidden_size, num_layers=1, bias=True,\n",
    "                        batch_first=batch_first))\n",
    "\n",
    "        for lstm in self.rnn_layers:\n",
    "            init_lstm_(lstm, init_weight)\n",
    "\n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "        else:\n",
    "            self.embedder = nn.Embedding(vocab_size, hidden_size,\n",
    "                                         padding_idx=config.PAD)\n",
    "            nn.init.uniform_(self.embedder.weight.data, -init_weight,\n",
    "                             init_weight)\n",
    "\n",
    "        self.classifier = Classifier(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def init_hidden(self, hidden):\n",
    "        \"\"\"\n",
    "        Converts flattened hidden state (from sequence generator) into a tuple\n",
    "        of hidden states.\n",
    "        :param hidden: None or flattened hidden state for decoder RNN layers\n",
    "        \"\"\"\n",
    "        if hidden is not None:\n",
    "            # per-layer chunks\n",
    "            hidden = hidden.chunk(self.num_layers)\n",
    "            # (h, c) chunks for LSTM layer\n",
    "            hidden = tuple(i.chunk(2) for i in hidden)\n",
    "        else:\n",
    "            hidden = [None] * self.num_layers\n",
    "\n",
    "        self.next_hidden = []\n",
    "        return hidden\n",
    "\n",
    "    def append_hidden(self, h):\n",
    "        \"\"\"\n",
    "        Appends the hidden vector h to the list of internal hidden states.\n",
    "        :param h: hidden vector\n",
    "        \"\"\"\n",
    "        if self.inference:\n",
    "            self.next_hidden.append(h)\n",
    "\n",
    "    def package_hidden(self):\n",
    "        \"\"\"\n",
    "        Flattens the hidden state from all LSTM layers into one tensor (for\n",
    "        the sequence generator).\n",
    "        \"\"\"\n",
    "        if self.inference:\n",
    "            hidden = torch.cat(tuple(itertools.chain(*self.next_hidden)))\n",
    "        else:\n",
    "            hidden = None\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, inputs, context, inference=False):\n",
    "        \"\"\"\n",
    "        Execute the decoder.\n",
    "        :param inputs: tensor with inputs to the decoder\n",
    "        :param context: state of encoder, encoder sequence lengths and hidden\n",
    "            state of decoder's LSTM layers\n",
    "        :param inference: if True stores and repackages hidden state\n",
    "        \"\"\"\n",
    "        self.inference = inference\n",
    "\n",
    "        enc_context, enc_len, hidden = context\n",
    "        hidden = self.init_hidden(hidden)\n",
    "\n",
    "        x = self.embedder(inputs)\n",
    "\n",
    "        x, h, attn, scores = self.att_rnn(x, hidden[0], enc_context, enc_len)\n",
    "        self.append_hidden(h)\n",
    "\n",
    "        x = torch.cat((x, attn), dim=2)\n",
    "        x = self.dropout(x)\n",
    "        x, h = self.rnn_layers[0](x, hidden[1])\n",
    "        self.append_hidden(h)\n",
    "\n",
    "        for i in range(1, len(self.rnn_layers)):\n",
    "            residual = x\n",
    "            x = torch.cat((x, attn), dim=2)\n",
    "            x = self.dropout(x)\n",
    "            x, h = self.rnn_layers[i](x, hidden[i + 1])\n",
    "            self.append_hidden(h)\n",
    "            x = x + residual\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        hidden = self.package_hidden()\n",
    "\n",
    "        return x, scores, [enc_context, enc_len, hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualRecurrentDecoder(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualRecurrentDecoder(\n",
       "  (att_rnn): RecurrentAttention(\n",
       "    (rnn): LSTM(1024, 1024)\n",
       "    (attn): BahdanauAttention(\n",
       "      (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (rnn_layers): ModuleList(\n",
       "    (0): LSTM(2048, 1024)\n",
       "    (1): LSTM(2048, 1024)\n",
       "    (2): LSTM(2048, 1024)\n",
       "  )\n",
       "  (embedder): Embedding(50000, 1024, padding_idx=0)\n",
       "  (classifier): Classifier(\n",
       "    (classifier): Linear(in_features=1024, out_features=50000, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic Seq2Seq module, with an encoder and a decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder=None, decoder=None, batch_first=False):\n",
    "        \"\"\"\n",
    "        Constructor for the Seq2Seq module.\n",
    "        :param encoder: encoder module\n",
    "        :param decoder: decoder module\n",
    "        :param batch_first: if True the model uses (batch, seq, feature)\n",
    "            tensors, if false the model uses (seq, batch, feature) tensors\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def encode(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        Applies the encoder to inputs with a given input sequence lengths.\n",
    "        :param inputs: tensor with inputs (batch, seq_len) if 'batch_first'\n",
    "            else (seq_len, batch)\n",
    "        :param lengths: vector with sequence lengths (excluding padding)\n",
    "        \"\"\"\n",
    "        return self.encoder(inputs, lengths)\n",
    "\n",
    "    def decode(self, inputs, context, inference=False):\n",
    "        \"\"\"\n",
    "        Applies the decoder to inputs, given the context from the encoder.\n",
    "        :param inputs: tensor with inputs (batch, seq_len) if 'batch_first'\n",
    "            else (seq_len, batch)\n",
    "        :param context: context from the encoder\n",
    "        :param inference: if True inference mode, if False training mode\n",
    "        \"\"\"\n",
    "        return self.decoder(inputs, context, inference)\n",
    "\n",
    "    def generate(self, inputs, context, beam_size):\n",
    "        \"\"\"\n",
    "        Autoregressive generator, works with SequenceGenerator class.\n",
    "        Executes decoder (in inference mode), applies log_softmax and topK for\n",
    "        inference with beam search decoding.\n",
    "        :param inputs: tensor with inputs to the decoder\n",
    "        :param context: context from the encoder\n",
    "        :param beam_size: beam size for the generator\n",
    "        returns: (words, logprobs, scores, new_context)\n",
    "            words: indices of topK tokens\n",
    "            logprobs: log probabilities of topK tokens\n",
    "            scores: scores from the attention module (for coverage penalty)\n",
    "            new_context: new decoder context, includes new hidden states for\n",
    "                decoder RNN cells\n",
    "        \"\"\"\n",
    "        logits, scores, new_context = self.decode(inputs, context, True)\n",
    "        logprobs = log_softmax(logits, dim=-1)\n",
    "        logprobs, words = logprobs.topk(beam_size, dim=-1)\n",
    "        return words, logprobs, scores, new_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNMT(Seq2Seq):\n",
    "    \"\"\"\n",
    "    GNMT v2 model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n",
    "                 batch_first=False, share_embedding=True):\n",
    "        \"\"\"\n",
    "        Constructor for the GNMT v2 model.\n",
    "        :param vocab_size: size of vocabulary (number of tokens)\n",
    "        :param hidden_size: internal hidden size of the model\n",
    "        :param num_layers: number of layers, applies to both encoder and\n",
    "            decoder\n",
    "        :param dropout: probability of dropout (in encoder and decoder)\n",
    "        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n",
    "            if false the model uses (seq, batch, feature)\n",
    "        :param share_embedding: if True embeddings are shared between encoder\n",
    "            and decoder\n",
    "        \"\"\"\n",
    "\n",
    "        super(GNMT, self).__init__(batch_first=batch_first)\n",
    "\n",
    "        if share_embedding:\n",
    "            embedder = nn.Embedding(vocab_size, hidden_size,\n",
    "                                    padding_idx=config.PAD)\n",
    "            nn.init.uniform_(embedder.weight.data, -0.1, 0.1)\n",
    "        else:\n",
    "            embedder = None\n",
    "\n",
    "        self.encoder = ResidualRecurrentEncoder(vocab_size, hidden_size,\n",
    "                                                num_layers, dropout,\n",
    "                                                batch_first, embedder)\n",
    "\n",
    "        self.decoder = ResidualRecurrentDecoder(vocab_size, hidden_size,\n",
    "                                                num_layers, dropout,\n",
    "                                                batch_first, embedder)\n",
    "\n",
    "    def forward(self, input_encoder, input_enc_len, input_decoder):\n",
    "        context = self.encode(input_encoder, input_enc_len)\n",
    "        context = (context, input_enc_len, None)\n",
    "        output, _, _ = self.decode(input_decoder, context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNMT(6, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNMT(\n",
       "  (encoder): ResidualRecurrentEncoder(\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(1024, 1024, batch_first=True)\n",
       "      (3): LSTM(1024, 1024, batch_first=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (embedder): Embedding(6, 1024, padding_idx=0)\n",
       "  )\n",
       "  (decoder): ResidualRecurrentDecoder(\n",
       "    (att_rnn): RecurrentAttention(\n",
       "      (rnn): LSTM(1024, 1024, batch_first=True)\n",
       "      (attn): BahdanauAttention(\n",
       "        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(2048, 1024, batch_first=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(2048, 1024, batch_first=True)\n",
       "    )\n",
       "    (embedder): Embedding(6, 1024, padding_idx=0)\n",
       "    (classifier): Classifier(\n",
       "      (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2048,  0.1186, -0.0807,  0.0784, -0.0871, -0.1731],\n",
       "         [-0.0969,  0.1670,  0.1123,  0.0247, -0.0475, -0.0835],\n",
       "         [-0.2177,  0.1940,  0.2938, -0.0204, -0.1376, -0.1611]],\n",
       "\n",
       "        [[-0.1987,  0.2661, -0.0771,  0.0107,  0.0217, -0.0023],\n",
       "         [-0.2596,  0.3329, -0.0649, -0.0270, -0.0527, -0.0622],\n",
       "         [-0.3528,  0.2856, -0.1123,  0.0050, -0.0754, -0.1573]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.LongTensor([[2,4,3],[3,2,0]]), torch.tensor([3,1]), torch.LongTensor([[1,4,2],[4,2,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recur_function(module, n):\n",
    "    n = n+1\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    for name, sub_module in sub_modules.items():\n",
    "        print(\"--\"*(n-1), name)\n",
    "        # sub modules of sub_module, if there are more than 1, we need further recursion\n",
    "        sub_sub_modules = sub_module.__dict__['_modules']\n",
    "        if len(sub_sub_modules) > 0:\n",
    "            recur_function(sub_module, n)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " encoder\n",
      "-- rnn_layers\n",
      "---- 0\n",
      "---- 1\n",
      "---- 2\n",
      "---- 3\n",
      "-- dropout\n",
      "-- embedder\n",
      " decoder\n",
      "-- att_rnn\n",
      "---- rnn\n",
      "---- attn\n",
      "------ linear_q\n",
      "------ linear_k\n",
      "---- dropout\n",
      "-- rnn_layers\n",
      "---- 0\n",
      "---- 1\n",
      "---- 2\n",
      "-- embedder\n",
      "-- classifier\n",
      "---- classifier\n",
      "-- dropout\n"
     ]
    }
   ],
   "source": [
    "recur_function(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for Sync between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nrun       = 2      \n",
    "batch_size = 1000\n",
    "fct        = 6\n",
    "\n",
    "inp_size_single = (1, 512*fct)\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "\n",
    "\n",
    "model1 = dm.parallelModelThreeLayerSplit(fct,[1,1],1)\n",
    "model2 = dm.parallelModelThreeLayerSplit(fct,[1,0],1)\n",
    "inp   = torch.ones((Nrun,) +inp_size).to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17842364311218262\n",
      "0.138647198677063\n"
     ]
    }
   ],
   "source": [
    "def train_sync(model):\n",
    "    times = []\n",
    "    output = model(inp[0])\n",
    "    for n in range(Nrun):       \n",
    "        start_epoch = time.time()\n",
    "        output = model(inp[n])\n",
    "        torch.cuda.synchronize(1)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        times.append(elapsed)\n",
    "\n",
    "    avg_time = sum(times)/Nrun\n",
    "    print(avg_time)\n",
    "    \n",
    "    return output\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:  \n",
    "    out1 = train_sync(model1)\n",
    "    time.sleep(0.1)\n",
    "    out2 = train_sync(model2)\n",
    "prof.export_chrome_trace(\"junk_sync.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008124113082885742\n",
      "0.0032290220260620117\n"
     ]
    }
   ],
   "source": [
    "def train_nosync(model):\n",
    "    times = []\n",
    "    output = model(inp[0])\n",
    "    for n in range(Nrun):       \n",
    "        start_epoch = time.time()\n",
    "        output = model(inp[n])\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        times.append(elapsed)\n",
    "\n",
    "    avg_time = sum(times)/Nrun\n",
    "    print(avg_time)\n",
    "    \n",
    "    return output\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:  \n",
    "    out1 = train_nosync(model1)\n",
    "    time.sleep(0.1)\n",
    "    out2 = train_nosync(model2)\n",
    "prof.export_chrome_trace(\"junk_no_sync.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the two profiles. In absence of syncing the gpu runs fully async to cpu. So the function train() can return but the gpu job given by it might still be running on the gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU-GPU Sync points: Using cuda.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "class _addLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n## Make this work  \\n    def forward(self, x):\\n        \\n        # On GPU0\\n        x = x.to(0)\\n        x = self.squeeze(x)\\n        x = self.fc1(x)\\n        self.event1.record()\\n        \\n        # On GPU0\\n        xa1 = self.fc2a1(x)\\n        xa2 = self.fc2a2(xa1)\\n        self.event2.record()\\n        \\n        # On GPU1\\n        self.event2.wait(torch.cuda.default_stream(1))\\n        with torch.cuda.stream(self.s):\\n            xa22 = xa2.to(1) \\n        \\n        # On GPU1\\n        self.event1.wait(torch.cuda.default_stream(1)) #bbloack default stream of GPU 1 for event1\\n        with torch.cuda.stream(self.s):   # Then establish a stream on gpu0 to transfer data to gpu1 without stopping computation going on on gpu0\\n            x1  = x.to(1)      \\n        xb1 = self.fc2b1(x1)\\n        xb2 = self.fc2b2(xb1)\\n        \\n    \\n        y = self.concatenate(xa22,xb2)\\n        y = self.fc3(y)\\n        y = self.fc4(y)\\n        \\n        return y\\n        '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParallelTwoLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, factor, repetable =0) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = int(512*self.factor)\n",
    "        self.linear2N = int(2048*self.factor)\n",
    "        self.linear3N = int(512*self.factor)\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = int(512*self.factor)\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer().to(0)\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to(0)\n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N).to(0)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N).to(0)\n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N).to(1)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N).to(1)\n",
    "        self.concatenate = _concatenateLayer().to(1)\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N).to(1)\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N).to(1)\n",
    "\n",
    "        if repetable:\n",
    "            torch.nn.init.constant_(self.fc1.weight, 1/512); torch.nn.init.zeros_(self.fc1.bias)\n",
    "            torch.nn.init.constant_(self.fc2a1.weight, -1/512); torch.nn.init.zeros_(self.fc2a1.bias)\n",
    "            torch.nn.init.constant_(self.fc2a2.weight, 1/512); torch.nn.init.zeros_(self.fc2a2.bias)\n",
    "            torch.nn.init.constant_(self.fc2b1.weight, -1/512); torch.nn.init.zeros_(self.fc2b1.bias)\n",
    "            torch.nn.init.constant_(self.fc2b2.weight, 1/512); torch.nn.init.zeros_(self.fc2b2.bias)\n",
    "            torch.nn.init.constant_(self.fc3.weight, 1/512); torch.nn.init.zeros_(self.fc3.bias)\n",
    "            torch.nn.init.constant_(self.fc4.weight, -1/512); torch.nn.init.zeros_(self.fc4.bias)\n",
    "        \n",
    "        self.s = torch.cuda.Stream(device=0)\n",
    "        self.s1 = torch.cuda.Stream(device=1)\n",
    "        self.event1 = torch.cuda.Event()\n",
    "        self.event2 = torch.cuda.Event()\n",
    "\n",
    "# Normal Serial execution\n",
    "    def forward(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "  \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        x1  = x.to(1)\n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        xa22 = xa2.to(1)\n",
    "\n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n",
    "    \n",
    "# Normal Serial execution rearranged\n",
    "    def forward0a(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x1  = x.to(1) ## has been moved up\n",
    "  \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        xa22 = xa2.to(1)\n",
    "\n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n",
    "    \n",
    "# Using events and synchronize -> allows overlap\n",
    "# FORWARD-1\n",
    "    def forward1(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()     \n",
    "                \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1\n",
    "        with torch.cuda.stream(self.s):\n",
    "            self.event1.synchronize()   # Stall CPU progress (thus stops all streams)\n",
    "            x1  = x.to(1)      \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        # On GPU1\n",
    "        with torch.cuda.stream(self.s):\n",
    "            self.event2.synchronize()\n",
    "            xa22 = xa2.to(1) \n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# but event.synchronize makes ALL threads wait for the event.\n",
    "# So maving the self.event1.synchronize() eariler stops execution of\n",
    "# all streams even though oonly self.s needs to wait for it\n",
    "# FORWARD-2\n",
    "    def forward2(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()   \n",
    "        \n",
    "        # On GPU1\n",
    "        with torch.cuda.stream(self.s):\n",
    "            self.event1.synchronize()   # Stall CPU progress (thus stops all streams)\n",
    "            x1  = x.to(1)      \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1\n",
    "        with torch.cuda.stream(self.s):\n",
    "            self.event2.synchronize()\n",
    "            xa22 = xa2.to(1) \n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "# How to make FORWARD-2 bbaehave like FORWARD-1?\n",
    "# Using self.event.synchronize() blocks the entire prorgram(CPU i.e all streams)\n",
    "# We must block specific streams for specific events\n",
    "\n",
    "    def forward3(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()\n",
    "        \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1\n",
    "        self.event1.wait(torch.cuda.default_stream(1)) #bbloack default stream of GPU 1 for event1\n",
    "        with torch.cuda.stream(self.s):   # Then establish a stream on gpu0 to transfer data to gpu1 without stopping computation going on on gpu0\n",
    "            x1  = x.to(1)      \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "         # On GPU1\n",
    "        self.event2.wait(torch.cuda.default_stream(1))\n",
    "        with torch.cuda.stream(self.s):\n",
    "            xa22 = xa2.to(1) \n",
    "        \n",
    "    \n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "## But still  'xa22 = xa2.to(1)' does not happen until \n",
    "## xb2 = self.fc2b2(xb1) completes even though xa2 is already ready\n",
    "## This is because on reception side, the default stream is used and \n",
    "## the default stream on 1 has to wait for self.fc2b2(xb1) to complete\n",
    "## bbefore reciveing xa2 to from gpu0 although it has been ready for long on gpu0\n",
    "## So, there must bbe two way signalling:\n",
    "## Default stream should only be blocked by local events waiting for inputs\n",
    "## and seprate streams must handle reception of inputs on reception side\n",
    "## and they must used global events (i.e self.event defined in the model)\n",
    "## FINAL OPTIMAL:\n",
    "    def forward4(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()\n",
    "        \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1 \n",
    "        event_local1 = torch.cuda.Event()\n",
    "        with torch.cuda.stream(self.s):\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                self.event1.wait(self.s1) \n",
    "                x1  = x.to(1) \n",
    "                event_local1.record()\n",
    "        event_local1.wait(torch.cuda.default_stream(1))        \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "         # On GPU1\n",
    "        event_local2 = torch.cuda.Event()\n",
    "        with torch.cuda.stream(self.s):\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                self.event2.wait(self.s1)\n",
    "                xa22 = xa2.to(1)\n",
    "                event_local2.record()\n",
    "        \n",
    "        event_local2.wait(torch.cuda.default_stream(1))\n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "## For experimenting Test:   \n",
    "    def forwardT(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()\n",
    "        \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1 \n",
    "        event_local1 = torch.cuda.Event()\n",
    "        with torch.cuda.stream(self.s):\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                self.event1.wait(self.s1) \n",
    "                x1  = x.to(1) \n",
    "                event_local1.record()\n",
    "        event_local1.wait(torch.cuda.default_stream(1))        \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "         # On GPU1\n",
    "        event_local2 = torch.cuda.Event()\n",
    "        with torch.cuda.stream(self.s):\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                self.event2.wait(self.s1)\n",
    "                xa22 = xa2.to(1)\n",
    "                event_local2.record()\n",
    "        \n",
    "        event_local2.wait(torch.cuda.default_stream(1))\n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "'''    \n",
    "## Make this work  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # On GPU0\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event1.record()\n",
    "        \n",
    "        # On GPU0\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        self.event2.record()\n",
    "        \n",
    "        # On GPU1\n",
    "        self.event2.wait(torch.cuda.default_stream(1))\n",
    "        with torch.cuda.stream(self.s):\n",
    "            xa22 = xa2.to(1) \n",
    "        \n",
    "        # On GPU1\n",
    "        self.event1.wait(torch.cuda.default_stream(1)) #bbloack default stream of GPU 1 for event1\n",
    "        with torch.cuda.stream(self.s):   # Then establish a stream on gpu0 to transfer data to gpu1 without stopping computation going on on gpu0\n",
    "            x1  = x.to(1)      \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "    \n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        \n",
    "        return y\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelThreeLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, factor, repetable =0) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = int(512*self.factor)\n",
    "        self.linear2N = int(2048*self.factor)\n",
    "        self.linear3N = int(512*self.factor)\n",
    "        self.linear4N = 3*self.linear3N\n",
    "        self.linear5N = int(512*self.factor)\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to(0)\n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N).to(0)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N).to(0)\n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N).to(0)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N).to(0)\n",
    "        self.fc2c1 = nn.Linear(self.linear2N, 2*self.linear3N).to(1)\n",
    "        self.fc2c2 = nn.Linear(2*self.linear3N, self.linear3N).to(1)\n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N).to(1)\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N).to(1)\n",
    "\n",
    "        if repetable:\n",
    "            torch.nn.init.constant_(self.fc1.weight, 1/512); torch.nn.init.zeros_(self.fc1.bias)\n",
    "            torch.nn.init.constant_(self.fc2a1.weight, -1/512); torch.nn.init.zeros_(self.fc2a1.bias)\n",
    "            torch.nn.init.constant_(self.fc2a2.weight, 1/512); torch.nn.init.zeros_(self.fc2a2.bias)\n",
    "            torch.nn.init.constant_(self.fc2b1.weight, -1/512); torch.nn.init.zeros_(self.fc2b1.bias)\n",
    "            torch.nn.init.constant_(self.fc2b2.weight, 1/512); torch.nn.init.zeros_(self.fc2b2.bias)\n",
    "            torch.nn.init.constant_(self.fc2c1.weight, -1/512); torch.nn.init.zeros_(self.fc2c1.bias)\n",
    "            torch.nn.init.constant_(self.fc2c2.weight, 1/512); torch.nn.init.zeros_(self.fc2c2.bias)\n",
    "            torch.nn.init.constant_(self.fc3.weight, 1/512); torch.nn.init.zeros_(self.fc3.bias)\n",
    "            torch.nn.init.constant_(self.fc4.weight, -1/512); torch.nn.init.zeros_(self.fc4.bias)\n",
    "\n",
    "            \n",
    "        self.s = torch.cuda.Stream(device=0)\n",
    "        self.s2 = torch.cuda.Stream(device=0)\n",
    "        self.event = torch.cuda.Event()\n",
    "## This takes longer because of sync point\n",
    "## CPU execution stops at sync point without starting the parallel branch\n",
    "    def forward1(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "               \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        #xa22 = xa2.to(1)   # Placing this here is bad as well\n",
    "        #xb22 = xb2.to(1)\n",
    "           \n",
    "        x1  = x.to(1)    #sync point\n",
    "        xc1 = self.fc2c1(x1)\n",
    "        xc2 = self.fc2c2(xc1)\n",
    "        \n",
    "        xa22 = xa2.to(1)\n",
    "        xb22 = xb2.to(1)\n",
    "        \n",
    "        y = self.concatenate(xa22,xb22,xc2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n",
    "    \n",
    "# This is faster  \n",
    "    def forward2(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        x1  = x.to(1)        # This has to be befoore any parallel branch start\n",
    "               \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "           \n",
    "        xc1 = self.fc2c1(x1)\n",
    "        xc2 = self.fc2c2(xc1)\n",
    "        \n",
    "        xa22 = xa2.to(1) # This must be after all bracnhes end\n",
    "        xb22 = xb2.to(1)\n",
    "        \n",
    "        y = self.concatenate(xa22,xb22,xc2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n",
    "    \n",
    "# This is NOT faster  \n",
    "    def forward(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        x1  = x.to(1)        # This has to be befoore any parallel branch start\n",
    "               \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xa22 = xa2.to(1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        xb22 = xb2.to(1)\n",
    "           \n",
    "        xc1 = self.fc2c1(x1)\n",
    "        xc2 = self.fc2c2(xc1)\n",
    "        \n",
    "        \n",
    "        y = self.concatenate(xa22,xb22,xc2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n",
    "    \n",
    "# This is faster  \n",
    "    def forward3(self, x):\n",
    "        \n",
    "        #event.wait(s)\n",
    "        \n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        self.event.record()\n",
    "        \n",
    "                     \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        \n",
    "        with torch.cuda.stream(self.s):\n",
    "            self.event.synchronize()\n",
    "            x1  = x.to(1)     \n",
    "        xc1 = self.fc2c1(x1)\n",
    "        xc2 = self.fc2c2(xc1)\n",
    "        \n",
    "        if 1:\n",
    "        #with torch.cuda.stream(self.s2):\n",
    "            xa22 = xa2.to(1) # This must be after all bracnhes end\n",
    "            xb22 = xb2.to(1)\n",
    "            \n",
    "        \n",
    "        y = self.concatenate(xa22,xb22,xc2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nrun = 50 \n",
    "fct = 6\n",
    "run_type = \"forward\" \n",
    "repetable = 1\n",
    "\n",
    "factor = fct\n",
    "inp_size_single = (1, int(512*factor))\n",
    "\n",
    "model = ParallelTwoLayer(factor, repetable)\n",
    "#model = ParallelThreeLayer(factor, repetable)\n",
    "\n",
    "\n",
    "opt_size = 512*fct\n",
    "#opt_size = (batch_size,512*fct)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "\n",
    "#run_type = \"forward\"\n",
    "run_type = \"training\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for _ in range(Nrun):\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            output = model(inp)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            torch.cuda.empty_cache() #important to have this. Else output may seem correct inspite of timing mismatches\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"testTrace.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(output)\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 14.428162574768066\n",
      "\n",
      "tensor([[0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555],\n",
      "        [0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555],\n",
      "        [0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555],\n",
      "        ...,\n",
      "        [0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555],\n",
      "        [0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555],\n",
      "        [0.4555, 0.4555, 0.4555,  ..., 0.4555, 0.4555, 0.4555]],\n",
      "       device='cuda:1', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Using predictable train data to check for correctness (when repetable=1)\n",
    "first_gpu = 0\n",
    "final_gpu = 1\n",
    "opt_size_tup = (batch_size, opt_size)\n",
    "\n",
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof1:\n",
    "        for run_no in range(Nrun):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)*(0.000001)*(run_no%3 -1)\n",
    "                labels = (torch.ones(opt_size_tup)*0.5).to(final_gpu)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "                labels = torch.randn(opt_size_tup).to(final_gpu)\n",
    "            \n",
    "            start = time.time()\n",
    "            inp = inp.to(first_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            #print(output)\n",
    "            #print(loss)\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            torch.cuda.empty_cache() #important to have this. Else output may seem correct inspite of timing mismatches\n",
    "            times.append(1000*(end-start))\n",
    "    prof1.export_chrome_trace(\"testTrace.json\")\n",
    "\n",
    "    baechi_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", baechi_time)\n",
    "    print()\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events in backprop (how does autograd handle it)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "class _addLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTwoLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, factor, repetable =0) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = int(512*self.factor)\n",
    "        self.linear2N = int(2048*self.factor)\n",
    "        self.linear5N = int(512*self.factor)\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer().to(0)\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to(0)\n",
    "        self.fc2 = nn.Linear(self.linear2N, self.linear5N).to(1)\n",
    "\n",
    "        if repetable:\n",
    "            torch.nn.init.constant_(self.fc1.weight, 1/512); torch.nn.init.zeros_(self.fc1.bias)\n",
    "            torch.nn.init.constant_(self.fc2.weight, 1/512); torch.nn.init.zeros_(self.fc2.bias)\n",
    "        \n",
    "        self.s = torch.cuda.Stream(device=0)\n",
    "        self.s1 = torch.cuda.Stream(device=1)\n",
    "        self.event1 = torch.cuda.Event()\n",
    "        self.event2 = torch.cuda.Event()\n",
    "\n",
    "# Normal Serial execution\n",
    "    def forward0(self, x):\n",
    "        x = x.to(0)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        x1  = x.to(1)\n",
    "  \n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        \n",
    "        \n",
    "        xb1 = self.fc2b1(x1)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        \n",
    "        xa22 = xa2.to(1)\n",
    "\n",
    "        y = self.concatenate(xa22,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.rand(5000,5000)\n",
    "t  = t0.pin_memory()\n",
    "#t = t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    t1 = t.cuda(0, non_blocking=True)\n",
    "    s = torch.mm(t,t)\n",
    "    t2 = t.cuda(1, non_blocking=True)\n",
    "    t3 = t.cuda(3, non_blocking=True)\n",
    "    \n",
    "prof.export_chrome_trace(\"exp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
