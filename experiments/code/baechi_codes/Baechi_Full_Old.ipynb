{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8663bc07",
   "metadata": {},
   "source": [
    "# Baechi Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.inception_modified as inception_modified\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import simple_model as sm\n",
    "\n",
    "######## For profiler (some experiments. Not required) #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "#########################################\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90253e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1585f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Change this settings in bbaechi_units.py file\n",
    "# itype       = 'forward'  # help: forward/all -> Conside forward path only or both\n",
    "# prof_rounds = 40      # help: 'rounds for profiler'\n",
    "# prof_gpu_id = 3      # help: 'which gpu to place the profiler'\n",
    "# batch_size  = '128'   # help: 'batch_size'\n",
    "# gpu_num     = 3      # help: 'number of gpu to use'\n",
    "# sch         = 'sct'  # help: 'sct/etf/topo'\n",
    "\n",
    "# args = Args(itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16045e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cb71e",
   "metadata": {},
   "source": [
    "#### ParallelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5954ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)\n",
    "    \n",
    "class _squeezeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "class _addLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f011808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        #super(ParallelModel, self).__init__()  # syntax in python2, works in python3\n",
    "        # Explained here: https://stackoverflow.com/questions/61288224/why-not-super-init-model-self-in-pytorch\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.add1 = _addLayer()\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.add1(y,xb2)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b217d432",
   "metadata": {},
   "source": [
    "class ParallelModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        #super(ParallelModel, self).__init__()  # syntax in python2, works in python3\n",
    "        # Explained here: https://stackoverflow.com/questions/61288224/why-not-super-init-model-self-in-pytorch\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4934de26",
   "metadata": {},
   "source": [
    "class ParallelModelSplit(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.linear1N, self.linear2N)\n",
    "        torch.nn.init.constant_(self.fc1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a1.bias)\n",
    "        \n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a2.bias)\n",
    "        \n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b1.bias)\n",
    "        \n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b2.bias)\n",
    "        \n",
    "        self.concatenate = _concatenateLayer()\n",
    "        \n",
    "        self.fc3   = nn.Linear(self.linear4N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc3.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "        self.add1  = _addLayer()\n",
    "        \n",
    "        self.fc4   = nn.Linear(self.linear5N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc4.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.add1(y,xb2)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765cae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModelSplit(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.linear1N, self.linear2N)\n",
    "        torch.nn.init.constant_(self.fc1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a1.bias)\n",
    "        \n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a2.bias)\n",
    "        \n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b1.bias)\n",
    "        \n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b2.bias)\n",
    "        \n",
    "        self.concatenate = _concatenateLayer()\n",
    "        \n",
    "        self.fc3   = nn.Linear(self.linear4N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc3.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "        \n",
    "        self.fc4   = nn.Linear(self.linear5N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc4.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        y = self.concatenate(xa2,xb2)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50d9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModelThree(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 3*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.linear1N, self.linear2N)\n",
    "        torch.nn.init.constant_(self.fc1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc2a1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a1.bias)\n",
    "        \n",
    "        self.fc2a2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2a2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2a2.bias)\n",
    "        \n",
    "        self.fc2b1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b1.bias)\n",
    "        \n",
    "        self.fc2b2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2b2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2b2.bias)\n",
    "        \n",
    "        self.fc2c1 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2c1.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2c1.bias)\n",
    "        \n",
    "        self.fc2c2 = nn.Linear(self.linear3N, self.linear3N)\n",
    "        torch.nn.init.constant_(self.fc2c2.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc2c2.bias)\n",
    "        \n",
    "        self.concatenate = _concatenateLayer()\n",
    "        \n",
    "        self.fc3   = nn.Linear(self.linear4N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc3.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "        \n",
    "        self.fc4   = nn.Linear(self.linear5N, self.linear5N)\n",
    "        torch.nn.init.constant_(self.fc4.weight, 1/512)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xa1 = self.fc2a1(x)\n",
    "        xa2 = self.fc2a2(xa1)\n",
    "        xb1 = self.fc2b1(x)\n",
    "        xb2 = self.fc2b2(xb1)\n",
    "        xc1 = self.fc2c1(x)\n",
    "        xc2 = self.fc2c2(xc1)\n",
    "        y = self.concatenate(xa2,xb2,xc2)\n",
    "        y = self.fc3(y)\n",
    "        y1 = self.fc4(y)\n",
    "        return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "647adb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TallParallelModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        #super(TwoLayerLinearModel, self).__init__()  # syntax in python2, works in python3\n",
    "        # Explained here: https://stackoverflow.com/questions/61288224/why-not-super-init-model-self-in-pytorch\n",
    "        super().__init__() # python 3 syntax\n",
    "        \n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "        self.squeeze = _squeezeLayer()\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.fc2a = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.fc2b = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.concatenate = _concatenateLayer()\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N)\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc5 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc6 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc7 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc8 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc9 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc10 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc11 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc12 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc13 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc14 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc15 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc16 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc17 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc18 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc19 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc20 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc21 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc22 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc23 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc24 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc25 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc26 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc27 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc28 = nn.Linear(self.linear5N, self.linear5N)\n",
    "        self.fc29 = nn.Linear(self.linear5N, self.linear5N)\n",
    "          \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.fc1(x)\n",
    "        xb = self.fc2b(x)\n",
    "        xa = self.fc2a(x)\n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        y = self.fc5(y)\n",
    "        y = self.fc6(y)\n",
    "        y = self.fc7(y)\n",
    "        y = self.fc8(y)\n",
    "        y = self.fc9(y)\n",
    "        y = self.fc10(y)\n",
    "        y = self.fc11(y)\n",
    "        y = self.fc12(y)\n",
    "        y = self.fc13(y)\n",
    "        y = self.fc14(y)\n",
    "        y = self.fc15(y)\n",
    "        y = self.fc16(y)\n",
    "        y = self.fc17(y)\n",
    "        y = self.fc18(y)\n",
    "        y = self.fc19(y)\n",
    "        y = self.fc20(y)\n",
    "        y = self.fc21(y)\n",
    "        y = self.fc22(y)\n",
    "        y = self.fc23(y)\n",
    "        y = self.fc24(y)\n",
    "        y = self.fc25(y)\n",
    "        y = self.fc26(y)\n",
    "        y = self.fc27(y)\n",
    "        y = self.fc28(y)\n",
    "        y = self.fc29(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575a8ec",
   "metadata": {},
   "source": [
    "# Settings of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6109aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ParallelModelThree\"\n",
    "Nrun = 11 \n",
    "fct = 6\n",
    "run_type = \"forward\" \n",
    "repetable = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44be2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"toyToyModel\":\n",
    "    model = sm.toyToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"linearModel\":\n",
    "    model = sm.linearModel(factor=fct)\n",
    "    inp_size_single = (1, 10000)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"parallelToyModel\":\n",
    "    model = sm.parallelToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"toyModel\":\n",
    "    model = sm.toyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"inception_v3\":\n",
    "    model = inception_modified.inception_v3(pretrained=True)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"TallParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = TallParallelModel(factor)\n",
    "    opt_size = 1000\n",
    "\n",
    "    \n",
    "if model_name == \"ParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModel(factor)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelSplit\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModelSplit(factor)\n",
    "    opt_size = 512*fct\n",
    "    repetable = 1\n",
    "    \n",
    "if model_name == \"ParallelModelThree\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModelThree(factor)\n",
    "    opt_size = 512*fct\n",
    "    repetable = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce28135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1edb6e",
   "metadata": {},
   "source": [
    "## Single GPU Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05411f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run_gpu = 3\n",
    "inp_size = (int(args.batch_size),) + inp_size_single\n",
    "model = model.to(single_run_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d8bc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 8.058309555053711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for _ in range(Nrun):\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu)\n",
    "            output = model(inp)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699da5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= args.prof_rounds * int(args.batch_size),\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=int(args.batch_size))\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn(opt_size).to(single_run_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcbc24de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        ...,\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.]],\n",
      "       device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c1a2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 1.65950775 GB\n",
      "Cached:    1.68359375 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5661f",
   "metadata": {},
   "source": [
    "## Baechi Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c429f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"toyToyModel\":\n",
    "    model = sm.toyToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"linearModel\":\n",
    "    model = sm.linearModel(factor=fct)\n",
    "    inp_size_single = (1, 10000)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"parallelToyModel\":\n",
    "    model = sm.parallelToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"toyModel\":\n",
    "    model = sm.toyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"inception_v3\":\n",
    "    model = inception_modified.inception_v3(pretrained=True)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"TallParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = TallParallelModel(factor)\n",
    "    opt_size = 1000\n",
    "\n",
    "    \n",
    "if model_name == \"ParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModel(factor)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelSplit\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModelSplit(factor)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelThree\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = ParallelModelThree(factor)\n",
    "    opt_size = 512*fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c01a6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = inp_size_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ef5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling started ********************\n",
      "--> Module name:  _squeezeLayer()\n",
      "--> Module name:  Linear(in_features=3072, out_features=12288, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=6144, bias=True)\n",
      "--> Module name:  Linear(in_features=6144, out_features=6144, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=6144, bias=True)\n",
      "--> Module name:  Linear(in_features=6144, out_features=6144, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=6144, bias=True)\n",
      "--> Module name:  Linear(in_features=6144, out_features=6144, bias=True)\n",
      "--> Module name:  _concatenateLayer()\n",
      "--> Module name:  Linear(in_features=18432, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=3072, out_features=3072, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 3072])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_dot started ********************\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4c40541dd8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4c4054c128>\n",
      "Dealing with this variable: <CatBackward object at 0x7f4be6164198>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc784e48>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc7847f0>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc799080>\n",
      "Dealing with this variable: <SqueezeBackward0 object at 0x7f4bcc799128>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc799198>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc7992e8>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc784ac8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc784dd8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc784b38>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc799080>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc7990f0>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc799160>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc784eb8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc799048>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7f4bcc799080>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc799240>\n",
      "Dealing with this variable: <TBackward object at 0x7f4bcc7991d0>\n",
      "Dealing with this variable: <TBackward object at 0x7f4be6164048>\n",
      "Dealing with this variable: <TBackward object at 0x7f4c4054c278>\n",
      "Sort topologically ********************\n",
      "Replacing sub module id ********************\n",
      "Filling in the edges ********************\n"
     ]
    }
   ],
   "source": [
    "return_graph, tester = build_graph(model, args.prof_gpu_id, args.prof_rounds, inp_size = inp_size)\n",
    "available_devices = range(args.gpu_num)\n",
    "available_device_list = {k:device_list[k] for k in available_devices}\n",
    "DEVICE_GRAPH_MULTIPLE = create_device_graph(available_device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0247c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx(return_graph, node_size=10, font_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0983b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 11:19:57,668 - m_sct_v1:157 - INFO - Start LP solver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer started.\n",
      "Presolve started.\n",
      "Linear dependency checker started.\n",
      "Linear dependency checker terminated.\n",
      "Eliminator started.\n",
      "Freed constraints in eliminator : 5\n",
      "Eliminator terminated.\n",
      "Eliminator - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - number                 : 0               \n",
      "Presolve terminated. Time: 0.00    \n",
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer  - threads                : 16              \n",
      "Optimizer  - solved problem         : the dual        \n",
      "Optimizer  - Constraints            : 11\n",
      "Optimizer  - Cones                  : 0\n",
      "Optimizer  - Scalar variables       : 22                conic                  : 0               \n",
      "Optimizer  - Semi-definite variables: 0                 scalarized             : 0               \n",
      "Factor     - setup time             : 0.00              dense det. time        : 0.00            \n",
      "Factor     - ML order time          : 0.00              GP order time          : 0.00            \n",
      "Factor     - nonzeros before factor : 35                after factor           : 42              \n",
      "Factor     - dense dim.             : 0                 flops                  : 5.88e+02        \n",
      "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
      "0   2.6e+00  3.0e+00  5.5e+00  1.00e+00   2.074615161e+00   1.272667238e+01   1.0e+00  0.01  \n",
      "1   2.5e+00  4.2e+00  2.6e+00  0.00e+00   5.349520107e+00   4.697669040e+00   3.2e+00  0.02  \n",
      "2   2.9e-01  4.9e-01  3.0e-01  1.30e+00   5.313489314e+00   5.249765374e+00   3.8e-01  0.02  \n",
      "3   1.1e-02  1.9e-02  1.2e-02  1.12e+00   5.188539325e+00   5.186242481e+00   1.5e-02  0.02  \n",
      "4   1.7e-06  2.8e-06  1.7e-06  1.00e+00   5.182024591e+00   5.182024224e+00   2.2e-06  0.02  \n",
      "5   1.7e-10  2.8e-10  1.7e-10  1.00e+00   5.182023800e+00   5.182023800e+00   2.2e-10  0.02  \n",
      "Basis identification started.\n",
      "Basis identification terminated. Time: 0.00\n",
      "Optimizer terminated. Time: 0.03    \n",
      "\n",
      "\n",
      "Interior-point solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 5.1820237999e+00    nrm: 5e+00    Viol.  con: 6e-11    var: 0e+00  \n",
      "  Dual.    obj: 5.1820238000e+00    nrm: 2e+00    Viol.  con: 5e-11    var: 6e-11  \n",
      "\n",
      "Basic solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 5.1820237999e+00    nrm: 5e+00    Viol.  con: 2e-10    var: 0e+00  \n",
      "  Dual.    obj: 5.1820237999e+00    nrm: 2e+00    Viol.  con: 0e+00    var: 6e-17  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 11:19:57,727 - m_sct_v1:162 - INFO - LP solver finished. Relaxed makespan soultion: 5.182024\n",
      "2021-11-03 11:19:57,728 - m_sct_v1:140 - INFO - Favorite child round threshold: 0.5\n",
      "2021-11-03 11:19:57,730 - m_sct:143 - INFO - # favorite child: 8\n",
      "2021-11-03 11:19:57,730 - m_sct:144 - INFO - # favorite child changes: 0\n",
      "2021-11-03 11:19:57,763 - m_sct:172 - INFO - SCT estimated runtime: 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "0\n",
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "Checking placement***************************************\n",
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "True\n",
      "Available mem:  8000000000\n",
      "Op memory: 393216.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  8000000000\n",
      "_squeezeLayer()  :  0\n",
      "Available mem:  7999606784.0\n",
      "Op memory: 152616960.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 152616960.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 152616960.0\n",
      "Available mem:  7999606784.0\n",
      "Op memory: 152616960.0\n",
      "0\n",
      "Available mem:  7999606784.0\n",
      "Op memory: 152616960.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7999606784.0\n",
      "Op memory: 152616960.0\n",
      "True\n",
      "Available mem:  7999606784.0\n",
      "Op memory: 152616960.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7999606784.0\n",
      "Linear(in_features=3072, out_features=12288, bias=True)  :  151044096\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "0\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "True\n",
      "Available mem:  7847383040.0\n",
      "Op memory: 302800896.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7847383040.0\n",
      "Linear(in_features=12288, out_features=6144, bias=True)  :  302014464\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "1\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Checking placement***************************************\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "True\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "********************\n",
      "Device id:  1\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  8000000000\n",
      "Linear(in_features=12288, out_features=6144, bias=True)  :  302014464\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 151805952.0\n",
      "0\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 151805952.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 151805952.0\n",
      "True\n",
      "Available mem:  7544582144.0\n",
      "Op memory: 151805952.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7544582144.0\n",
      "Linear(in_features=6144, out_features=6144, bias=True)  :  151019520\n",
      "Available mem:  7393562624.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7393562624.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 302800896.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 302800896.0\n",
      "0\n",
      "Available mem:  7393562624.0\n",
      "Op memory: 302800896.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7393562624.0\n",
      "Op memory: 302800896.0\n",
      "True\n",
      "Available mem:  7393562624.0\n",
      "Op memory: 302800896.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7393562624.0\n",
      "Linear(in_features=12288, out_features=6144, bias=True)  :  302014464\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "1\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "True\n",
      "Available mem:  7697199104.0\n",
      "Op memory: 151805952.0\n",
      "********************\n",
      "Device id:  1\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7697199104.0\n",
      "Linear(in_features=6144, out_features=6144, bias=True)  :  151019520\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7546179584.0\n",
      "Op memory: 151805952.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 151805952.0\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "0\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "Checking placement***************************************\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "True\n",
      "Available mem:  7092334592.0\n",
      "Op memory: 151805952.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  7092334592.0\n",
      "Linear(in_features=6144, out_features=6144, bias=True)  :  151019520\n",
      "Available mem:  6941315072.0\n",
      "Op memory: 2359296.0\n",
      "Available mem:  7546179584.0\n",
      "Op memory: 2359296.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 2359296.0\n",
      "0\n",
      "Available mem:  6941315072.0\n",
      "Op memory: 2359296.0\n",
      "Checking placement***************************************\n",
      "Available mem:  6941315072.0\n",
      "Op memory: 2359296.0\n",
      "True\n",
      "Available mem:  6941315072.0\n",
      "Op memory: 2359296.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  6941315072.0\n",
      "_concatenateLayer()  :  0\n",
      "Available mem:  6940528640.0\n",
      "Op memory: 226897920.0\n",
      "Available mem:  7546966016.0\n",
      "Op memory: 226897920.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 226897920.0\n",
      "Available mem:  6940528640.0\n",
      "Op memory: 226897920.0\n",
      "0\n",
      "Available mem:  6940528640.0\n",
      "Op memory: 226897920.0\n",
      "Checking placement***************************************\n",
      "Available mem:  6940528640.0\n",
      "Op memory: 226897920.0\n",
      "True\n",
      "Available mem:  6940528640.0\n",
      "Op memory: 226897920.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  6940528640.0\n",
      "Linear(in_features=18432, out_features=3072, bias=True)  :  226504704\n",
      "Available mem:  6715990016.0\n",
      "Op memory: 38154240.0\n",
      "Available mem:  7546966016.0\n",
      "Op memory: 38154240.0\n",
      "Available mem:  8000000000\n",
      "Op memory: 38154240.0\n",
      "Available mem:  6715990016.0\n",
      "Op memory: 38154240.0\n",
      "0\n",
      "Available mem:  6715990016.0\n",
      "Op memory: 38154240.0\n",
      "Checking placement***************************************\n",
      "Available mem:  6715990016.0\n",
      "Op memory: 38154240.0\n",
      "True\n",
      "Available mem:  6715990016.0\n",
      "Op memory: 38154240.0\n",
      "********************\n",
      "Device id:  0\n",
      "Memory limit:  8000000000\n",
      "Availale memory:  6715990016.0\n",
      "Linear(in_features=3072, out_features=3072, bias=True)  :  37761024\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    placed_op_graph = m_sct(return_graph, DEVICE_GRAPH_MULTIPLE)\n",
    "copy_p(return_graph, tester)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84552b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_squeezeLayer()\n",
      "0\n",
      "\n",
      "Linear(in_features=3072, out_features=12288, bias=True)\n",
      "0\n",
      "\n",
      "Linear(in_features=12288, out_features=6144, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=6144, out_features=6144, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=12288, out_features=6144, bias=True)\n",
      "0\n",
      "\n",
      "Linear(in_features=6144, out_features=6144, bias=True)\n",
      "0\n",
      "\n",
      "Linear(in_features=12288, out_features=6144, bias=True)\n",
      "0\n",
      "\n",
      "Linear(in_features=6144, out_features=6144, bias=True)\n",
      "0\n",
      "\n",
      "_concatenateLayer()\n",
      "0\n",
      "\n",
      "Linear(in_features=18432, out_features=3072, bias=True)\n",
      "0\n",
      "\n",
      "Linear(in_features=3072, out_features=3072, bias=True)\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_gpu = -1\n",
    "for node_id in tester.sub_module_nodes:\n",
    "    print(tester.sub_module_nodes[node_id].module)\n",
    "    curr_gpu_id = tester.sub_module_nodes[node_id].p\n",
    "    print(curr_gpu_id)\n",
    "    if first_gpu < 0:\n",
    "        first_gpu = curr_gpu_id\n",
    "    print()\n",
    "final_gpu = curr_gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29868c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:               _squeezeLayer()\n",
      "GPU:                  0\n",
      "Memory change:        0\n",
      "Layer size:           0\n",
      "Net memory occupied:  0\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=12288, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        302088192\n",
      "Layer size:           151044096\n",
      "Net memory occupied:  302088192\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=6144, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        604028928\n",
      "Layer size:           302014464\n",
      "Net memory occupied:  604028928\n",
      "**************************************************\n",
      "Module:               Linear(in_features=6144, out_features=6144, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        302039040\n",
      "Layer size:           151019520\n",
      "Net memory occupied:  906067968\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=6144, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        604028928\n",
      "Layer size:           302014464\n",
      "Net memory occupied:  906117120\n",
      "**************************************************\n",
      "Module:               Linear(in_features=6144, out_features=6144, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        302039040\n",
      "Layer size:           151019520\n",
      "Net memory occupied:  1208156160\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=6144, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        604028928\n",
      "Layer size:           302014464\n",
      "Net memory occupied:  1812185088\n",
      "**************************************************\n",
      "Module:               Linear(in_features=6144, out_features=6144, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        302039040\n",
      "Layer size:           151019520\n",
      "Net memory occupied:  2114224128\n",
      "**************************************************\n",
      "Module:               _concatenateLayer()\n",
      "GPU:                  0\n",
      "Memory change:        0\n",
      "Layer size:           0\n",
      "Net memory occupied:  2114224128\n",
      "**************************************************\n",
      "Module:               Linear(in_features=18432, out_features=3072, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        453009408\n",
      "Layer size:           226504704\n",
      "Net memory occupied:  2567233536\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=3072, bias=True)\n",
      "GPU:                  0\n",
      "Memory change:        75522048\n",
      "Layer size:           37761024\n",
      "Net memory occupied:  2642755584\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baechi_units.Assign at 0x7f4beee1bb00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Assign(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc852cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 3072)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = (int(args.batch_size),) + inp_size_single\n",
    "inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3a81473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 6.62684440612793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof1:\n",
    "        for _ in range(Nrun):\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            #with torch.no_grad():\n",
    "            inp = inp.to(first_gpu)  ### Code works even without this\n",
    "                                            ### However, not having this gives a time penalty\n",
    "                                            ### of ~6%\n",
    "            output = tester.model(inp)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "    prof1.export_chrome_trace(\"trace_baechi.json\")\n",
    "\n",
    "    baechi_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", baechi_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e51a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= args.prof_rounds * int(args.batch_size),\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=int(args.batch_size))\n",
    "\n",
    "\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof1:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn(opt_size).to(final_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(first_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    prof1.export_chrome_trace(\"trace_baechi.json\")\n",
    "\n",
    "    baechi_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", baechi_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5f21101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        ...,\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.],\n",
      "        [373248., 373248., 373248.,  ..., 373248., 373248., 373248.]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8f2b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 2.46748352 GB\n",
      "Cached:    2.49023438 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.84603882 GB\n",
      "Cached:    0.8671875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    3.859375 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 2.46125793 GB\n",
      "Cached:    2.49023438 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.84384155 GB\n",
      "Cached:    0.8671875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    3.859375 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfecee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Type: forward\n",
      "Single GPU Mean execution time (ms):  8.058309555053711\n",
      "Baechi Execution Time (ms):  6.62684440612793\n"
     ]
    }
   ],
   "source": [
    "print(\"Run Type:\", run_type)\n",
    "print(\"Single GPU Mean execution time (ms): \", single_gpu_time)\n",
    "print(\"Baechi Execution Time (ms): \",baechi_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435baa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
