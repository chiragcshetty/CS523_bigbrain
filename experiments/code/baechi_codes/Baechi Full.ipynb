{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8663bc07",
   "metadata": {},
   "source": [
    "# Baechi Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b6673c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.inception_modified as inception_modified\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import simple_model as sm\n",
    "import baechiTest_dummyModels as dm\n",
    "\n",
    "######## For profiler (some experiments. Not required) #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "#########################################\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90253e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units_old import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1585f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Change this settings in bbaechi_units.py file\n",
    "# itype       = 'forward'  # help: forward/all -> Conside forward path only or both\n",
    "# prof_rounds = 40      # help: 'rounds for profiler'\n",
    "# prof_gpu_id = 3      # help: 'which gpu to place the profiler'\n",
    "# batch_size  = '128'   # help: 'batch_size'\n",
    "# gpu_num     = 3      # help: 'number of gpu to use'\n",
    "# sch         = 'sct'  # help: 'sct/etf/topo'\n",
    "\n",
    "# args = Args(itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16045e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 1.96911621 GB\n",
      "Cached:    1.9921875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.26580811 GB\n",
      "Cached:    1.2890625 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    1.92382812 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575a8ec",
   "metadata": {},
   "source": [
    "# Settings of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6109aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ParallelThreeLayer\"\n",
    "Nrun = 50 \n",
    "fct = 6\n",
    "run_type = \"forward\" \n",
    "#run_type = \"training\"\n",
    "repetable = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44be2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"toyToyModel\":\n",
    "    model = sm.toyToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"linearModel\":\n",
    "    model = sm.linearModel(factor=fct)\n",
    "    inp_size_single = (1, 10000)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"parallelToyModel\":\n",
    "    model = sm.parallelToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"toyModel\":\n",
    "    model = sm.toyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"inception_v3\":\n",
    "    model = inception_modified.inception_v3(pretrained=True)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"TallParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = dm.tallParallelModel(factor, repetable)\n",
    "    opt_size = 1000\n",
    "\n",
    "    \n",
    "if model_name == \"ParallelTwoLayer\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelTwoLayer(factor, repetable)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelThreeLayer\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelThreeLayer(factor, repetable)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelThreeLayerOld\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelThreeLayerOld(factor)\n",
    "    opt_size = 512*fct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fce28135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 1.96911621 GB\n",
      "Cached:    1.9921875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.26580811 GB\n",
      "Cached:    1.2890625 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    1.92382812 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1edb6e",
   "metadata": {},
   "source": [
    "## Single GPU Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05411f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run_gpu = 3\n",
    "inp_size = (int(args.batch_size),) + inp_size_single\n",
    "model = model.to(single_run_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d8bc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 3.912001848220825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for _ in range(Nrun):\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu)\n",
    "            output = model(inp)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            #torch.cuda.empty_cache() #important to have this. Else output may seem correct inspite of timing mismatches\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "699da5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= args.prof_rounds * int(args.batch_size),\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=int(args.batch_size))\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn(opt_size).to(single_run_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    single_gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", single_gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcbc24de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        ...,\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.]],\n",
      "       device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c1a2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 1.96911621 GB\n",
      "Cached:    1.9921875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.26580811 GB\n",
      "Cached:    1.2890625 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.81349182 GB\n",
      "Cached:    1.92578125 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 1.96911621 GB\n",
      "Cached:    1.97070312 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.26580811 GB\n",
      "Cached:    1.26757812 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5661f",
   "metadata": {},
   "source": [
    "## Baechi Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33806255",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"toyToyModel\":\n",
    "    model = sm.toyToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"linearModel\":\n",
    "    model = sm.linearModel(factor=fct)\n",
    "    inp_size_single = (1, 10000)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"parallelToyModel\":\n",
    "    model = sm.parallelToyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"toyModel\":\n",
    "    model = sm.toyModel(factor=fct)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"inception_v3\":\n",
    "    model = inception_modified.inception_v3(pretrained=True)\n",
    "    inp_size_single = (3, 299, 299)\n",
    "    opt_size = 1000\n",
    "\n",
    "if model_name == \"TallParallelModel\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, 512*factor)\n",
    "    model = dm.tallParallelModel(factor, repetable)\n",
    "    opt_size = 1000\n",
    "\n",
    "    \n",
    "if model_name == \"ParallelTwoLayer\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelTwoLayer(factor, repetable)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelThreeLayer\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelThreeLayer(factor, repetable)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelThreeLayerOld\":\n",
    "    factor = fct\n",
    "    inp_size_single = (1, int(512*factor))\n",
    "    model = dm.parallelThreeLayerOld(factor)\n",
    "    opt_size = 512*fct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c01a6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = inp_size_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42ef5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling started ********************\n",
      "--> Module name:  _squeezeLayer()\n",
      "--> Module name:  Linear(in_features=3072, out_features=12288, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=3072, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=3072, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=12288, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=3072, out_features=3072, bias=True)\n",
      "--> Module name:  _concatenateLayer()\n",
      "--> Module name:  Linear(in_features=9216, out_features=3072, bias=True)\n",
      "--> Module name:  Linear(in_features=3072, out_features=3072, bias=True)\n",
      "make_dot started ********************\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfe06d1198>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb81aaa90>\n",
      "Dealing with this variable: <CatBackward object at 0x7fdfb81aaef0>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb81bbcf8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814ac88>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a9b0>\n",
      "Dealing with this variable: <SqueezeBackward0 object at 0x7fdfb814a860>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814ab00>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814ad68>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814ada0>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814ac18>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a2e8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a9b0>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814add8>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814a748>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a710>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a6d8>\n",
      "Dealing with this variable: <AddmmBackward object at 0x7fdfb814a9b0>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb8154dd8>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb814a828>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb81aab70>\n",
      "Dealing with this variable: <TBackward object at 0x7fdfb81aa390>\n",
      "Sort topologically ********************\n",
      "Replacing sub module id ********************\n",
      "Filling in the edges ********************\n"
     ]
    }
   ],
   "source": [
    "return_graph, tester = build_graph(model, args.prof_gpu_id, args.prof_rounds, inp_size = inp_size)\n",
    "#available_devices = range(args.gpu_num)\n",
    "available_devices = [1,2]\n",
    "available_device_list = {k:device_list[k] for k in available_devices}\n",
    "DEVICE_GRAPH_MULTIPLE = create_device_graph(available_device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0247c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx(return_graph, node_size=10, font_size=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b53e2b1",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    placed_op_graph = m_sct(return_graph, DEVICE_GRAPH_MULTIPLE)\n",
    "copy_p(return_graph, tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8bcdcd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 17:44:07,380 - m_sct_v1:157 - INFO - Start LP solver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer started.\n",
      "Presolve started.\n",
      "Linear dependency checker started.\n",
      "Linear dependency checker terminated.\n",
      "Eliminator started.\n",
      "Freed constraints in eliminator : 5\n",
      "Eliminator terminated.\n",
      "Eliminator - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - number                 : 0               \n",
      "Presolve terminated. Time: 0.00    \n",
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer  - threads                : 16              \n",
      "Optimizer  - solved problem         : the dual        \n",
      "Optimizer  - Constraints            : 11\n",
      "Optimizer  - Cones                  : 0\n",
      "Optimizer  - Scalar variables       : 22                conic                  : 0               \n",
      "Optimizer  - Semi-definite variables: 0                 scalarized             : 0               \n",
      "Factor     - setup time             : 0.00              dense det. time        : 0.00            \n",
      "Factor     - ML order time          : 0.00              GP order time          : 0.00            \n",
      "Factor     - nonzeros before factor : 35                after factor           : 42              \n",
      "Factor     - dense dim.             : 0                 flops                  : 5.88e+02        \n",
      "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
      "0   2.4e+00  3.0e+00  4.2e+00  1.00e+00   9.285608927e-01   8.077002482e+00   1.0e+00  0.01  \n",
      "1   2.2e+00  4.4e+00  2.3e+00  0.00e+00   3.697830811e+00   3.257587096e+00   3.4e+00  0.02  \n",
      "2   2.9e-01  5.8e-01  3.1e-01  1.27e+00   3.494219122e+00   3.446468156e+00   4.5e-01  0.02  \n",
      "3   1.2e-02  2.4e-02  1.3e-02  1.08e+00   3.442807869e+00   3.440799557e+00   1.9e-02  0.02  \n",
      "4   1.8e-06  3.6e-06  1.9e-06  1.00e+00   3.439696040e+00   3.439695701e+00   2.8e-06  0.02  \n",
      "5   1.8e-10  3.6e-10  1.9e-10  1.00e+00   3.439695490e+00   3.439695490e+00   2.8e-10  0.02  \n",
      "Basis identification started.\n",
      "Basis identification terminated. Time: 0.00\n",
      "Optimizer terminated. Time: 0.04    \n",
      "\n",
      "\n",
      "Interior-point solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 3.4396954901e+00    nrm: 3e+00    Viol.  con: 5e-11    var: 0e+00  \n",
      "  Dual.    obj: 3.4396954901e+00    nrm: 1e+00    Viol.  con: 5e-11    var: 7e-11  \n",
      "\n",
      "Basic solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 3.4396954901e+00    nrm: 3e+00    Viol.  con: 3e-10    var: 0e+00  \n",
      "  Dual.    obj: 3.4396954900e+00    nrm: 1e+00    Viol.  con: 1e-17    var: 1e-16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 17:44:07,438 - m_sct_v1:162 - INFO - LP solver finished. Relaxed makespan soultion: 3.439695\n",
      "2021-11-11 17:44:07,439 - m_sct_v1:140 - INFO - Favorite child round threshold: 0.5\n",
      "2021-11-11 17:44:07,440 - m_sct:143 - INFO - # favorite child: 7\n",
      "2021-11-11 17:44:07,440 - m_sct:144 - INFO - # favorite child changes: 0\n",
      "2021-11-11 17:44:07,445 - m_sct:172 - INFO - SCT estimated runtime: 0.000004\n"
     ]
    }
   ],
   "source": [
    "placed_op_graph = m_sct(return_graph, DEVICE_GRAPH_MULTIPLE)\n",
    "copy_p(return_graph, tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84552b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_squeezeLayer()\n",
      "1\n",
      "\n",
      "Linear(in_features=3072, out_features=12288, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=12288, out_features=3072, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=3072, out_features=3072, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=12288, out_features=3072, bias=True)\n",
      "2\n",
      "\n",
      "Linear(in_features=3072, out_features=3072, bias=True)\n",
      "2\n",
      "\n",
      "Linear(in_features=12288, out_features=3072, bias=True)\n",
      "1\n",
      "\n",
      "Linear(in_features=3072, out_features=3072, bias=True)\n",
      "1\n",
      "\n",
      "_concatenateLayer()\n",
      "2\n",
      "\n",
      "Linear(in_features=9216, out_features=3072, bias=True)\n",
      "2\n",
      "\n",
      "Linear(in_features=3072, out_features=3072, bias=True)\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_gpu = -1\n",
    "for node_id in tester.sub_module_nodes:\n",
    "    print(tester.sub_module_nodes[node_id].module)\n",
    "    curr_gpu_id = tester.sub_module_nodes[node_id].p\n",
    "    print(curr_gpu_id)\n",
    "    if first_gpu < 0:\n",
    "        first_gpu = curr_gpu_id\n",
    "    print()\n",
    "final_gpu = curr_gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29868c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:               _squeezeLayer()\n",
      "GPU:                  1\n",
      "Memory change:        0\n",
      "Layer size:           0\n",
      "Net memory occupied:  2114322432\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=12288, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        302088192\n",
      "Layer size:           151044096\n",
      "Net memory occupied:  2416410624\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=3072, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        302014464\n",
      "Layer size:           151007232\n",
      "Net memory occupied:  2718425088\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=3072, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        75522048\n",
      "Layer size:           37761024\n",
      "Net memory occupied:  2793947136\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=3072, bias=True)\n",
      "GPU:                  2\n",
      "Memory change:        302014464\n",
      "Layer size:           151007232\n",
      "Net memory occupied:  1661165568\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=3072, bias=True)\n",
      "GPU:                  2\n",
      "Memory change:        75522048\n",
      "Layer size:           37761024\n",
      "Net memory occupied:  1736687616\n",
      "**************************************************\n",
      "Module:               Linear(in_features=12288, out_features=3072, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        302014464\n",
      "Layer size:           151007232\n",
      "Net memory occupied:  3095961600\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=3072, bias=True)\n",
      "GPU:                  1\n",
      "Memory change:        75522048\n",
      "Layer size:           37761024\n",
      "Net memory occupied:  3171483648\n",
      "**************************************************\n",
      "Module:               _concatenateLayer()\n",
      "GPU:                  2\n",
      "Memory change:        0\n",
      "Layer size:           0\n",
      "Net memory occupied:  1736687616\n",
      "**************************************************\n",
      "Module:               Linear(in_features=9216, out_features=3072, bias=True)\n",
      "GPU:                  2\n",
      "Memory change:        226516992\n",
      "Layer size:           113258496\n",
      "Net memory occupied:  1963204608\n",
      "**************************************************\n",
      "Module:               Linear(in_features=3072, out_features=3072, bias=True)\n",
      "GPU:                  2\n",
      "Memory change:        75522048\n",
      "Layer size:           37761024\n",
      "Net memory occupied:  2038726656\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baechi_units_old.Assign at 0x7fdfb80a9e80>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Assign(tester)\n",
    "#tester.model = tester.model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc852cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 3072)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = (int(args.batch_size),) + inp_size_single\n",
    "inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3a81473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 3.7340283393859863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof1:\n",
    "        for _ in range(Nrun):\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            #with torch.no_grad():\n",
    "            inp = inp.to(first_gpu)  ### Code works even without this\n",
    "                                            ### However, not having this gives a time penalty\n",
    "                                            ### of ~6%\n",
    "            output = tester.model(inp)\n",
    "            torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            #torch.cuda.empty_cache() #important to have this. Else output may seem correct inspite of timing mismatches\n",
    "            #print_mem(args.prof_gpu_id)\n",
    "            times.append(1000*(end-start))\n",
    "    prof1.export_chrome_trace(\"trace_baechi.json\")\n",
    "\n",
    "    baechi_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", baechi_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e51a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= args.prof_rounds * int(args.batch_size),\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=int(args.batch_size))\n",
    "\n",
    "\n",
    "    times = []\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof1:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn(opt_size).to(final_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(first_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = tester.model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            torch.cuda.empty_cache() #important to have this. Else output may seem correct inspite of timing mismatches\n",
    "            times.append(1000*(end-start))\n",
    "    prof1.export_chrome_trace(\"trace_baechi.json\")\n",
    "\n",
    "    baechi_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", baechi_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5f21101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        ...,\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.],\n",
      "        [93312., 93312., 93312.,  ..., 93312., 93312., 93312.]],\n",
      "       device='cuda:2', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8f2b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.95623779 GB\n",
      "Cached:    2.9765625 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.90237427 GB\n",
      "Cached:    1.921875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    1.92382812 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.95367432 GB\n",
      "Cached:    2.9765625 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 1.89871216 GB\n",
      "Cached:    1.921875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    1.92382812 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dfecee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Type: forward\n",
      "Single GPU Mean execution time (ms):  3.912001848220825\n",
      "Baechi Execution Time (ms):  3.7340283393859863\n"
     ]
    }
   ],
   "source": [
    "print(\"Run Type:\", run_type)\n",
    "print(\"Single GPU Mean execution time (ms): \", single_gpu_time)\n",
    "print(\"Baechi Execution Time (ms): \",baechi_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435baa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84743bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadab0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19427d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
