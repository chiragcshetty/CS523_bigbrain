{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.pytorch_modified_inception as pytorch_modified_inception\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import simple_model as sm\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For profiler #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "## Defined in this round about way (instead of just directly assigning) to keep it compatibble with summarize.py\n",
    "class Args:\n",
    "     def __init__(self,itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch):\n",
    "         self.type = itype\n",
    "         self.prof_rounds = prof_rounds\n",
    "         self.prof_gpu_id = prof_gpu_id\n",
    "         self.batch_size = batch_size\n",
    "         self.gpu_num = gpu_num\n",
    "         self.sch = sch\n",
    "            \n",
    "itype       = 'all'  # help: forward/all -> Conside forward path only or both\n",
    "prof_rounds = 4      # help: 'rounds for profiler'\n",
    "prof_gpu_id = 0      # help: 'which gpu to place the profiler'\n",
    "batch_size  = '32'   # help: 'batch_size'\n",
    "gpu_num     = 4      # help: 'number of gpu to use'\n",
    "sch         = 'sct'  # help: 'sct/etf/topo'\n",
    "\n",
    "args = Args(itype, prof_rounds, prof_gpu_id, batch_size, gpu_num, sch)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Function: placer_lib.create_device_graph\n",
    "    -> Creates a graph with devices as nodes and unit weight edges between them\n",
    "    -> Each node: graph.add_node(device_id,\n",
    "                                 id=device_id,\n",
    "                                 name=device_info[\"name\"],\n",
    "                                 size=0,\n",
    "                                 memory_limit=device_info[\"memory_size\"])\n",
    "\"\"\"\n",
    "DEVICE_GRAPH_SINGLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 17179869184, 'type': ''}})\n",
    "DEVICE_GRAPH_MULTIPLE = create_device_graph({0: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:0', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             1: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:1', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             2: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:2', 'memory_size': 8000000000, 'type': ''}, \n",
    "                                             3: {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:3', 'memory_size': 8000000000, 'type': ''}})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    we are going to use streams to allow parallel processing\n",
    "\"\"\"\n",
    "COMPUTE0 = torch.cuda.Stream(device=0)\n",
    "COMPUTE1 = torch.cuda.Stream(device=1)\n",
    "COMPUTE2 = torch.cuda.Stream(device=2)\n",
    "COMPUTE3 = torch.cuda.Stream(device=3)\n",
    "COMPUTE_STREAM = {0:COMPUTE0,1:COMPUTE1,2:COMPUTE2,3:COMPUTE3}\n",
    "\n",
    "\n",
    "## A global variable can be directly called form inside a function\n",
    "## But to change it, use the 'global' keyword\n",
    "## Source: https://stackoverflow.com/questions/10588317/python-function-global-variables\n",
    "def del_all():\n",
    "    ## Clear the GPU\n",
    "    try:\n",
    "        global model\n",
    "        del model\n",
    "    except:\n",
    "        print(\"No model\")\n",
    "    try:\n",
    "        global inp\n",
    "        del inp\n",
    "    except:\n",
    "        print(\"No inp\")\n",
    "    try:\n",
    "        global labels\n",
    "        del labels\n",
    "    except:\n",
    "        print(\"No Labels\")\n",
    "    try:\n",
    "        global output\n",
    "        del output\n",
    "    except:\n",
    "        print(\"No Output\")\n",
    "    try:\n",
    "        global loss\n",
    "        del loss\n",
    "    except:\n",
    "        print(\"No Loss\")\n",
    "    try:\n",
    "        global optimizer\n",
    "        del optimizer\n",
    "    except:\n",
    "        print(\"No optimizer\")\n",
    "    print(\"Emptying cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print_mem(args.prof_gpu_id)\n",
    "    \n",
    "# Get the leaf operations in a model. model.modules() gives not just the leaves, bbut higher levels as well\n",
    "# Ref: https://stackoverflow.com/questions/54846905/pytorch-get-all-layers-of-model\n",
    "# More explanation: https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/4\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = {}\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return {id(model): model}\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.update(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.update(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "\n",
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")\n",
    "\n",
    "# print memory of given GPU. ex: gpu_no = 0\n",
    "def print_mem(gpu_id, cached=2):\n",
    "    mem_allocated = round(torch.cuda.memory_allocated(gpu_id)/1024**3,8)\n",
    "    mem_cached    = round(torch.cuda.memory_reserved(gpu_id)/1024**3,8)\n",
    "    if cached>0:\n",
    "        print('Allocated:', mem_allocated , 'GB')\n",
    "    if cached>1:\n",
    "        print('Cached:   ', mem_cached    , 'GB')\n",
    "    return mem_allocated, mem_cached\n",
    "\n",
    "#### Estimate size of the model (in GB or MB)\n",
    "\n",
    "def estimate_model_size(model, unit='MB'): \n",
    "    persistent_memory = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        persistent_memory += param.element_size() * param.nelement()\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(persistent_memory/1024**3,8)\n",
    "        print(\"Estimated Model Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    elif unit == 'B':\n",
    "        gb_mem = persistent_memory\n",
    "        print(\"Estimated Model Memory:\",gb_mem, \"Bytes\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(persistent_memory/1024**2,8)\n",
    "        print(\"Estimated Model Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "    \n",
    "def estimate_input_size(inp, unit='MB'):\n",
    "    input_size = 0\n",
    "    if isinstance(inp, torch.Tensor): \n",
    "        input_size += float(torch.prod(torch.tensor(inp.size())))\n",
    "    if isinstance(inp, list): \n",
    "        for sub_inp in inp:\n",
    "            if isinstance(sub_inp, torch.Tensor): input_size += float(torch.prod(torch.tensor(sub_inp.size())))\n",
    "\n",
    "    input_size = input_size*torch.rand((1,1)).element_size() # multiply by 4\n",
    "    if unit == 'GB':\n",
    "        gb_mem = round(input_size/1024**3,8)\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"GB\")\n",
    "        return gb_mem\n",
    "    if unit == 'B':\n",
    "        gb_mem = input_size\n",
    "        print(\"Estimated Input Memory:\",gb_mem, \"B\")\n",
    "        return gb_mem\n",
    "    else:\n",
    "        mb_mem = round(input_size/1024**2,8)\n",
    "        print(\"Estimated Input Memory:\", mb_mem, \"MB\")\n",
    "        return mb_mem\n",
    "\n",
    "    \n",
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "        print(f\"delta used/peak {self.used}/{self.peaked}\")\n",
    "\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inp = torch.rand((1,1)).to(0)\n",
    "inp.dtype\n",
    "print_mem(0,0)\n",
    "del inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTracemalloc():\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = (self.end-self.begin)\n",
    "        self.peaked = (self.peak-self.begin)\n",
    "\n",
    "def _calculate_time_and_memory(function, *input):\n",
    "    \"\"\"\n",
    "    - Helper function in forward wrapper\n",
    "    - Calculates forward runtime, peak memory used and static memory used\n",
    "    - Verified: Memory measurement context doesn't add overhead to\n",
    "      time measurement\n",
    "    \"\"\"\n",
    "    with TorchTracemalloc() as tt:\n",
    "        torch.cuda.synchronize('cuda:0')\n",
    "        start_time = time.time()\n",
    "        result = function(*input)\n",
    "        torch.cuda.synchronize('cuda:0')\n",
    "        stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, tt.used, tt.peaked , result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.112741470336914,\n",
       " 134578176,\n",
       " 134611456,\n",
       " tensor([[[[ 4.4548e-01,  7.1091e-02,  4.4385e-01,  ..., -5.1342e-02,\n",
       "             3.1152e-01, -8.9722e-02],\n",
       "           [ 2.8740e-01,  1.3975e-01,  3.8090e-01,  ...,  2.8432e-01,\n",
       "             3.5253e-01, -2.1155e-01],\n",
       "           [ 3.5314e-01,  1.8756e-01,  9.2431e-02,  ..., -6.6950e-02,\n",
       "             1.8295e-01,  1.6360e-02],\n",
       "           ...,\n",
       "           [ 3.4881e-02,  1.9179e-01,  2.3429e-01,  ...,  9.1239e-02,\n",
       "            -3.9884e-02, -1.5244e-03],\n",
       "           [ 1.3473e-01,  1.1092e-01,  3.0242e-01,  ...,  9.3249e-02,\n",
       "             2.0884e-01,  1.5638e-01],\n",
       "           [ 3.2495e-01,  1.0392e-01,  3.6976e-02,  ...,  2.6723e-01,\n",
       "            -1.8213e-01,  1.2905e-01]],\n",
       " \n",
       "          [[-2.1707e-01,  3.9914e-01,  2.2452e-01,  ...,  2.1434e-01,\n",
       "             2.2052e-01,  1.4559e-01],\n",
       "           [ 2.8395e-02, -2.0778e-01,  3.9858e-01,  ...,  1.0712e-01,\n",
       "            -5.1491e-02, -4.2790e-01],\n",
       "           [-5.5490e-01,  1.7486e-01, -8.6600e-02,  ..., -1.6715e-01,\n",
       "             5.3518e-02,  1.2079e-01],\n",
       "           ...,\n",
       "           [-1.0728e-01, -7.7669e-02, -7.9142e-02,  ...,  5.1862e-02,\n",
       "            -1.7830e-02, -5.4989e-02],\n",
       "           [-4.1081e-01, -3.2147e-01, -9.2852e-02,  ...,  2.3648e-02,\n",
       "            -8.4430e-02,  2.3449e-01],\n",
       "           [-1.7831e-01, -2.8997e-02, -1.3808e-01,  ..., -3.1122e-01,\n",
       "            -4.7214e-02, -2.2009e-01]],\n",
       " \n",
       "          [[-3.2532e-01, -1.3705e-01, -1.9384e-01,  ..., -2.7526e-01,\n",
       "            -1.7327e-01, -7.3833e-03],\n",
       "           [-3.3046e-01, -1.3804e-01,  1.9518e-03,  ..., -1.4559e-01,\n",
       "            -9.1693e-03, -3.0556e-01],\n",
       "           [-2.9077e-01, -1.8437e-01,  1.2815e-01,  ..., -2.3779e-04,\n",
       "            -3.1705e-01,  3.9598e-02],\n",
       "           ...,\n",
       "           [-2.6798e-01,  1.8722e-02, -5.0861e-02,  ...,  9.2033e-02,\n",
       "            -1.5389e-01, -2.1926e-02],\n",
       "           [-1.4400e-01, -1.2216e-01, -9.5389e-02,  ...,  2.5922e-02,\n",
       "             7.4900e-02, -1.0223e-01],\n",
       "           [-5.3521e-02,  1.5927e-01,  3.5455e-02,  ...,  1.3911e-01,\n",
       "             2.7695e-01,  3.8506e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.2174e-01,  1.8207e-01,  1.6828e-01,  ...,  3.9157e-01,\n",
       "            -1.5645e-02,  3.2824e-01],\n",
       "           [ 2.5971e-01,  2.2981e-01,  4.4686e-01,  ...,  1.4099e-01,\n",
       "             2.2346e-01, -4.2990e-02],\n",
       "           [-4.1158e-02,  3.1446e-01,  1.1331e-01,  ...,  3.1490e-01,\n",
       "             1.8165e-01, -9.1947e-03],\n",
       "           ...,\n",
       "           [-2.3297e-02,  1.9650e-01,  2.3877e-01,  ...,  1.6681e-01,\n",
       "             2.1647e-03,  1.4095e-01],\n",
       "           [-5.6323e-03,  3.9226e-02,  1.5642e-01,  ...,  4.7626e-01,\n",
       "             2.5883e-01,  5.4480e-02],\n",
       "           [-1.9018e-01, -1.3009e-02,  1.6412e-02,  ...,  6.8468e-02,\n",
       "            -6.0840e-02, -1.7109e-01]],\n",
       " \n",
       "          [[-4.1071e-01,  1.1771e-02, -1.4587e-01,  ..., -2.0956e-01,\n",
       "            -4.0804e-01, -4.2586e-02],\n",
       "           [-4.3828e-01, -5.2863e-01, -4.9534e-01,  ..., -1.4363e-01,\n",
       "            -7.0910e-01, -1.1026e-01],\n",
       "           [-4.0925e-01, -4.4564e-01, -3.8044e-01,  ..., -4.1859e-01,\n",
       "            -1.1119e-01, -2.7586e-01],\n",
       "           ...,\n",
       "           [-4.5917e-01, -3.0994e-01, -3.0783e-01,  ..., -2.7806e-01,\n",
       "            -2.4312e-01, -2.3005e-01],\n",
       "           [-2.6894e-01, -3.4872e-01, -3.9673e-01,  ..., -3.6395e-01,\n",
       "            -3.0414e-01, -4.0305e-01],\n",
       "           [-6.3601e-01, -4.5781e-01, -4.7561e-01,  ..., -2.0357e-01,\n",
       "            -3.9404e-01, -4.0913e-01]],\n",
       " \n",
       "          [[-1.7528e-01, -2.6497e-01, -2.9339e-01,  ..., -1.5003e-02,\n",
       "            -1.0640e-01,  2.0822e-02],\n",
       "           [ 1.0239e-01, -3.8716e-02, -2.2937e-01,  ..., -2.5312e-01,\n",
       "            -5.8564e-02,  1.0634e-01],\n",
       "           [-4.0756e-02, -1.7616e-01, -4.2383e-01,  ..., -4.5669e-01,\n",
       "            -3.4227e-01,  1.1748e-02],\n",
       "           ...,\n",
       "           [-5.5428e-02, -1.2173e-01, -2.3375e-01,  ..., -1.6050e-01,\n",
       "            -4.5374e-01, -3.1072e-01],\n",
       "           [-6.5397e-01, -4.4659e-01, -9.8605e-02,  ..., -4.9581e-01,\n",
       "            -1.7356e-01, -1.4863e-01],\n",
       "           [ 1.8467e-01, -1.5436e-01, -2.8222e-01,  ...,  3.2490e-03,\n",
       "             2.6790e-01,  2.6991e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1310e-01,  9.8677e-02,  2.6028e-01,  ...,  5.3364e-01,\n",
       "             8.4550e-02,  1.3268e-01],\n",
       "           [ 2.6197e-01,  1.5166e-01,  5.8334e-02,  ...,  3.8038e-01,\n",
       "             3.3334e-01, -1.8014e-01],\n",
       "           [ 1.5788e-01, -3.3301e-01, -2.4732e-01,  ...,  4.3561e-01,\n",
       "             2.5080e-01, -6.8747e-03],\n",
       "           ...,\n",
       "           [ 4.6160e-01,  1.5916e-01,  2.8582e-01,  ...,  4.4561e-02,\n",
       "             2.6119e-01,  1.0415e-01],\n",
       "           [ 2.0613e-01,  2.2012e-01,  1.3488e-01,  ...,  8.4669e-02,\n",
       "             1.9453e-01,  2.4945e-02],\n",
       "           [ 1.4976e-01,  2.6856e-01,  3.6424e-01,  ...,  9.9681e-02,\n",
       "             2.9255e-01,  6.7452e-02]],\n",
       " \n",
       "          [[ 2.6731e-02,  1.7543e-01,  4.0973e-01,  ...,  7.2063e-02,\n",
       "             2.0087e-01,  7.0061e-03],\n",
       "           [-2.2722e-01, -9.6951e-02,  4.7897e-02,  ..., -1.2676e-03,\n",
       "             2.4402e-02, -1.8721e-01],\n",
       "           [-1.7876e-01,  1.2950e-01,  5.8405e-02,  ..., -1.2053e-01,\n",
       "            -2.1278e-01,  1.9174e-01],\n",
       "           ...,\n",
       "           [-3.8846e-01, -2.4628e-02, -3.0351e-01,  ..., -3.1192e-01,\n",
       "             1.5474e-01, -1.9344e-01],\n",
       "           [-1.9779e-02, -7.6035e-02,  5.8826e-02,  ...,  1.1328e-02,\n",
       "            -1.4308e-01, -6.9685e-03],\n",
       "           [-2.8776e-01,  9.8797e-02, -4.6255e-01,  ..., -2.5670e-01,\n",
       "            -3.8070e-01, -1.9789e-01]],\n",
       " \n",
       "          [[-3.6008e-01,  1.4961e-01, -1.5626e-01,  ..., -6.8435e-02,\n",
       "            -1.5917e-01, -2.8196e-01],\n",
       "           [-1.3944e-01, -7.8757e-02,  2.3978e-02,  ..., -3.1820e-02,\n",
       "            -8.7697e-02, -1.9056e-01],\n",
       "           [-3.0029e-01,  1.5335e-02,  4.7060e-02,  ..., -9.1103e-02,\n",
       "            -3.8742e-02, -3.1401e-01],\n",
       "           ...,\n",
       "           [-4.9187e-01,  1.6787e-02,  1.6033e-02,  ..., -2.6969e-01,\n",
       "            -3.9870e-01, -3.2555e-01],\n",
       "           [-3.1095e-01, -7.7926e-02, -2.9356e-01,  ..., -1.5654e-01,\n",
       "             8.6121e-02, -9.7060e-02],\n",
       "           [ 3.6038e-02,  2.4766e-01,  3.4596e-01,  ...,  4.6632e-01,\n",
       "             8.6996e-02, -7.7036e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.0102e-01,  4.7555e-02, -3.3097e-02,  ...,  3.8200e-01,\n",
       "             1.8935e-01, -1.9119e-01],\n",
       "           [ 1.0052e-01, -2.9687e-02,  4.5901e-02,  ..., -1.0066e-01,\n",
       "             2.1102e-01,  4.5847e-02],\n",
       "           [ 5.7619e-02,  4.5532e-01,  3.0384e-01,  ...,  2.5971e-01,\n",
       "             1.6768e-01, -1.1463e-01],\n",
       "           ...,\n",
       "           [-1.2412e-01,  1.2800e-01,  1.7705e-01,  ...,  2.6810e-01,\n",
       "             6.0547e-02, -1.6991e-01],\n",
       "           [-1.0706e-01,  4.5998e-01,  1.2451e-01,  ...,  2.1485e-01,\n",
       "             1.5680e-01, -1.3487e-01],\n",
       "           [-2.4947e-01, -1.8099e-02,  1.2364e-01,  ..., -1.3425e-01,\n",
       "            -1.8056e-01, -6.1322e-02]],\n",
       " \n",
       "          [[-9.6581e-02, -3.7392e-01, -3.1746e-01,  ..., -4.0122e-01,\n",
       "            -4.7958e-01, -3.0294e-01],\n",
       "           [-2.2764e-01, -4.4997e-01, -6.3409e-01,  ..., -2.4411e-01,\n",
       "            -3.1886e-01, -6.2369e-01],\n",
       "           [-3.1656e-01, -2.2737e-01, -5.4368e-01,  ..., -5.1906e-01,\n",
       "            -9.3933e-02, -4.2295e-01],\n",
       "           ...,\n",
       "           [-3.4925e-01, -5.6859e-01, -3.6037e-01,  ..., -4.2544e-01,\n",
       "            -2.7292e-01, -4.2372e-01],\n",
       "           [-1.9855e-01, -4.2341e-01, -2.3624e-01,  ..., -1.8253e-01,\n",
       "            -4.5508e-01, -1.6284e-01],\n",
       "           [-5.2920e-01, -5.1835e-01, -5.3110e-01,  ..., -3.5790e-01,\n",
       "            -3.7551e-01, -3.9656e-01]],\n",
       " \n",
       "          [[-9.5981e-02, -7.6530e-03, -3.4050e-01,  ..., -2.5945e-01,\n",
       "            -3.8030e-01, -7.0861e-02],\n",
       "           [ 3.5549e-01, -3.6657e-01, -3.6607e-01,  ..., -3.4666e-01,\n",
       "            -3.0784e-01, -1.5633e-01],\n",
       "           [-2.0152e-01, -2.2116e-01, -3.0400e-01,  ..., -3.7177e-01,\n",
       "            -3.1340e-02,  5.4687e-02],\n",
       "           ...,\n",
       "           [-3.2298e-01, -1.3058e-01, -2.3164e-01,  ..., -1.6496e-01,\n",
       "            -2.0808e-01,  1.2771e-01],\n",
       "           [-2.0760e-01, -1.4742e-01,  7.8658e-02,  ..., -9.4661e-02,\n",
       "            -2.7204e-01, -2.7260e-01],\n",
       "           [ 3.0360e-01, -2.6635e-01,  8.5350e-02,  ..., -6.4478e-02,\n",
       "            -4.2612e-02,  1.8021e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.1327e-01,  1.4639e-01,  1.9547e-01,  ...,  2.3785e-01,\n",
       "             6.1247e-02, -4.4378e-02],\n",
       "           [ 6.5366e-02,  1.1682e-01,  3.2167e-02,  ...,  1.2521e-01,\n",
       "            -1.6095e-01,  2.2197e-02],\n",
       "           [ 1.4670e-01,  1.2969e-01,  1.5685e-01,  ...,  1.7926e-01,\n",
       "             1.3215e-01, -2.7737e-02],\n",
       "           ...,\n",
       "           [ 3.1452e-01,  2.1191e-01,  5.5242e-02,  ...,  1.3021e-01,\n",
       "             3.9593e-01,  1.7393e-01],\n",
       "           [ 1.3438e-01,  2.3933e-01,  4.2286e-01,  ...,  4.0693e-01,\n",
       "             3.8870e-02, -1.3576e-01],\n",
       "           [ 2.2855e-01, -6.1599e-02,  3.6447e-01,  ...,  4.0758e-01,\n",
       "             1.5355e-01,  9.0712e-02]],\n",
       " \n",
       "          [[-1.6397e-01,  4.2612e-01,  8.9753e-02,  ...,  8.0365e-02,\n",
       "             4.2726e-02, -1.1882e-01],\n",
       "           [-2.3325e-01,  1.6664e-01,  1.2406e-01,  ...,  5.1231e-02,\n",
       "             2.0500e-03,  4.5769e-02],\n",
       "           [-1.3347e-01, -2.2292e-01,  2.9798e-01,  ..., -2.9189e-01,\n",
       "            -6.5900e-02, -1.8634e-01],\n",
       "           ...,\n",
       "           [-1.9148e-01,  8.4744e-03, -3.0710e-01,  ..., -4.4224e-01,\n",
       "            -3.1632e-01,  4.8053e-02],\n",
       "           [ 1.7063e-03, -2.0978e-01, -2.8910e-01,  ...,  3.5249e-02,\n",
       "             4.9510e-02, -2.3958e-01],\n",
       "           [-3.5338e-01, -2.4677e-01, -1.4949e-01,  ..., -1.3153e-01,\n",
       "            -1.2033e-01, -1.9142e-01]],\n",
       " \n",
       "          [[-3.4164e-01, -1.4953e-02, -4.3912e-02,  ..., -4.0148e-01,\n",
       "            -1.5166e-01, -1.8180e-01],\n",
       "           [-1.6086e-01, -2.7496e-01,  2.9015e-02,  ..., -4.2840e-01,\n",
       "            -8.9592e-02,  5.4969e-02],\n",
       "           [-2.8238e-01,  3.4662e-02,  1.9473e-01,  ..., -9.4986e-03,\n",
       "             6.5294e-02,  1.7556e-01],\n",
       "           ...,\n",
       "           [-2.7709e-01, -3.4284e-02,  1.5241e-02,  ..., -1.6391e-01,\n",
       "            -1.1876e-01,  2.6736e-01],\n",
       "           [-1.1606e-01, -2.3330e-01, -2.0028e-01,  ..., -1.9717e-01,\n",
       "             2.6318e-01, -1.7474e-02],\n",
       "           [-6.5142e-02,  2.0476e-01,  2.0916e-01,  ...,  2.1657e-01,\n",
       "             2.1638e-01,  3.4280e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5924e-01,  3.2970e-01,  3.9662e-01,  ...,  2.0523e-01,\n",
       "             1.5647e-01,  6.2057e-02],\n",
       "           [ 3.0493e-01,  4.7282e-01,  3.0890e-01,  ...,  2.8114e-01,\n",
       "             6.5255e-01, -1.1273e-01],\n",
       "           [-5.9225e-02,  5.5566e-01,  6.3446e-01,  ...,  9.2721e-02,\n",
       "             1.5628e-01, -6.6833e-02],\n",
       "           ...,\n",
       "           [ 1.3904e-01,  1.0586e-01,  2.9785e-01,  ...,  2.5207e-01,\n",
       "             3.1905e-01,  2.7691e-01],\n",
       "           [-1.6569e-01,  1.2257e-01,  6.3509e-01,  ...,  3.3660e-01,\n",
       "             2.0533e-01,  9.1385e-02],\n",
       "           [ 4.2081e-02, -2.0887e-01,  6.0274e-02,  ...,  4.5456e-02,\n",
       "            -9.1879e-02, -4.5901e-01]],\n",
       " \n",
       "          [[-3.2135e-01, -2.6047e-01, -4.2023e-01,  ...,  1.6226e-02,\n",
       "            -5.4457e-01, -1.6673e-01],\n",
       "           [-4.3844e-01, -3.3671e-01, -2.3074e-01,  ..., -2.0262e-01,\n",
       "            -6.6064e-01, -4.3374e-01],\n",
       "           [-3.1096e-01, -5.1588e-01, -3.1391e-01,  ..., -2.6208e-01,\n",
       "            -3.8910e-01, -5.4399e-01],\n",
       "           ...,\n",
       "           [-3.8197e-01, -7.8350e-01, -5.4742e-01,  ..., -6.5995e-01,\n",
       "            -7.4814e-01, -5.0423e-01],\n",
       "           [-4.6159e-01, -2.3990e-01, -4.1964e-01,  ..., -7.2216e-01,\n",
       "            -6.1565e-01, -3.9770e-01],\n",
       "           [-4.3814e-01, -4.5496e-01, -3.6244e-01,  ..., -5.0328e-01,\n",
       "            -4.5350e-01, -4.3698e-01]],\n",
       " \n",
       "          [[-5.9719e-02, -2.1635e-01, -1.4926e-01,  ..., -5.1185e-01,\n",
       "            -3.5283e-01, -9.3229e-02],\n",
       "           [-1.1155e-01,  1.5347e-01,  1.2803e-01,  ..., -2.7633e-01,\n",
       "            -7.3839e-02, -2.0851e-01],\n",
       "           [-1.1161e-01,  1.0351e-01, -4.7535e-02,  ..., -4.0893e-02,\n",
       "            -3.1809e-02, -2.4225e-02],\n",
       "           ...,\n",
       "           [ 5.6232e-02, -9.5673e-02, -9.0299e-02,  ...,  2.5609e-02,\n",
       "             1.6862e-02, -1.1299e-01],\n",
       "           [ 3.6092e-02,  1.1435e-01, -1.6501e-01,  ...,  5.7888e-02,\n",
       "             1.5151e-02,  2.7204e-02],\n",
       "           [ 3.0822e-02,  1.0977e-01, -3.6521e-02,  ...,  1.4298e-01,\n",
       "             1.8428e-01,  6.7132e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 4.3042e-01,  8.2123e-02,  2.4227e-01,  ...,  3.8194e-01,\n",
       "             2.2587e-01,  2.9805e-02],\n",
       "           [ 5.0973e-02,  3.0109e-01,  2.5403e-01,  ...,  1.9305e-01,\n",
       "             2.9240e-01,  8.1175e-02],\n",
       "           [ 2.9587e-01,  1.6216e-02,  1.2642e-01,  ...,  1.9401e-01,\n",
       "             6.5775e-02,  1.6876e-01],\n",
       "           ...,\n",
       "           [ 1.1012e-01, -5.7276e-02,  7.5288e-02,  ..., -1.0077e-01,\n",
       "            -8.2544e-02, -3.0957e-01],\n",
       "           [ 3.3534e-01,  1.7622e-01,  2.0075e-01,  ...,  1.2646e-01,\n",
       "             4.2697e-01, -1.8317e-01],\n",
       "           [ 2.6485e-01,  3.2625e-01,  2.3830e-01,  ...,  2.9832e-01,\n",
       "             3.2106e-01,  3.2417e-01]],\n",
       " \n",
       "          [[ 1.9090e-01,  2.8074e-01,  8.6841e-02,  ...,  2.3668e-01,\n",
       "             2.8446e-01,  3.5347e-01],\n",
       "           [ 1.8381e-01,  5.7633e-02,  4.6902e-02,  ..., -2.4457e-01,\n",
       "            -7.4498e-02, -1.3792e-01],\n",
       "           [-4.7556e-01, -3.5364e-01, -1.0247e-02,  ..., -1.1098e-01,\n",
       "            -2.0829e-01, -2.5580e-01],\n",
       "           ...,\n",
       "           [ 1.3724e-01,  1.0407e-01,  7.2264e-03,  ..., -7.5554e-02,\n",
       "             2.6896e-01, -1.1399e-01],\n",
       "           [-5.6495e-01, -4.9225e-02,  1.4608e-01,  ..., -1.2851e-01,\n",
       "             1.2733e-01,  1.7936e-01],\n",
       "           [-3.2848e-01,  6.1269e-02, -6.1885e-02,  ..., -3.6646e-01,\n",
       "            -4.9053e-01, -3.5638e-01]],\n",
       " \n",
       "          [[-1.2279e-01, -9.9267e-02, -2.0365e-01,  ..., -8.8607e-02,\n",
       "             7.6519e-02, -4.0010e-02],\n",
       "           [-2.0858e-01, -1.6149e-01, -1.6920e-01,  ...,  9.6203e-03,\n",
       "            -9.9604e-02, -2.7490e-01],\n",
       "           [-3.1134e-01, -9.7013e-02,  2.8740e-01,  ..., -4.4768e-02,\n",
       "            -2.5864e-01,  1.1731e-01],\n",
       "           ...,\n",
       "           [-3.1301e-01, -3.0464e-01, -1.2670e-01,  ..., -4.1168e-01,\n",
       "            -6.1949e-02, -3.4438e-02],\n",
       "           [-5.0778e-01,  1.5414e-01,  6.1453e-02,  ..., -3.9281e-01,\n",
       "             1.8395e-02, -2.2364e-01],\n",
       "           [-3.1527e-01,  2.0130e-03,  3.4879e-01,  ...,  3.5081e-01,\n",
       "             5.8527e-01,  3.2090e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.0622e-01,  2.3725e-01,  2.3028e-01,  ...,  1.5885e-01,\n",
       "             3.2282e-01, -6.4789e-02],\n",
       "           [ 9.6167e-02,  4.6304e-01,  7.8849e-02,  ...,  8.7572e-02,\n",
       "             4.3092e-01,  1.6593e-01],\n",
       "           [-2.8502e-01,  2.9197e-01, -4.1826e-02,  ...,  3.2089e-01,\n",
       "             1.3327e-01, -1.2440e-01],\n",
       "           ...,\n",
       "           [-3.5099e-02,  2.6036e-01,  1.5286e-01,  ...,  2.2616e-01,\n",
       "             3.2655e-01,  2.5947e-01],\n",
       "           [-1.1453e-01,  4.8909e-01,  2.7760e-01,  ...,  1.5748e-01,\n",
       "             3.4017e-01, -1.3596e-01],\n",
       "           [ 8.0649e-02, -1.7779e-01, -2.2393e-01,  ..., -1.0742e-01,\n",
       "            -1.1171e-01, -6.3787e-01]],\n",
       " \n",
       "          [[-3.2160e-01, -2.8010e-01, -4.4200e-01,  ..., -6.2376e-01,\n",
       "            -3.0094e-01, -5.0919e-01],\n",
       "           [-5.5006e-01, -4.4104e-01, -3.3909e-01,  ..., -4.2314e-01,\n",
       "            -4.5952e-01, -2.5452e-01],\n",
       "           [-4.9848e-01, -5.3557e-01, -3.3864e-01,  ..., -3.9562e-01,\n",
       "            -2.8562e-01, -4.5321e-01],\n",
       "           ...,\n",
       "           [-4.1325e-01, -4.0977e-01, -4.9335e-01,  ..., -4.2391e-01,\n",
       "            -4.0303e-01, -2.6789e-01],\n",
       "           [-6.8812e-01, -4.7093e-01, -4.1961e-01,  ..., -6.5761e-01,\n",
       "            -4.1365e-01, -2.6126e-01],\n",
       "           [-4.1368e-01, -6.9832e-01, -6.3508e-01,  ..., -4.8761e-01,\n",
       "            -6.6252e-01, -1.3494e-01]],\n",
       " \n",
       "          [[-9.2115e-02, -3.5846e-01, -9.8297e-02,  ..., -1.6495e-01,\n",
       "            -1.2627e-02, -2.3468e-01],\n",
       "           [ 2.3483e-02, -1.0720e-01, -5.0740e-01,  ...,  1.4123e-01,\n",
       "            -2.0796e-01,  4.5219e-02],\n",
       "           [-2.6119e-02, -4.7325e-01, -4.1996e-01,  ...,  1.3886e-01,\n",
       "            -3.5260e-01,  4.1300e-02],\n",
       "           ...,\n",
       "           [-9.2836e-02, -3.3760e-01, -1.8268e-01,  ..., -4.5596e-02,\n",
       "            -3.7708e-01, -3.3809e-01],\n",
       "           [ 7.1010e-02, -1.5780e-01, -7.7064e-02,  ..., -1.3448e-01,\n",
       "            -3.8905e-01, -1.7800e-01],\n",
       "           [-2.4864e-02,  1.1889e-01,  4.3100e-03,  ..., -7.7165e-02,\n",
       "             1.3591e-01, -1.9441e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.9735e-01,  1.3066e-01,  9.5914e-02,  ...,  4.2901e-01,\n",
       "             1.7979e-01,  1.1057e-01],\n",
       "           [ 1.8609e-01,  1.7687e-01,  2.3919e-01,  ..., -2.9462e-02,\n",
       "             2.6345e-01, -1.2505e-02],\n",
       "           [ 1.4303e-01,  2.6099e-02,  3.3123e-01,  ...,  4.4873e-01,\n",
       "            -3.7035e-02, -3.5519e-02],\n",
       "           ...,\n",
       "           [ 2.0651e-01, -7.3575e-02,  1.6300e-01,  ...,  4.3133e-02,\n",
       "             2.6664e-02,  2.8159e-02],\n",
       "           [ 2.5546e-01,  2.6448e-01,  3.2628e-01,  ...,  2.8608e-01,\n",
       "             1.4996e-01,  1.9192e-01],\n",
       "           [ 1.9986e-01,  2.4223e-01,  2.7958e-01,  ...,  3.6838e-02,\n",
       "             4.8874e-01, -1.2449e-01]],\n",
       " \n",
       "          [[-1.9979e-01,  1.0273e-02,  1.1060e-01,  ...,  1.6329e-01,\n",
       "             8.0892e-02,  7.0485e-02],\n",
       "           [-2.2848e-01, -6.9310e-02, -6.3334e-02,  ..., -1.1728e-01,\n",
       "             2.2276e-01,  1.3578e-01],\n",
       "           [-5.5983e-02, -3.1665e-02,  6.9197e-03,  ...,  2.8725e-02,\n",
       "            -1.2822e-01, -2.4435e-02],\n",
       "           ...,\n",
       "           [-1.1535e-01,  4.2561e-02,  1.2253e-01,  ...,  4.6853e-02,\n",
       "            -1.1562e-01, -8.8906e-02],\n",
       "           [-8.4634e-02,  9.7527e-02,  3.0935e-01,  ..., -2.3322e-02,\n",
       "            -1.6497e-01, -1.2685e-01],\n",
       "           [-3.5117e-01, -3.7469e-01, -2.9687e-01,  ..., -4.3079e-01,\n",
       "            -2.2669e-01, -1.6327e-01]],\n",
       " \n",
       "          [[-8.4622e-02,  3.1095e-02, -1.4837e-01,  ..., -3.5930e-01,\n",
       "            -2.3713e-01, -1.3639e-01],\n",
       "           [-9.0095e-02, -3.1687e-01, -1.1718e-02,  ...,  9.2157e-02,\n",
       "            -7.9231e-02, -1.9557e-01],\n",
       "           [-5.1014e-03, -3.7662e-01,  3.5040e-02,  ...,  4.0433e-02,\n",
       "             6.5982e-02, -2.1459e-01],\n",
       "           ...,\n",
       "           [-4.7519e-01, -1.7764e-01, -3.9642e-01,  ...,  6.0865e-02,\n",
       "            -4.0702e-02, -3.9921e-03],\n",
       "           [-3.4701e-01, -1.3998e-01,  3.0433e-02,  ..., -1.6296e-01,\n",
       "            -3.3343e-02,  7.4151e-02],\n",
       "           [-1.2133e-01,  2.1107e-01,  5.2415e-01,  ...,  3.5919e-01,\n",
       "             3.2973e-01, -5.4776e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0437e-02,  1.2813e-01,  3.4686e-01,  ...,  3.7204e-01,\n",
       "             1.6270e-01, -8.8322e-02],\n",
       "           [ 1.6330e-01,  2.0914e-01,  3.7060e-01,  ...,  4.2359e-01,\n",
       "             2.2809e-01,  1.6578e-01],\n",
       "           [ 2.3724e-01,  2.2614e-01,  1.1951e-01,  ...,  3.7229e-01,\n",
       "             8.1198e-02,  4.0030e-01],\n",
       "           ...,\n",
       "           [ 3.6806e-01, -1.5377e-02,  5.9068e-01,  ...,  5.0060e-01,\n",
       "             4.7863e-01, -1.1000e-01],\n",
       "           [ 2.3376e-01,  8.2611e-02,  1.4472e-01,  ...,  1.1369e-01,\n",
       "             1.3165e-01, -6.3471e-02],\n",
       "           [-1.7646e-01, -4.6974e-02,  1.2565e-01,  ..., -1.2984e-01,\n",
       "            -1.2883e-01, -3.9457e-01]],\n",
       " \n",
       "          [[ 8.2080e-02, -6.1550e-01, -4.7139e-01,  ..., -4.4208e-01,\n",
       "            -4.0921e-01, -2.6496e-01],\n",
       "           [-3.6219e-01, -4.3753e-01, -3.3453e-01,  ..., -5.0271e-01,\n",
       "            -4.4027e-01, -1.4307e-01],\n",
       "           [-5.0453e-01, -5.4329e-01, -6.4729e-02,  ..., -4.1687e-01,\n",
       "            -3.2148e-01, -4.0609e-01],\n",
       "           ...,\n",
       "           [-5.0405e-01, -3.6507e-01, -7.0746e-01,  ..., -6.6805e-01,\n",
       "            -3.5493e-01, -2.7087e-01],\n",
       "           [-7.1703e-01, -5.5335e-01, -6.8450e-01,  ..., -4.4762e-01,\n",
       "            -4.2066e-01, -2.1492e-01],\n",
       "           [-2.6773e-01, -6.6476e-01, -6.3112e-01,  ..., -7.4530e-01,\n",
       "            -5.3302e-01, -4.7093e-01]],\n",
       " \n",
       "          [[-1.6906e-01, -6.6101e-02, -3.4210e-01,  ..., -1.8516e-01,\n",
       "            -4.1348e-01, -1.9428e-01],\n",
       "           [-2.2454e-02, -2.5151e-01,  1.4355e-01,  ...,  3.1613e-02,\n",
       "            -9.9019e-02, -5.1623e-02],\n",
       "           [-1.1786e-01, -3.8190e-01, -2.6587e-01,  ..., -3.5086e-01,\n",
       "            -1.1512e-01, -2.6308e-01],\n",
       "           ...,\n",
       "           [-9.0616e-02, -4.6466e-02,  3.6833e-02,  ..., -1.0388e-01,\n",
       "            -2.4387e-01, -1.9100e-01],\n",
       "           [-1.3306e-01,  2.0561e-01, -2.5953e-01,  ..., -2.6653e-01,\n",
       "            -1.8008e-01, -9.0898e-02],\n",
       "           [-2.8551e-02,  1.3065e-01, -4.5783e-03,  ...,  3.2081e-02,\n",
       "            -2.7518e-02, -1.6040e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0538e-01,  1.8180e-01,  2.9957e-01,  ..., -1.9815e-02,\n",
       "             3.6400e-01,  3.1124e-01],\n",
       "           [ 2.0467e-01, -1.4251e-01,  3.2831e-01,  ...,  1.9150e-01,\n",
       "             5.0220e-02, -1.5213e-01],\n",
       "           [ 2.6406e-01,  2.2005e-01, -1.1581e-01,  ...,  1.7474e-01,\n",
       "             2.1063e-01,  1.7142e-01],\n",
       "           ...,\n",
       "           [ 6.7577e-01,  2.0988e-01,  1.3841e-01,  ...,  1.8563e-01,\n",
       "             8.7617e-02,  6.9277e-03],\n",
       "           [ 3.5846e-03, -2.5587e-01,  7.0874e-02,  ..., -8.2649e-02,\n",
       "            -1.2204e-01,  4.0589e-02],\n",
       "           [ 2.8665e-01,  2.1025e-01,  7.1782e-02,  ..., -3.2415e-02,\n",
       "             2.7502e-01,  1.1557e-02]],\n",
       " \n",
       "          [[ 3.0574e-03,  1.1633e-01, -7.8815e-02,  ...,  4.6499e-01,\n",
       "             3.0274e-01, -7.5716e-02],\n",
       "           [ 1.1018e-01,  4.8273e-02, -6.5945e-02,  ..., -5.4657e-02,\n",
       "            -2.0391e-02,  8.6579e-02],\n",
       "           [-1.5746e-01, -7.1425e-02,  1.2376e-01,  ..., -1.7300e-01,\n",
       "            -3.0241e-02,  2.5315e-01],\n",
       "           ...,\n",
       "           [-3.0175e-01, -3.7285e-01, -1.3476e-01,  ..., -2.2201e-02,\n",
       "            -1.0671e-01, -7.9261e-02],\n",
       "           [-1.6634e-01, -2.4605e-01,  1.5758e-01,  ..., -6.4429e-02,\n",
       "             1.6814e-01, -1.2815e-01],\n",
       "           [-2.9982e-01, -1.9558e-01, -4.7352e-01,  ..., -2.0424e-01,\n",
       "            -3.2969e-01, -3.3716e-01]],\n",
       " \n",
       "          [[-4.8626e-01, -4.5249e-01,  2.0357e-01,  ..., -2.0849e-01,\n",
       "            -3.6815e-01, -3.8779e-01],\n",
       "           [-9.4776e-02,  1.2911e-01,  2.6908e-02,  ...,  3.0108e-02,\n",
       "            -1.5145e-02, -1.6008e-01],\n",
       "           [-1.5480e-01, -2.2604e-01,  7.6127e-02,  ..., -1.7994e-02,\n",
       "            -5.4026e-02,  1.2629e-02],\n",
       "           ...,\n",
       "           [-1.1447e-01,  1.1845e-01, -2.0642e-01,  ..., -4.2135e-02,\n",
       "            -1.9070e-01,  1.0556e-02],\n",
       "           [-2.7158e-01, -1.1594e-01, -4.5849e-02,  ...,  2.6674e-01,\n",
       "            -2.4203e-02, -3.7023e-02],\n",
       "           [-1.6439e-01,  1.3809e-01,  2.5228e-01,  ...,  3.1025e-02,\n",
       "             3.3824e-01,  3.1017e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.3323e-02,  4.4154e-01,  5.3470e-02,  ...,  2.4732e-01,\n",
       "             1.4087e-01,  1.4588e-01],\n",
       "           [ 4.3275e-02,  3.9986e-01,  2.9525e-01,  ...,  2.3881e-01,\n",
       "            -4.1614e-02,  2.1699e-02],\n",
       "           [-6.0153e-02,  3.2732e-01,  3.9606e-01,  ...,  6.7421e-02,\n",
       "             4.1097e-01, -1.0728e-01],\n",
       "           ...,\n",
       "           [-1.4949e-01,  9.0896e-02,  1.9265e-01,  ...,  1.7433e-01,\n",
       "             2.0756e-01,  1.0648e-01],\n",
       "           [ 2.2761e-02, -1.6224e-04,  2.7790e-01,  ...,  2.3673e-01,\n",
       "             3.4291e-01,  1.8765e-01],\n",
       "           [ 5.1343e-02,  2.0786e-01, -7.9631e-02,  ...,  4.4919e-02,\n",
       "            -2.0975e-01, -2.2010e-01]],\n",
       " \n",
       "          [[-4.2757e-01, -4.4411e-01, -2.3276e-01,  ..., -2.1202e-01,\n",
       "            -4.2569e-01, -3.7706e-01],\n",
       "           [-3.1595e-01, -5.1093e-01, -2.9845e-01,  ..., -4.7206e-01,\n",
       "            -6.2109e-01, -5.4576e-02],\n",
       "           [-3.5108e-01, -3.7750e-01, -6.3028e-01,  ..., -4.5661e-01,\n",
       "            -4.5010e-01, -2.1920e-01],\n",
       "           ...,\n",
       "           [-4.7965e-01, -4.0134e-01, -3.7297e-01,  ..., -3.3728e-01,\n",
       "            -2.5151e-02, -7.1189e-01],\n",
       "           [-3.1787e-01, -4.9412e-01, -3.6302e-01,  ..., -6.2151e-01,\n",
       "            -4.6484e-01, -2.7659e-01],\n",
       "           [-4.2617e-01, -3.6775e-01, -3.3828e-01,  ..., -7.0924e-01,\n",
       "            -8.5160e-01, -3.5227e-01]],\n",
       " \n",
       "          [[-1.0707e-01, -5.0468e-01, -4.0922e-01,  ..., -2.7541e-01,\n",
       "            -2.3112e-01, -2.9719e-01],\n",
       "           [-1.4698e-01, -1.8215e-01, -1.7582e-01,  ..., -1.1776e-01,\n",
       "            -4.8789e-03, -3.2585e-01],\n",
       "           [ 3.0057e-02, -3.2691e-01, -3.4537e-01,  ..., -4.2008e-01,\n",
       "            -3.8905e-01,  2.8453e-01],\n",
       "           ...,\n",
       "           [ 9.4308e-02, -1.6801e-01, -2.4150e-01,  ..., -5.5473e-01,\n",
       "             6.1188e-02,  8.4335e-02],\n",
       "           [-2.2085e-01,  3.3520e-02, -1.7839e-01,  ..., -1.6869e-01,\n",
       "            -3.1278e-01, -2.5274e-01],\n",
       "           [ 1.1805e-01,  2.6655e-01, -8.3708e-02,  ...,  2.1000e-01,\n",
       "             1.2029e-02,  3.0077e-06]]]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = (32, 3, 299, 299)\n",
    "inp = torch.rand(inp_size).to('cuda:0')\n",
    "convLayer1 = nn.Conv2d(3, 192, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)).to('cuda:0')\n",
    "_calculate_time_and_memory(convLayer1.forward, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1,1)).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp = torch.rand((1,1)).to(0)\n",
    "inp.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1,129)).to(0)\n",
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del fc1\n",
    "    del fc2\n",
    "    del fc3\n",
    "    del inp\n",
    "    del out\n",
    "    del out1\n",
    "    \n",
    "    del adn\n",
    "    del y\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (32, 3, 299, 299)\n",
    "inp = torch.rand(inp_size).to('cuda:0')\n",
    "convLayer1 = nn.Conv2d(3, 64*8, 20).to('cuda:0')\n",
    "with TorchTracemalloc() as tt:\n",
    "    out = convLayer1(inp)\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N   = 50\n",
    "with TorchTracemalloc() as tt:\n",
    "    fc1 = nn.Linear(N, 1000).to('cuda:0')\n",
    "    fc2 = nn.Linear(1000, 1000).to('cuda:0')\n",
    "    fc3 = nn.Linear(1000, 1000).to('cuda:0')\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1, N)\n",
    "with TorchTracemalloc() as tt:\n",
    "    inp = torch.rand(inp_size).to('cuda:0')\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    #with torch.no_grad():\n",
    "    if 1:\n",
    "        out = fc3(fc2(fc1(inp)))\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    adn = torch.rand((1,1000)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    y = out + adn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adn.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    target = torch.rand((1,1000)).to('cuda:0')\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    loss_fn = nn.MSELoss()  # LogSoftmax + ClassNLL Loss\n",
    "    err = loss_fn(out, target)\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    err.backward()\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    fc1.zero_grad(set_to_none=True)\n",
    "print(tt.peak-tt.used)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "fc1 = nn.Linear(N, 1000).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate(function, *input):\n",
    "\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    with TorchTracemalloc() as tt:\n",
    "    #if 1:\n",
    "        start_time = time.time()\n",
    "        result = function(*input)\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_wrapper(module, *input):\n",
    "    old_fwd = module.forward\n",
    "    t, res = _calculate(old_fwd, *input)\n",
    "    print(\"Time: \", t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(module, *input):\n",
    "    print(\"I am the dummy forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1.forward = dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1,N)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1.forward = dummy.__get__(fc1, fc1.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1.forward = fwd_wrapper.__get__(fc1, fc1.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fc1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate(function, *input):\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    with TorchTracemalloc() as tt:\n",
    "    #if 1:\n",
    "        start_time = time.time()\n",
    "        result = function(*input)\n",
    "        torch.cuda.synchronize('cuda:0')\n",
    "        stop_time = time.time()\n",
    "    return (stop_time - start_time) * 1000, result\n",
    "\n",
    "def fwd_wrapper(module, *input):\n",
    "    t, res = _calculate(old_fwd, *input)\n",
    "    print(\"Time: \", t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "old_fwd = conv1.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1, 3, 299, 299)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.forward = fwd_wrapper.__get__(conv1, conv1.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    res = conv1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autograd testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 3*a**3\n",
    "a1.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = b**2\n",
    "b1.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 2*a1 - 5*b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_bwd = Q.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_bwd(ctx, grad_output):\n",
    "    print(\"I am inside bwd!\")\n",
    "    out = old_bwd(ctx, grad_output)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo: using hooks vs modifying the forward function\n",
    "\n",
    "############### Using forward Wrapper #################\n",
    "\n",
    "conv2 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "inp = torch.rand((1, 3, 299, 299)).to('cuda:0')\n",
    " \n",
    "start_time = 0\n",
    "stop_time  = 0\n",
    "times = [start_time, stop_time]\n",
    "\n",
    "## save original forward function\n",
    "original_forward = conv2.forward\n",
    "\n",
    "## define a wrapper around the original forward function\n",
    "def modified_forward(module, *input):\n",
    "    #print(\"I am about to start the forward run!\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[0] = time.time()\n",
    "    out = original_forward(module,*input)\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[1] = time.time()\n",
    "    #print(\"I am done with the forward\")\n",
    "    return out\n",
    "\n",
    "## set wrapper as the new forward \n",
    "conv2.forward = modified_forward\n",
    "\n",
    "## forward run\n",
    "metrics = []\n",
    "for _ in range(500):\n",
    "    out2 = conv2(inp)\n",
    "    t = (times[1] - times[0])*1000.0\n",
    "    #print(t)\n",
    "    metrics.append(t)\n",
    "print(\"*****************************************\")\n",
    "wrapper_mean = np.mean(metrics)\n",
    "print(wrapper_mean)\n",
    "print(np.std(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### Using forward hooks #########################\n",
    "\n",
    "## Setup\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "\n",
    "start_time = 0\n",
    "stop_time  = 0\n",
    "times = [start_time, stop_time]\n",
    "\n",
    "## pre-forward hook\n",
    "def print_pre(module, input):\n",
    "    #print(\"I am about to start the forward run!\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[0] = time.time()\n",
    "\n",
    "# # post-forward hook  \n",
    "def print_post(module ,input, output):\n",
    "    #print(\"I am done with the forward\")\n",
    "    torch.cuda.synchronize('cuda:0')\n",
    "    times[1] = time.time()\n",
    "\n",
    "## Register the hooks\n",
    "conv1.register_forward_pre_hook(print_pre)\n",
    "conv1.register_forward_hook(print_post)\n",
    "\n",
    "\n",
    "metrics = []\n",
    "for _ in range(500):\n",
    "    out1 = conv1(inp)\n",
    "    t = (times[1] - times[0])*1000.0\n",
    "    #print(t)\n",
    "    metrics.append(t)\n",
    "print(\"*****************************************\")\n",
    "hook_mean = np.mean(metrics)\n",
    "print(hook_mean)\n",
    "print(np.std(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(wrapper_mean - hook_mean)/hook_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m0, _  = print_mem(0,0)\n",
    "conv2 = nn.Conv2d(3, 640, kernel_size=11, stride=4, padding=2).to('cuda:0')\n",
    "m1, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual model mem: \", m1-m0)\n",
    "estimate_model_size(conv2, 'GB')\n",
    "\n",
    "inp = torch.rand((1, 3, 2990, 2990)).to('cuda:0')\n",
    "m2, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual input mem: \", m2-m1)\n",
    "estimate_input_size(inp, 'GB')\n",
    "\n",
    "with TorchTracemalloc() as tt:\n",
    "    output = conv2(inp)\n",
    "print(tt.used, tt.peaked)\n",
    "out_size = estimate_input_size(output, 'B')\n",
    "assert (tt.used==512*np.ceil(out_size/512)) # since memory is allotted in blocks of 512B\n",
    "print(\"Peak and used difference:\", tt.peaked - tt.used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del output\n",
    "del conv2\n",
    "del inp\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m0, _  = print_mem(0,0)\n",
    "N = 50000\n",
    "fc1 = nn.Linear(N, 1000).to('cuda:0')\n",
    "m1, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual model mem: \", m1-m0)\n",
    "estimate_model_size(fc1, 'GB\\n')\n",
    "\n",
    "inp = torch.rand((1,N)).to('cuda:0')\n",
    "m2, _  = print_mem(0,0)\n",
    "\n",
    "print(\"Actual input mem: \", m2-m1)\n",
    "estimate_input_size(inp, 'GB')\n",
    "\n",
    "with TorchTracemalloc() as tt:\n",
    "    output = fc1(inp)\n",
    "print(tt.used, tt.peaked)\n",
    "out_size = estimate_input_size(output, 'B')\n",
    "assert (tt.used==512*np.ceil(out_size/512)) # since memory is allotted in blocks of 512B\n",
    "print(\"Peak and used difference:\", tt.peaked - tt.used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del output\n",
    "del fc1\n",
    "del inp\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.add('0')\n",
    "b.add('9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int = 1000, factor: int = 1) -> None:\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.linear1N = 4096\n",
    "        self.linear2N = 4096\n",
    "        self.linear3N = 4096\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(10000,  self.linear1N)\n",
    "        self.rl1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.linear1N, self.linear2N)\n",
    "        self.rl2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.linear2N, self.linear3N)\n",
    "        self.rl3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(self.linear3N, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.rl1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.rl2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.rl3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "make_dot is modified to add nodes for only the autograd corresponding to layers\n",
    "'''\n",
    "\n",
    "def make_dot(var, cur_model):\n",
    "    \"\"\"\n",
    "    this function build a DiGraph for the model, by tracing the grad function of each layer's output\n",
    "    :return: the DiGraph\n",
    "    \"\"\"\n",
    "    dot = nx.DiGraph()\n",
    "    seen = set()\n",
    "    output_nodes = (var.grad_fn,) if not isinstance(var, tuple) else tuple(v.grad_fn for v in var)\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            cur_id = None\n",
    "            if var.metadata != {}:\n",
    "                if ('module' in var.metadata):\n",
    "                    # this submodule has a forward function, so it's information is previously recorded in Profiling\n",
    "                    cur_id = id(var.metadata['module'])\n",
    "                    # retrieve the node representing this submodule\n",
    "                    cur_node = cur_model.sub_module_nodes[id(var.metadata['module'])]\n",
    "                    dot.add_node(id(var.metadata['module']), \n",
    "                                 model = str(cur_node.module), \n",
    "                                 name = str(cur_node.name), \n",
    "                                 weight=cur_node.weight_forward,\n",
    "                                 reverse_weight=cur_node.weight_backward,\n",
    "                                 id=id(var.metadata['module']), \n",
    "                                 topo_order=id(var.metadata['module']), \n",
    "                                 temporary_memory=cur_node.temporary_memory, \n",
    "                                 persistent_memory=cur_node.persistent_memory,\n",
    "                                 output_memory=[cur_node.output_memory], \n",
    "                                 output_tensors=cur_node.output_memory, \n",
    "                                 colocation_group=\"\")\n",
    "                    \n",
    "                    if hasattr(var, 'next_functions'):\n",
    "                        for u in var.next_functions:\n",
    "                            if u[0] is not None and torch.is_tensor(u[0]) is False and hasattr(u[0], 'variable') is False:\n",
    "                                if u[0].metadata != {}:\n",
    "                                    if ('module' in u[0].metadata):\n",
    "                                        next_id = id(u[0].metadata['module'])\n",
    "                                        cur_model.sub_module_nodes[next_id].children.add(cur_id)\n",
    "                                        cur_model.sub_module_nodes[cur_id].parent.add(next_id)\n",
    "                                    elif ('parent' in u[0].metadata):\n",
    "                                        u[0].metadata['parent'].add(cur_id)\n",
    "                                    else:\n",
    "                                        print(\"Error:\", u[0], \" has metadata that is neither module nor parent!\")\n",
    "                                        return 0\n",
    "                                else:\n",
    "                                    u[0].metadata['parent'] = set()\n",
    "                                    u[0].metadata['parent'].add(cur_id)\n",
    "                                    \n",
    "                                add_nodes(u[0])\n",
    "                                \n",
    "                elif ('parent' in var.metadata):\n",
    "                    cur_id_list = []\n",
    "                    for parent in var.metadata['parent']:\n",
    "                        cur_id_list.append(parent)\n",
    "                    if hasattr(var, 'next_functions'):\n",
    "                        for u in var.next_functions:\n",
    "                            if u[0] is not None and torch.is_tensor(u[0]) is False and hasattr(u[0], 'variable') is False:\n",
    "                                if u[0].metadata != {}:\n",
    "                                    if ('module' in u[0].metadata):\n",
    "                                        next_id = id(u[0].metadata['module'])\n",
    "                                        for cur_id in cur_id_list:\n",
    "                                            cur_model.sub_module_nodes[next_id].children.add(cur_id)\n",
    "                                            cur_model.sub_module_nodes[cur_id].parent.add(next_id)\n",
    "                                    elif ('parent' in u[0].metadata):\n",
    "                                        for cur_id in cur_id_list:\n",
    "                                            u[0].metadata['parent'].add(cur_id)\n",
    "                                    else:\n",
    "                                        print(\"Error:\", u[0], \" has metadata that is neither module nor parent!\")\n",
    "                                        return 0\n",
    "                                else:\n",
    "                                    u[0].metadata['parent'] = set()\n",
    "                                    for cur_id in cur_id_list:\n",
    "                                        u[0].metadata['parent'].add(cur_id)\n",
    "                                add_nodes(u[0])\n",
    "                \n",
    "            else:\n",
    "                ## All functions will have either 'module' or 'parent' metadata\n",
    "                print(\"Error:\", var, \" does not have any metadata!\")\n",
    "                return 0\n",
    "\n",
    "            seen.add(var)\n",
    "\n",
    "    if isinstance(var, tuple):\n",
    "        # handle multiple outputs\n",
    "        for v in var:\n",
    "            add_nodes(v.grad_fn)\n",
    "    else:\n",
    "        add_nodes(var.grad_fn)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(*inputs):\n",
    "    input_list = list(inputs)\n",
    "    print(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [2]]\n"
     ]
    }
   ],
   "source": [
    "test_func(1,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = nn.Conv2d(3, 2, kernel_size=2)\n",
    "c2 = nn.Conv2d(2, 2, kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1, 3, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = c1(inp)\n",
    "out2 = c2(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 99, 99])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 98, 98])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = torch.flatten(out2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = nn.AdaptiveAvgPool2d((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = ap(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_forward_functions(model)\n",
    "del original_forwards\n",
    "del gpu_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _concatenateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super(TwoLayerLinearModel, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to('cuda:0')\n",
    "        self.fc2a = nn.Linear(self.linear2N, self.linear3N).to('cuda:1')\n",
    "        self.fc2b = nn.Linear(self.linear2N, self.linear3N).to('cuda:0')\n",
    "        self.concatenate = _concatenateLayer().to('cuda:0')\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N).to('cuda:0')\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N).to('cuda:0')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "\n",
    "        x1 = x.to('cuda:1')\n",
    "        xb = self.fc2b(x)\n",
    "        xa = self.fc2a(x1)\n",
    "        \n",
    "        xa = xa.to('cuda:0')\n",
    "        \n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerLinearModel_stream(nn.Module):\n",
    "\n",
    "    def __init__(self, factor: int = 1) -> None:\n",
    "        super(TwoLayerLinearModel_stream, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.linear1N = 512*self.factor\n",
    "        self.linear2N = 2048*self.factor\n",
    "        self.linear3N = 1024*self.factor\n",
    "        self.linear4N = 2*self.linear3N\n",
    "        self.linear5N = 512*self.factor\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear1N, self.linear2N).to('cuda:0')\n",
    "        self.fc2a = nn.Linear(self.linear2N, self.linear3N).to('cuda:1')\n",
    "        self.fc2b = nn.Linear(self.linear2N, self.linear3N).to('cuda:0')\n",
    "        self.concatenate = _concatenateLayer().to('cuda:0')\n",
    "        self.fc3 = nn.Linear(self.linear4N, self.linear5N).to('cuda:0')\n",
    "        self.fc4 = nn.Linear(self.linear5N, self.linear5N).to('cuda:0')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to('cuda:0')\n",
    "        x = self.fc1(x)\n",
    "        xb = self.fc2b(x)\n",
    "\n",
    "        with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "        #if 1:\n",
    "            with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "            #if 1:\n",
    "                x = x.to('cuda:1')\n",
    "\n",
    "        xa = self.fc2a(x)\n",
    "        \n",
    "        ## Order of defining streams (0 first and then 1) matters a little\n",
    "        #with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "        if 1:\n",
    "            #with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "            if 1:\n",
    "                xa = xa.to('cuda:0')\n",
    "        \n",
    "        y = self.concatenate(xa,xb)\n",
    "        y = self.fc3(y)\n",
    "        y = self.fc4(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.43326044 GB\n",
      "Cached:    0.89257812 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.20395136 GB\n",
      "Cached:    0.43359375 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 5\n",
    "model = TwoLayerLinearModel_stream(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.84935236 GB\n",
      "Cached:    0.89257812 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.39928293 GB\n",
      "Cached:    0.43359375 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (128, 512*factor)\n",
    "inp   = torch.rand(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 7.3741912841796875\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(20):\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        start = time.time()\n",
    "        output = model(inp)\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        end = time.time()\n",
    "        times.append(1000*(end-start))\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "print(\"Mean time taken:\", np.mean(times[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.43326044 GB\n",
      "Cached:    0.89257812 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.20395136 GB\n",
      "Cached:    0.43359375 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.00374079 GB\n",
      "Cached:    0.89257812 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.00373697 GB\n",
      "Cached:    0.43359375 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del inp\n",
    "del output\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "0.31549930572509766\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros((1000, 1000)).to(0)\n",
    "b = torch.ones((1000, 1000)).to(0)\n",
    "\n",
    "start = time.time()\n",
    "        \n",
    "#with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "if 1:\n",
    "    c = 0\n",
    "    for i in range(10):\n",
    "        c = c + (torch.sum(torch.count_nonzero(a)))\n",
    "        time.sleep(0.005)\n",
    "\n",
    "for i in range(50):\n",
    "        a = a+b\n",
    "        time.sleep(0.005)\n",
    "print(c)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-219501de16b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "del a\n",
    "del b\n",
    "del c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc0 = nn.Linear(1000, 1000).to(0)\n",
    "fc1 = nn.Linear(1000, 1000).to(1)\n",
    "\n",
    "def junk1(x):\n",
    "    x = x.to(0)\n",
    "    x = fc0(x)\n",
    "    x = x.to(1)\n",
    "    x = fc1(x)\n",
    "    x = x.to(0)\n",
    "    return x\n",
    "    \n",
    "def junk2(x):\n",
    "    x = x.to(0)\n",
    "    x = fc0(x)\n",
    "    with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "            with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "                x = x.to(1)\n",
    "    x = fc1(x)\n",
    "    x = x.to(0)\n",
    "    return x\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0983, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(50):\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "        inp  = torch.ones((1,1000)).to(0)\n",
    "        out1 =  junk1(inp)\n",
    "        out2 =  junk2(inp)\n",
    "        #print(out1==out2)\n",
    "        torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "print(out1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a\n",
    "del fc\n",
    "del a1\n",
    "del out\n",
    "del fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "nums = tuple(random.random() for _ in range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.909421920776367\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    inp_list = list(nums)\n",
    "    out = tuple(inp_list)\n",
    "print((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp  = torch.ones((1000,1000)).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005660057067871094\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    inp2 = inp.to(0)\n",
    "print((time.time()-start)*1000/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8403, 0.5308]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((1,2))\n",
    "b = torch.rand((1,2))\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.rand((100000,10000)).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = l.to(1)\n",
    "\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "    c = torch.sum(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1632., device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n",
    "\n",
    "d = torch.sum(b)\n",
    "print(c-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "del l\n",
    "del b, c\n",
    "del d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(500100.9181, device='cuda:1', dtype=torch.float64)\n",
      "tensor(500100.9181, device='cuda:1', dtype=torch.float64)\n",
      "tensor(500100.9181, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0., device='cuda:1', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#l = torch.rand((1000,1000), dtype = torch.double) # No problem with this\n",
    "l = torch.rand((1000,1000), dtype = torch.double).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "b = l.to(1)\n",
    "    \n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "# No sum diff if this is here\n",
    "# torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "    stream_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(stream_sum)\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(stream_sum-default_sum)\n",
    "\n",
    "del l\n",
    "del b, stream_sum\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache() #IF this is there the  the sum diff is always negative\n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(500228.1464, device='cuda:1', dtype=torch.float64)\n",
      "tensor(499980.1120, device='cuda:1', dtype=torch.float64)\n",
      "tensor(500228.1464, device='cuda:0', dtype=torch.float64)\n",
      "tensor(248.0344, device='cuda:1', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#l = torch.rand((1000,1000), dtype = torch.double) # No problem with this\n",
    "l = torch.rand((1000,1000), dtype = torch.double).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[0]):\n",
    "    with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "        b = l.to(1)\n",
    "    \n",
    "with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "    stream_sum = torch.sum(b, dtype = torch.double)\n",
    "    \n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(stream_sum)\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(stream_sum-default_sum)\n",
    "\n",
    "## Here stream sum is equal to actual sum!!!\n",
    "\n",
    "del l\n",
    "del b, stream_sum\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache() #IF this is there the  the sum diff is always negative\n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65622272., device='cuda:1', dtype=torch.float64)\n",
      "tensor(0., device='cuda:0', dtype=torch.float64)\n",
      "tensor(65622272., device='cuda:1', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "l = (n*torch.ones((10000,10000), dtype = torch.double)).to(0)\n",
    "actual_sum = torch.sum(l, dtype = torch.double)\n",
    "\n",
    "with torch.cuda.stream(COMPUTE_STREAM[0]):   # No probblem if there's only one of the two streams\n",
    "    with torch.cuda.stream(COMPUTE_STREAM[1]):\n",
    "        b = l.to(1)\n",
    "\n",
    "default_sum = torch.sum(b, dtype = torch.double)\n",
    "\n",
    "print(default_sum)\n",
    "print(actual_sum)\n",
    "print(default_sum-actual_sum.to(1))\n",
    "\n",
    "del l\n",
    "del b,\n",
    "del default_sum\n",
    "del actual_sum\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache() #If this is there the  the sum diff is always negative\n",
    "torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2); torch.cuda.synchronize(3)\n",
    "n=1-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModuleNode:\n",
    "    \"\"\"\n",
    "    This class represents a submodel (ex. conv2d layer) in the given model (ex. inception_v3). \n",
    "    It is represented as a node in the return graph\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # store the entire submodel\n",
    "        self.module = None\n",
    "        # submodel name\n",
    "        self.name = None\n",
    "\n",
    "        # nodes that must finish processing before this node (direct dependencies)\n",
    "        self.parent = set()\n",
    "        # nodes that depends on this node\n",
    "        self.children = set()\n",
    "\n",
    "        # forward function's estimated runtime\n",
    "        self.weight_forward = 0\n",
    "        # backward function's estimated runtime\n",
    "        self.weight_backward = 0\n",
    "        # id represented by the model's location (python's id function)\n",
    "        self.id_hash = None\n",
    "        # sudo id used, for one model, this sudo id starts from 0 and add 1 for each new node\n",
    "        # -- self.id = None\n",
    "        # storage used by submodel's parameters (weight, bias)\n",
    "        self.persistent_memory = 0\n",
    "        # submodel's input's size\n",
    "        self.input_memory = 0\n",
    "        # submodel's output's size\n",
    "        self.output_memory = 0\n",
    "        # temporary memory used in forward run\n",
    "        self.temporary_memory = 0\n",
    "        \n",
    "        # gpu assigned to the submodule\n",
    "        self.p = None\n",
    "        \n",
    "########################################################################\n",
    "\n",
    "class Profiling:\n",
    "    \"\"\"\n",
    "    This class produce the profile, this class referenced \"https://github.com/msr-fiddle/pipedream\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, gpu=0, rounds=20, input_size=(50, 10)):\n",
    "        \"\"\"\n",
    "        model: ex. inception_v3 model, alexnet model, etc\n",
    "        gpu: choose in between {0,1,2,3}\n",
    "        rounds: number of rounds to run the profiling\n",
    "        \"\"\"\n",
    "        self.gpu = gpu\n",
    "        self.model = model.to(self.gpu)\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.rounds = rounds\n",
    "        # first few rounds are inaccurate, so I choose to discard the results from the first 1/4 rounds\n",
    "        self.ignore_rounds = int(self.rounds/4)\n",
    "        # counting variable, runs from 0 - self.rounds\n",
    "        self.cur_round = 0\n",
    "\n",
    "        # used to calculate backward runtime for each submodule\n",
    "        self.back_record = []\n",
    "        # all submodules record of the form {id of the layer(submodule) : SubModuleNode created out of tha layer}\n",
    "        self.sub_module_nodes = {}\n",
    "        # use id_hash to record the order of submodules's execution\n",
    "        self.submodule_order = []\n",
    "\n",
    "        # internal use only, record the original forward functions for submodules\n",
    "        self.forward_original_methods = {}\n",
    "        # internal use only, switch back to the original forward functions after profiling\n",
    "        self.detach_record = set()\n",
    "        # Collect handles to all hooks added, so as to remove them in detach()\n",
    "        self.hook_handles = []\n",
    "\n",
    "\n",
    "    def recur_function(self, module):\n",
    "        \"\"\"\n",
    "        modify self.model: adding forward timing, backward timing, input output sizes, etc\n",
    "        :param module: the model to recursively add forward/backward wrappers to\n",
    "        \"\"\"\n",
    "        this_profiler = self\n",
    "        sub_modules = module.__dict__['_modules']\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            # sub modules of sub_module, if there are more than 1, we need further recursion\n",
    "            sub_sub_modules = sub_module.__dict__['_modules']\n",
    "            if len(sub_sub_modules) > 0:\n",
    "                self.recur_function(sub_module)\n",
    "                continue\n",
    "            \n",
    "            def _calculate_time_and_memory(function, *input):\n",
    "                \"\"\"\n",
    "                - Helper function in forward wrapper\n",
    "                - Calculates forward runtime, peak memory used and static memory used\n",
    "                - Verified: Memory measurement context doesn't add overhead to\n",
    "                  time measurement\n",
    "                \"\"\"\n",
    "                with TorchTracemalloc(self.gpu) as tt:\n",
    "                    torch.cuda.synchronize(self.gpu)\n",
    "                    start_time = time.time()\n",
    "                    result = function(*input)\n",
    "                    torch.cuda.synchronize(self.gpu)\n",
    "                    stop_time = time.time()\n",
    "                return (stop_time - start_time) * 1000, tt.used, tt.peaked , result\n",
    "\n",
    "            def forward_wrapper(cur_module, *input):\n",
    "                \"\"\"\n",
    "                use this wrapper to replace the original forward function in submodules\n",
    "                :param cur_module: the input submodule\n",
    "                \"\"\"\n",
    "                # original forward function\n",
    "                \n",
    "                function = this_profiler.forward_original_methods[cur_module]\n",
    "                if this_profiler.cur_round < this_profiler.ignore_rounds:\n",
    "                    if this_profiler.cur_round == 0:\n",
    "                        # record submodule execution order only in the first round\n",
    "                        print('-->', \"Module name: \",cur_module)\n",
    "                        this_profiler.submodule_order.append(id(cur_module))\n",
    "                    # do not record first few rounds\n",
    "                    result = function(*input)\n",
    "                    return result\n",
    "                \n",
    "                ## collect relevant information of cur module\n",
    "                forward_time, used_mem, peak_mem, result = _calculate_time_and_memory(function, *input)\n",
    "                \n",
    "                ## Input size in bytes\n",
    "                input_size = 0\n",
    "                for inp in input:\n",
    "                    input_size = input_size + estimate_tensor_size(inp, 'B')\n",
    "                \n",
    "                ## Model size in bytes\n",
    "                persistent_memory = estimate_model_size(cur_module,'B', False)\n",
    "\n",
    "                output_memory = estimate_tensor_size(result, 'B')\n",
    "                \n",
    "                '''\n",
    "                if not(used_mem==512*np.ceil(output_memory/512)):\n",
    "                    print('*'*50)\n",
    "                    print(\"In sumodule \", cur_module , ':' )\n",
    "                    print(\"Output memory is: \", output_memory)\n",
    "                    print(\"But used memory is: \", used_mem)\n",
    "                    print(\"They dont match upto a factor of 512 (since mem bolcks are alotted in 512 byte locks) as expected\")\n",
    "                    print('*'*50)\n",
    "                '''\n",
    "                    \n",
    "                temporary_memory = peak_mem - used_mem\n",
    "\n",
    "                # record a SubModuleNode for each model layer\n",
    "                if id(cur_module) not in this_profiler.sub_module_nodes:\n",
    "                    cur_node = SubModuleNode()\n",
    "                    cur_node.id_hash = id(cur_module)\n",
    "                    cur_node.module = cur_module\n",
    "                    cur_node.name = cur_module.__class__.__name__\n",
    "                    \n",
    "                    #***********?????????????????????????????????????????***************************\n",
    "                    ########## REMOVE THIS ######################\n",
    "                    cur_node.persistent_memory = persistent_memory\n",
    "                    cur_node.temporary_memory = temporary_memory\n",
    "                    cur_node.output_memory = output_memory\n",
    "                    cur_node.input_memory = input_size\n",
    "                    #############################################\n",
    "                    #***********?????????????????????????????????????????***************************\n",
    "                    \n",
    "                    ### And Uncomment this\n",
    "                    #cur_node.persistent_memory = persistent_memory\n",
    "                    #cur_node.temporary_memory = temporary_memory\n",
    "                    #cur_node.output_memory = output_memory\n",
    "                    #cur_node.input_memory = input_size\n",
    "                    \n",
    "                    print(\"Module name: \", cur_node.name)\n",
    "                    print(\"Persistent Mem:\", cur_node.persistent_memory)\n",
    "                    print(\"Temporary Mem:\", cur_node.temporary_memory )\n",
    "                    print(\"Output Mem:\", cur_node.output_memory)\n",
    "                    \n",
    "                else:\n",
    "                    cur_node = this_profiler.sub_module_nodes[id(cur_module)]\n",
    "                # we want weight_forward as the average forward runtime of the relevent rounds\n",
    "                cur_node.weight_forward += forward_time / (this_profiler.rounds - this_profiler.ignore_rounds)\n",
    "                this_profiler.sub_module_nodes[id(cur_module)] = cur_node\n",
    "\n",
    "                return result\n",
    "\n",
    "            def hook(cur_module, inputs, output):\n",
    "                # this is for retriving the module inside make dot function\n",
    "                if isinstance(output, tuple):\n",
    "                    for i in range(len(output)):\n",
    "                        print(output[i]\n",
    "                        \n",
    "                        #otp.grad_fn.metadata['module'] = cur_module\n",
    "                else:\n",
    "                    output.grad_fn.metadata['module'] = cur_module\n",
    "                print(\"*\"*50)\n",
    "\n",
    "            def backward_post_hook(cur_module, input, output):\n",
    "                \"\"\"\n",
    "                add backward hook to record backward runtime\n",
    "                :param cur_module: the input submodule\n",
    "                \"\"\"\n",
    "                if this_profiler.cur_round < this_profiler.ignore_rounds:\n",
    "                    # do not record first few rounds\n",
    "                    return\n",
    "                torch.cuda.synchronize(0)\n",
    "                cur_time = time.time() * 1000\n",
    "                this_profiler.back_record.append((id(cur_module), cur_time))\n",
    "\n",
    "            if sub_module in self.forward_original_methods:\n",
    "                # only record the original forward functions once\n",
    "                continue\n",
    "\n",
    "            self.forward_original_methods[sub_module] = sub_module.forward\n",
    "            sub_module.forward = forward_wrapper.__get__(sub_module, sub_module.__class__)\n",
    "            fhook_handle = sub_module.register_forward_hook(hook)\n",
    "            bhook_handle =  sub_module.register_backward_hook(backward_post_hook)\n",
    "            this_profiler.hook_handles.append(fhook_handle)\n",
    "            this_profiler.hook_handles.append(bhook_handle)\n",
    "            \n",
    "            \n",
    "    def detach(self, module):\n",
    "        \"\"\"\n",
    "        use this helper function to detach all forward wrappers\n",
    "        \"\"\"\n",
    "        this_profiler = self\n",
    "        sub_modules = module.__dict__['_modules']\n",
    "        for name, sub_module in sub_modules.items():\n",
    "            sub_sub_modules = sub_module.__dict__['_modules']\n",
    "            if len(sub_sub_modules) > 0:\n",
    "                self.detach(sub_module)\n",
    "                continue\n",
    "            if sub_module in self.detach_record:\n",
    "                continue\n",
    "\n",
    "            self.detach_record.add(sub_module)\n",
    "            sub_module.forward = self.forward_original_methods[sub_module]\n",
    "        ## Remove all the hooks that were added\n",
    "        for handle in this_profiler.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return: the model's output of the final round\n",
    "        \"\"\"\n",
    "        self.sub_module_nodes = {}\n",
    "        self.recur_function(self.model)\n",
    "\n",
    "        dataset = torch.randint(self.input_size[0], (self.rounds * int(args.batch_size), 2), dtype=torch.long).to(self.gpu)\n",
    "\n",
    "        for batch_idx in range(self.rounds):\n",
    "            inp = dataset[ int(args.batch_size)*(self.rounds-1):int(args.batch_size)*(self.rounds) ]\n",
    "            self.cur_round = batch_idx\n",
    "            \n",
    "            hidden = model.init_hidden(int(args.batch_size))\n",
    "\n",
    "            torch.cuda.synchronize(self.gpu)\n",
    "            output = self.model(inp, hidden)\n",
    "            torch.cuda.synchronize(self.gpu)\n",
    "\n",
    "        self.detach(self.model)\n",
    "        return output\n",
    "\n",
    "########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size,\n",
    "            int(hidden_size/2),  # Bi-directional processing will ouput vectors of double size, therefore I reduced output dimensionality\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    # word_inputs: (batch_size, seq_length), h: (h_or_c, layer_n_direction, batch, seq_length)\n",
    "    def forward(self, word_inputs, hidden_in):         \n",
    "        # embedded (batch_size, seq_length, hidden_size)\n",
    "        gpuid = word_inputs.get_device()\n",
    "\n",
    "        hidden = (hidden_in[0].to(gpuid), hidden_in[1].to(gpuid))\n",
    "        \n",
    "        embedded = self.embedding(word_inputs)\n",
    "        # output (batch_size, seq_length, hidden_size*directions)\n",
    "        # hidden (h: (num_layers*directions, batch_size, hidden_size),\n",
    "        #         c: (num_layers*directions, batch_size, hidden_size))\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batches):\n",
    "        #hidden = torch.zeros(2, self.n_layers*2, batches, int(self.hidden_size/2))\n",
    "        h_s = torch.zeros(self.n_layers*2, batches, int(self.hidden_size/2))\n",
    "        c_s = torch.zeros(self.n_layers*2, batches, int(self.hidden_size/2))\n",
    "        hidden = (h_s, c_s)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "hidden_dim = 10\n",
    "n_layers = 2\n",
    "\n",
    "model = EncoderRNN(vocab_size, hidden_dim, n_layers).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (vocab_size, hidden_dim)\n",
    "tester = Profiling(model, args.prof_gpu_id, args.prof_rounds, input_size = inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Module name:  Embedding(50, 10)\n",
      "**************************************************\n",
      "--> Module name:  LSTM(10, 5, num_layers=2, batch_first=True, bidirectional=True)\n",
      "torch.Size([32, 2, 10])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d17095487b16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-d1616e48a3d2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-e295a777c89b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_inputs, hidden_in)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# hidden (h: (num_layers*directions, batch_size, hidden_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#         c: (num_layers*directions, batch_size, hidden_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                     \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     self._forward_hooks.values()):\n\u001b[0;32m-> 1076\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-d1616e48a3d2>\u001b[0m in \u001b[0;36mhook\u001b[0;34m(cur_module, inputs, output)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0;31m#otp.grad_fn.metadata['module'] = cur_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "tester.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(50, 10)\n",
      "  (lstm): LSTM(10, 5, num_layers=2, batch_first=True, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "hidden_dim = 10\n",
    "n_layers = 2\n",
    "\n",
    "model = EncoderRNN(vocab_size, hidden_dim, n_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = torch.LongTensor([[1, 2, 30, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_hidden = model.init_hidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_outputs, module_hidden = moduleRNN(word_input, module_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randint(vocab_size, (20, 7), dtype=torch.long)\n",
    "otp = torch.randint(vocab_size, (20, 10), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Profiling(model, gpu, rounds, input_size = inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 5\n",
    "n_layers = 2\n",
    "\n",
    "ls = nn.LSTM(\n",
    "            hidden_size,\n",
    "            int(hidden_size/2),  # Bi-directional processing will ouput vectors of double size, therefore I reduced output dimensionality\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
    "            bidirectional=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2240],\n",
      "        [-0.4726],\n",
      "        [ 1.0271],\n",
      "        [-1.0378]])\n",
      "tensor([[[ 0.9209]],\n",
      "\n",
      "        [[-1.2188]],\n",
      "\n",
      "        [[ 0.0925]]])\n",
      "tensor([[[ 0.2240],\n",
      "         [-0.4726],\n",
      "         [ 1.0271],\n",
      "         [-1.0378]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6969],\n",
       "         [ 1.3935],\n",
       "         [-0.1062],\n",
       "         [ 1.9587]],\n",
       "\n",
       "        [[-1.4428],\n",
       "         [-0.7462],\n",
       "         [-2.2459],\n",
       "         [-0.1810]],\n",
       "\n",
       "        [[-0.1315],\n",
       "         [ 0.5651],\n",
       "         [-0.9346],\n",
       "         [ 1.1303]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 1)\n",
    "b = torch.randn(4, 1)\n",
    "print(b)\n",
    "print(a[:,None,:])\n",
    "print(b[None,:,:])\n",
    "\n",
    "a[:,None,:]-b[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
