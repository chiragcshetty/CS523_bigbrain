{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca834d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.inception_modified as inception_modified\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import simple_model as sm\n",
    "import dummyModels as dm\n",
    "\n",
    "\n",
    "######## For profiler (some experiments. Not required) #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "#########################################\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cdccbee",
   "metadata": {},
   "source": [
    "from baechi_units import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0e18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451db3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bb8da",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694df43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ParallelModelThreeLayerSplit\"\n",
    "batch_size = 32\n",
    "fct = 6\n",
    "\n",
    "Nrun = 3\n",
    "run_type = \"training\" \n",
    "repetable = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0962f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"ParallelModel\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 0\n",
    "    model = dm.parallelModel(fct).to(single_run_gpu)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelSplit\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 3\n",
    "    model = dm.parallelModelSplit(fct,[single_run_gpu,0], repetable)\n",
    "    opt_size = 512*fct\n",
    "\n",
    "if model_name == \"ParallelModelThreeLayer\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 0\n",
    "    model = dm.parallelModelThreeLayer(fct).to(single_run_gpu)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelThreeLayerSplit\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 2\n",
    "    model = dm.parallelModelThreeLayerSplit(fct,[single_run_gpu,1], repetable)\n",
    "    opt_size = 512*fct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6481acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (batch_size,) + inp_size_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3d9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for _ in range(Nrun):\n",
    "            #torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu)\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6213d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp   = torch.ones(inp_size).to(single_run_gpu)\n",
    "output = model(inp)\n",
    "last_gpu = output.get_device()\n",
    "\n",
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    #if 1:\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn((batch_size, opt_size)).to(last_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_split2.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3843e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3e31f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 3.3061986 GB\n",
      "Cached:    3.86328125 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd038674",
   "metadata": {},
   "source": [
    "## Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd37709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "fct = 6\n",
    "\n",
    "Nrun = 30 \n",
    "run_type = \"training\" \n",
    "repetable = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ba3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size_single = (1, 512*fct)\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "opt_size = 512*fct\n",
    "\n",
    "model1 = dm.parallelModelThreeLayerSplit(fct,[1,1], 0)\n",
    "model2 = dm.parallelModelThreeLayerSplit(fct,[2,2], 0)\n",
    "model3 = dm.parallelModelThreeLayerSplit(fct,[2,1], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b521e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 86.74815893173218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_gpu1 = 1\n",
    "last_gpu2 = 2\n",
    "last_gpu3 = 2\n",
    "\n",
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer1 = optim.SGD(model1.parameters(), lr = 0.0001); \n",
    "    optimizer2 = optim.SGD(model2.parameters(), lr = 0.0001); \n",
    "    optimizer3 = optim.SGD(model3.parameters(), lr = 0.0001); \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            \n",
    "            labels1 = torch.randn((batch_size, opt_size)).to(last_gpu1)\n",
    "            labels2 = torch.randn((batch_size, opt_size)).to(last_gpu2)\n",
    "            labels3 = torch.randn((batch_size, opt_size)).to(last_gpu3)\n",
    "            \n",
    "            start = time.time()\n",
    "            optimizer1.zero_grad();optimizer2.zero_grad();optimizer3.zero_grad()\n",
    "            \n",
    "            output1 = model1(inp)\n",
    "            output2 = model2(inp)\n",
    "            output3 = model3(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss1 = criterion(output1, labels1 )\n",
    "            loss2 = criterion(output2, labels2 )\n",
    "            loss3 = criterion(output3, labels3 )\n",
    "            ##################################################################################\n",
    "            loss1.backward(loss1)\n",
    "            loss2.backward(loss2)\n",
    "            loss3.backward(loss3)\n",
    "            \n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db761b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fdaed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 4.14967394 GB\n",
      "Cached:    4.703125 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 5.76782322 GB\n",
      "Cached:    6.32421875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model1, model2, model3\n",
    "del inp\n",
    "del output1, output2, output3\n",
    "try:\n",
    "    del labels1, labels2, labels3\n",
    "    del optimizer1, optimizer2, optimizer3\n",
    "    del loss1, loss2, loss3\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e722127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93834e3d",
   "metadata": {},
   "source": [
    "# With Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1243df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2cc233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model_split, fct, batch_size, Nrun, done_flag ): \n",
    "    inp_size_single = (1, 512*fct)\n",
    "    model = dm.parallelModelThreeLayerSplit(fct,model_split, 0)\n",
    "    \n",
    "    inp_size = (batch_size,) + inp_size_single\n",
    "    inp   = torch.ones(inp_size)\n",
    "    output = model(inp)\n",
    "    last_gpu = output.get_device()\n",
    "    opt_size = tuple(output.size())[1]\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn((batch_size, opt_size)).to(last_gpu)\n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n",
    "\n",
    "    del model\n",
    "    del inp\n",
    "    del output\n",
    "    try:\n",
    "        del labels\n",
    "        del optimizer\n",
    "        del loss\n",
    "    except: pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "    done_flag[0] = 1\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6118d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "fct = 6\n",
    "Nrun = 50 \n",
    "\n",
    "done_flag1 = [0]\n",
    "done_flag2 = [0]\n",
    "done_flag3 = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73851536",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = threading.Thread(target=run_train, args=([1,1], fct, batch_size, Nrun,done_flag1,))\n",
    "run2 = threading.Thread(target=run_train, args=([2,2], fct, batch_size, Nrun,done_flag2,))\n",
    "run3 = threading.Thread(target=run_train, args=([1,2], fct, batch_size, Nrun,done_flag3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce499f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1.start(); time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3cb5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.start(); time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25980c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbba2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "Mean time taken: 88.66408467292786\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.46199083 GB\n",
      "Cached:    2.64453125 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 4.14967394 GB\n",
      "Cached:    4.3671875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "[1] [1] [1]\n",
      "Mean time taken: 67.50410199165344\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.46199083 GB\n",
      "Cached:    2.64453125 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.84384155 GB\n",
      "Cached:    1.05859375 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "[1] [1] [1]\n",
      "Mean time taken: 55.809569358825684\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n",
      "[1] [1] [1]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    print(done_flag1, done_flag2, done_flag3)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd81d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
