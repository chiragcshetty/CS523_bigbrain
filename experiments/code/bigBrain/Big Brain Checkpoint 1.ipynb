{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d25bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "#############################################\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "#sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "#import reformated_models.inception_modified as inception_modified\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "#import simple_model as sm\n",
    "\n",
    "## Placer libs of baechi\n",
    "#sys.path.append('/home/cshetty2/sct')\n",
    "#from placer.placer_lib import *\n",
    "##############################################\n",
    "\n",
    "import dummyModels as dm\n",
    "\n",
    "\n",
    "######## For profiler (some experiments. Not required) #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "###############################Utilities#################################\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "        \n",
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")\n",
    "#########################################\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff1ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from random import expovariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073f78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968cda29",
   "metadata": {},
   "source": [
    "Experiment aims to demonstrate the advantage of packing training jobs on GPU (based on the resource requirements) rather than allotting integer number of GPUs per job (as container based approach would do)\n",
    "\n",
    "Measure: Throughput for these two cases and compare:\n",
    "- (1) one GPU allocation for each incoming jobs on FCFS basis\n",
    "- (2) decide a split of the incoming model among the two GPUs and run on best effort basis (the entire model maybe placed on one GPUs)\n",
    "\n",
    "Expectation: (2) should be more than (1) for two reasons:\n",
    "- granualr GPU allocation is more efficient\n",
    "- the model choosen itself runs faster when split across 2 gpus than 1 as described in next section\n",
    "\n",
    "Primitives:\n",
    "\n",
    "- GPUs : Setup consists of only 2 GPUs on one node\n",
    "- Jobs : All incoming jobs will be of same kind - to train a model dm.parallelModelThreeLayerSplit as described below\n",
    "- Training Script: run_train() is the main training routine. When a job arrives, a split for the model among two GPU's is \n",
    "- Job queue: jobs arrive as poisson process. Job queue maintains the cluster state and allots jobs with appropriate split while preventing OOMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0319b0a",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5835308",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "run_train: train the dm.parallelModelThreeLayerSplit model using given settings\n",
    "\n",
    "*****MODEL*******\n",
    "- dm.parallelModelThreeLayerSplit: Looks as below\n",
    "           ----|---|----                       .....Branch 1\n",
    "          /             \\\n",
    "     | --- ----|---|---------|-----|--(output) .....Branch 2\n",
    "    (L1)  \\             /  (L3)   (L4)\n",
    "           ----|---|----                       .....Branch 3\n",
    "           \n",
    "- On one GPU it runs as GPU1: (L1)(Branch1)(Branch2)(Branch3)(L3)(L4)\n",
    "- On 2 GPU's it runs as GPU1: (L1)(Branch1)(Branch2)(L3)(L4)\n",
    "                        GPU2:     (Branch3)\n",
    "                        \n",
    "- 2 GPU execution is about 20% faster\n",
    "- Even when provided 3 GPUs, fitting in 2 GPUs gives least step time\n",
    "\n",
    "-Inputs to the model:dm.parallelModelThreeLayerSplit(fct, model_split, repetable)\n",
    "    -fct : Factor by which all the layers will be scaled. eg: fct =2 will doubble size of all layers\n",
    "    -model_split : A list of ints [g1, g2]: g1, g2 are device ids for GPU1, GPU2 above\n",
    "        - Though not exact, for this exp we assume [gpu1,gpu1] occupies 0.6 of gpu1, [gpu1,gpu2] occupies 0.3 of each gpu1 and gpu2\n",
    "    -repetable : When repetable is non-zero, the model weights are initialized to 1/512 and biases to 0\n",
    "******************\n",
    "\n",
    "run_train arguments:\n",
    "- model_split, fct: As described above\n",
    "- batch_size: For training (done using fake dataset)\n",
    "- Nrun: Number of batches (steps)\n",
    "- done_flag: A singleton list (so that it is mutable) to indicate end of thread (contains avg step time)\n",
    "- exit_time: Time when job is completed\n",
    "\n",
    "'''\n",
    "\n",
    "def run_train(job_id, model_split, fct, batch_size, Nrun, done_flag, exit_time ): \n",
    "    mysplit = job_queue[job_id]['model_split']\n",
    "    \n",
    "    inp_size_single = (1, 512*fct)\n",
    "    model = dm.parallelModelThreeLayerSplit(fct, model_split, 0)\n",
    "    \n",
    "    inp_size = (batch_size,) + inp_size_single\n",
    "    inp   = torch.ones(inp_size)\n",
    "    output = model(inp)\n",
    "    last_gpu = output.get_device()      ## GPU on which labbels should be placed for training\n",
    "    opt_size = tuple(output.size())[1]\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:  \n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            ## NOTE: If more GPUs are used, take care to sync them all!\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn((batch_size, opt_size)).to(last_gpu)\n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #if 1 in mysplit: \n",
    "            #    torch.cuda.synchronize(1);\n",
    "            #if 2 in mysplit:\n",
    "            #    torch.cuda.synchronize(2);\n",
    "            \n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_split.json\")\n",
    "    if len(times)>10:\n",
    "        gpu_time = np.mean(times[10:])\n",
    "    else:\n",
    "        gpu_time = True\n",
    "    \n",
    "    #### Release memory\n",
    "    del model\n",
    "    del inp\n",
    "    del output\n",
    "    try:\n",
    "        del labels\n",
    "        del optimizer\n",
    "        del loss\n",
    "    except: pass\n",
    "    gc.collect()              ## To clean any circular references\n",
    "    torch.cuda.empty_cache() ## Empty cache used by Pytorch (does so across all threads)\n",
    "    #print_gpu_memory()\n",
    "    done_flag[0] = gpu_time\n",
    "    exit_time[0] = time.time()\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    #print(\"Job done. Exiting! My split was: \", job_queue[job_id]['model_split'])\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be865bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b1e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48fd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### GLOBALS #####################################3\n",
    "\n",
    "### dict of (gpu_id, fraction of gpu available)\n",
    "resource_manager   = {}  \n",
    "resource_manager[1]= 1\n",
    "resource_manager[2]= 1\n",
    "\n",
    "### dict of job_id -> dict{model_split, status_flag, arrival_time, entry_time, exit_time}\n",
    "job_queue          = {}  \n",
    "\n",
    "end_exp_flag = [0] #flag to indicate end of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439d3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_algo_1():\n",
    "    if resource_manager[1] == 1:\n",
    "        return [1,1]\n",
    "    elif resource_manager[2] == 1:\n",
    "        return [2,2]\n",
    "    else:\n",
    "        return None   \n",
    "    \n",
    "def get_split_algo_2():\n",
    "    if resource_manager[1] > 0.5:\n",
    "        return [1,1]\n",
    "    elif resource_manager[2] > 0.5:\n",
    "        return [2,2]\n",
    "    elif (resource_manager[1] > 0.2 and resource_manager[2] > 0.2):\n",
    "        return [1,2]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989006c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_resource(split, update_type):\n",
    "    if update_type == \"release\":\n",
    "        change =  1.0\n",
    "    elif update_type == \"acquire\":\n",
    "        change = -1.0\n",
    "     \n",
    "    if split == [1,1]:\n",
    "        resource_manager[1] = resource_manager[1] + 0.7*change\n",
    "    elif split == [2,2]:\n",
    "        resource_manager[2] = resource_manager[2] + 0.7*change\n",
    "    elif split == [1,2]:\n",
    "        resource_manager[1] = resource_manager[1] + 0.3*change\n",
    "        resource_manager[2] = resource_manager[2] + 0.3*change\n",
    "    else:\n",
    "        print(\"Error! split is invalid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9beaade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_arrivals(rate, T_exp):\n",
    "    exp_start_time  = time.time()\n",
    "    t = exp_start_time \n",
    "    job_id = 0\n",
    "    \n",
    "    while t < exp_start_time + T_exp:\n",
    "        time.sleep(expovariate(rate))\n",
    "        job_id = job_id+1\n",
    "        job_queue[job_id] = {'model_split':None, 'status_flag':[0], \\\n",
    "                             'arrival_time':[time.time()], 'entry_time':[0], 'exit_time':[0]}\n",
    "        t = time.time()\n",
    "        \n",
    "    end_exp_flag[0] = 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a4be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Jobs in process: 0\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 0\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 0\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 1\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 1\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 1\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************Mean time taken: 7.361626625061035\n",
      "\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  0\n",
      "********************\n",
      "********************Mean time taken: 6.524980068206787\n",
      "\n",
      "Jobs in process: 1\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  1\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 0\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 0\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 1\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 0\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 1\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 2\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  2\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-59a45e9d9ce0>\", line 72, in run_train\n",
      "    loss.backward(loss)\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/_tensor.py\", line 255, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 149, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 1; 7.80 GiB total capacity; 4.30 GiB already allocated; 85.31 MiB free; 4.46 GiB reserved in total by PyTorch)\n",
      "\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-59a45e9d9ce0>\", line 39, in run_train\n",
      "    model = dm.parallelModelThreeLayerSplit(fct, model_split, 0)\n",
      "  File \"../dummyModels.py\", line 291, in parallelModelThreeLayerSplit\n",
      "    model = ParallelModelThreeLayerSplit(factor, layers, repetable)\n",
      "  File \"../dummyModels.py\", line 187, in __init__\n",
      "    self.fc3   = nn.Linear(self.linear4N, self.linear5N).to(layers[0])\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 852, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 850, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 1; 7.80 GiB total capacity; 4.30 GiB already allocated; 85.31 MiB free; 4.46 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************Mean time taken: 6.509649753570557\n",
      "\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  2\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  3\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  3\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  3\n",
      "********************\n",
      "Mean time taken: 6.558477878570557\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  4\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  4\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  4\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  4\n",
      "********************\n",
      "Mean time taken: 6.554210186004639\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  5\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  5\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  5\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  5\n",
      "********************\n",
      "Mean time taken: 6.540036201477051\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  6\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  6\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  6\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 4\n",
      "Jobs Completed:  6\n",
      "********************\n",
      "Mean time taken: 6.535923480987549\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  7\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  7\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  7\n",
      "********************\n",
      "Mean time taken: 6.564188003540039\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 3\n",
      "Jobs Completed:  8\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 2\n",
      "Jobs Completed:  8\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  8\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  8\n",
      "********************\n",
      "Mean time taken: 6.501054763793945\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  9\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  9\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  9\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  9\n",
      "********************\n",
      "Mean time taken: 6.531918048858643\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  10\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  10\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  10\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  10\n",
      "********************\n",
      "Mean time taken: 6.584775447845459\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  11\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  11\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  11\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  11\n",
      "********************\n",
      "Mean time taken: 6.579732894897461\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  12\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  12\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  12\n",
      "********************\n",
      "Mean time taken: 6.506586074829102\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  13\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  13\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  13\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  13\n",
      "********************\n",
      "Mean time taken: 6.520318984985352\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  14\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  14\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  14\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  14\n",
      "********************\n",
      "Mean time taken: 6.574416160583496\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  15\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  15\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  15\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  15\n",
      "********************\n",
      "Mean time taken: 6.571054458618164\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  16\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  16\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  16\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  16\n",
      "********************\n",
      "Mean time taken: 6.53376579284668\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  17\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 6\n",
      "Jobs Completed:  17\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  17\n",
      "********************\n",
      "Mean time taken: 6.5232038497924805\n",
      "********************\n",
      "Jobs in process: 2\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  18\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  18\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  18\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  18\n",
      "********************\n",
      "Mean time taken: 6.56200647354126\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 7\n",
      "Jobs Completed:  19\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 9\n",
      "Jobs Completed:  19\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 9\n",
      "Jobs Completed:  19\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 9\n",
      "Jobs Completed:  19\n",
      "********************\n",
      "Mean time taken: 6.558775901794434\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 8\n",
      "Jobs Completed:  20\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 11\n",
      "Jobs Completed:  20\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 12\n",
      "Jobs Completed:  20\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 13\n",
      "Jobs Completed:  20\n",
      "********************\n",
      "Mean time taken: 6.526124477386475\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 13\n",
      "Jobs Completed:  21\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 13\n",
      "Jobs Completed:  21\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 13\n",
      "Jobs Completed:  21\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 14\n",
      "Jobs Completed:  21\n",
      "********************\n",
      "Mean time taken: 6.5207600593566895\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 14\n",
      "Jobs Completed:  22\n",
      "********************\n",
      "********************\n",
      "Jobs in process: 3\n",
      "Jobs waiting: 14\n",
      "Jobs Completed:  22\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "############### MAIN #########################\n",
    "\n",
    "T_exp      = 100        # (in sec) Time for which the exp will run\n",
    "rate       = 0.4         # job arrival rate\n",
    "Nrun       = 30      \n",
    "batch_size = 32\n",
    "fct        = 6\n",
    "\n",
    "job_server = threading.Thread(target=job_arrivals, args=(rate,T_exp, ))\n",
    "job_server.start()\n",
    "\n",
    "job_served     = 0\n",
    "jobs_in_process = []\n",
    "jobs_completed = [] \n",
    "no_jobs_waiting = []\n",
    "\n",
    "t = time.time()\n",
    "while not end_exp_flag[0]:\n",
    "    try: \n",
    "        ## Check if a new job arrived. Will fail if no new job\n",
    "        new_job = job_queue[job_served+1] \n",
    "        \n",
    "        #print(job_served+1)\n",
    "        #print(job_queue)\n",
    "        \n",
    "        if (new_job['entry_time'][0]):\n",
    "            print(\"Something went wrong! New job already has entry time\")\n",
    "        \n",
    "        ## get a resource allocation for new job\n",
    "        split = get_split_algo_2()\n",
    "        \n",
    "        ## If resource is available then a split is returned \n",
    "        ## Else just wait\n",
    "        if split:   \n",
    "            new_job['model_split'] = split\n",
    "            ### Update resources\n",
    "            update_resource(split, 'acquire')\n",
    "            if resource_manager[1]<0 or resource_manager[2]<0:\n",
    "                print(\"Error! There's nothing like negative resources!\")\n",
    "            \n",
    "            ### Spawn a thread to start the new job\n",
    "            new_job['entry_time'][0]  = time.time()\n",
    "            job_submit = threading.Thread(target=run_train, args=(job_served+1, new_job['model_split'], fct, \\\n",
    "                                                                  batch_size, Nrun, new_job['status_flag'], \\\n",
    "                                                                  new_job['exit_time'], ))\n",
    "            job_submit.start()\n",
    "            job_served = job_served+1\n",
    "            jobs_in_process.append(job_served)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    completed = []\n",
    "    for inprocess_job_id in jobs_in_process:\n",
    "        if job_queue[inprocess_job_id]['status_flag'][0]:\n",
    "            completed.append(inprocess_job_id)   \n",
    "            ## Release resources\n",
    "            update_resource(job_queue[inprocess_job_id]['model_split'], 'release')\n",
    "    \n",
    "    jobs_completed = jobs_completed +  completed \n",
    "    for i in completed:\n",
    "        jobs_in_process.remove(i)\n",
    "        \n",
    "    t_now = time.time()\n",
    "    if t_now-t>1:\n",
    "        ## Check for completed jobs \n",
    "        no_waiting = len(job_queue)-job_served\n",
    "        no_jobs_waiting.append(no_waiting)\n",
    "        print('*'*20)\n",
    "        print(\"Jobs in process:\", len(jobs_in_process))\n",
    "        print(\"Jobs waiting:\",no_waiting )\n",
    "        print(\"Jobs Completed: \", len(jobs_completed))\n",
    "        print('*'*20)\n",
    "        t = t_now\n",
    "        \n",
    "\n",
    "    time.sleep(0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1001c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiaUlEQVR4nO3de3Tc513n8fdX97tmbMu2bHnkuI1zcRLHshrSpBAgUEpvgbJ7TnpoKdBdA8vSclug2wMczi4czinLZWHp4qUhObQnXNLQdgu0zabQAm3SWiPbcew4ThzP2JZsSfaM7qPLzLN/zIysy4w0mhlJM7/5vM7xkeb2m+8jS995fs/z/L6POecQERFvqdrqAEREpPiU3EVEPEjJXUTEg5TcRUQ8SMldRMSDajbzzXbs2OH279+/mW8pIlL2+vr6RpxzHet5zaYm9/3793PixInNfEsRkbJnZqH1vkbDMiIiHqTkLiLiQUruIiIepOQuIuJBSu4iIh60ZnI3syfMbMjMzmR47JfNzJnZjo0JT0RE8pFLz/1J4B3L7zSzfcD3A+EixyQiIgVac527c+7rZrY/w0N/APwK8PliByUiUm4uDk/wuZMDkKWM+g/3dHHbjuZNiyevi5jM7L3AVefcKTNb67nHgGMAgUAgn7cTESl5f/LV13i2/yrZUmJPt7+0k7uZNQEfB96ey/Odc8eB4wC9vb3aGUREPKkvHOEHDu3izz7Yu9WhAPmtlnkTcBtwyswuAV1A0Mx2FzMwEZFyMTIxQ+jGFEe7/VsdyoJ199ydcy8BO9O3Uwm+1zk3UsS4RETKRjAUAaAnUDrJPZelkE8D3wTuMLMrZvbhjQ9LRKR89IUj1FYb9+xt3+pQFuSyWub9azy+v2jRiIiUof5QlEN72mmord7qUBboClURkQLMzic4dSVaUuPtoOQuIlKQc4NjzMwnSmq8HZTcRUQK0peeTO32bW0gyyi5i4gUIBiOsKe9gc72xq0OZQkldxGRAgRDEXpKbLwdlNxFRPI2ODrNwGis5MbbQcldRCRvwVAUoORWykCehcNERLwsnnD8/UuDTM/Or/q8L798nfqaKu7qbNukyHKn5C4isszXLwzzkaf7c3ruIwc7qKspvUEQJXcRkWVOXLpJdZXx/C8+smbi3tFSv0lRrY+Su4jIMsFQlLs729i/ifXXi630ziVERLbQfDzByctRegK+rQ6lIEruIiKLvHJtnOm5eEmuXV8PJXcRkUWC4dKrzZ4PJXcRkUWCoQg7W+vp8pdWOYH1UnIXEVmkLxyhJ+DHsu10XSaU3EVEUobGY1y+OV2SV5yul5K7iEhKupxAqZXvzYeSu4hISn84Ql11FYf2lM5eqPnKZYPsJ8xsyMzOLLrvE2b2ipmdNrO/MzPfhkYpIrIJ+kIRDu1tK6m9UPOVS8/9SeAdy+57DrjHOXcf8CrwsSLHJSKyqWbnE5y+OsrRMl8CmbZmcnfOfR24uey+rzjn0uXSXgC6NiA2EZFN8/LAKLPzibK/eCmtGGPuPwn8Y7YHzeyYmZ0wsxPDw8NFeDsRkeILhqNA+V+8lFZQcjezjwPzwGeyPcc5d9w51+uc6+3o6Cjk7URENkwwFGGvr5Hd7Q1bHUpR5F0V0sw+BLwbeNQ554oXkojI5guGI/Tu37bVYRRNXsndzN4B/CrwiHNuqrghiYhsroHoNIOjsbKvBLlYLkshnwa+CdxhZlfM7MPAnwCtwHNmdtLM/vcGxykismHSxcK8cGVq2po9d+fc+zPc/akNiEVEZEsEQ1EaaktzL9R86QpVEal4feEI93X5qK32Tkr0TktERPIQm4tzdmDUM0sg05TcRaSivXR1lLm489R4Oyi5i0iFC4aSk6lHPLRSBpTcRaTC9YUi7N/exI6W+q0OpaiU3EWkYjnnCIajnqkns5iSu4hUrMs3pxmZmPHcZCoouYtIBfPixUtpSu4iUrFOXxmlqa6ag7tatzqUolNyF5GKNTwxw662BqqrbKtDKToldxGpWNGpWXxNtVsdxoZQcheRinVzchZ/U91Wh7EhlNxFpGJFp+aU3EVEvCYyNYtfwzIiIt4xMx9najaOv1k9dxERz4hOzQFoQlVExEsiU7MAGnMXEfGSm5PJ5K6eu4iIh6SHZbZpzF1ExDsqfljGzJ4wsyEzO7Povm1m9pyZXUh99V7VHRHxNE2owpPAO5bd92vA886524HnU7dFRMpGZHKWprpq6muqtzqUDbFmcnfOfR24uezux4CnUt8/BfxQccMSEdlYEQ9fnQr5j7nvcs4NAqS+7sz2RDM7ZmYnzOzE8PBwnm8nIlJcEQ8XDYNNmFB1zh13zvU653o7Ojo2+u1ERHKSLD2gnvty182sEyD1dah4IYmIbLzo1JxnSw9A/sn9C8CHUt9/CPh8ccIREdkcXi4aBrkthXwa+CZwh5ldMbMPA78LfL+ZXQC+P3VbRKQsxBOO0ek5fB4elqlZ6wnOufdneejRIsciIrIpRqfncI7K7rmLiHiN169OBSV3EalA0SlvFw0DJXcRqUCRyWTpAfXcRUQ8JD0s49WKkKDkLiIVyOtFw0DJXUQq0M2pWWqqjJb6NRcMli0ldxGpONGpWXxNdZjZVoeyYbz7sSUiFSV8Y4of/dQLTM/GMzxq/PLbD/L4AwEgOaHq5TXuoOQuIh5x8kqUyzeneez+PSuGW54/N8TnTw7cSu4eLxoGSu4i4hGD0WkA/vsP3UNrw9JeeW31y/z1ty8zH09QU11FdGqO7u1NWxHmptGYu4h4wuBojNb6mhWJHeBIwMf0XJxXro0DyQlVLy+DBCV3EfGIwdFpOn0NGR872p3c5rk/HME5tzCh6mVK7iLiCYOjMXa3N2Z8bK+vkZ2t9fSFIkzOxpmLO89PqCq5i4gnDERj7GnP3HM3M3oCfoLhKJFJ7xcNAyV3EfGAmfk4IxMzdGbpuUNyaCZ8c4rXhiYAb1+dCkruIuIBQ2MzAFnH3AF6un0APP/KdQBPb7EHSu4i4gEDqWWQnVmGZQAO7WmnrrqK588lt3zWmLuISIkbHI0BrDos01BbzaG9bQvP1Zi7iEiJGxhN9tz3rDIsA3A04F/4vr1RPfeszOwXzOxlMztjZk+b2eo/WRGRDXBtNEZ7Yy1NdatfdN+TWu/e1lBDTbW3+7Z5t87M9gIfAXqdc/cA1cDjxQpMRCRXA9HYquPtaemLmbw+mQqF15apARrNbA5oAgYKD0mkfJ2/Ns7H/+4l5uKJFY9VVRn/9Z138Zb923I61hsjk/zel8/ziX9/35o90kz+7bURPvHl8zjn1v3aTPZta+J/Pn6EqqqNKZMbm4vzy397io88ejsHd7UueeyT//w6XzozuHC7p9vPb77n0MLtwdHpnJL7rrYG9voaPX91KhTQc3fOXQV+DwgDg8Coc+4ry59nZsfM7ISZnRgeHs4/UpEy8C8XhjkRitDeVIe/eem/c4NjPBu8kvOxPn/yKn//0iAvvnEzr1j+5sRlLlwfXxFHPv8SDr54epCLIxN5xZKLYCjCF08P8rn+q0vud87xf/7lIjcmZ/E31zE5G+epb1xiYmZ+4TmDozE6fdknUxf7pbcf5Ccf3l/M0EtS3j13M/MDjwG3AVHgb83sA865Ty9+nnPuOHAcoLe3tzhdCJESNRCN0VRXzVM/8ZYVG0H8+F98i75QJOdjpZ8bDEX4njt2rjuWvlCER+7o4E9/9Oi6X7vc68MTPPo/vkZfKMKbd7au/YI8pNu7/Gd06cYUNydn+S8/cC/vfyDA114d5kNPfItTl6M8/OYdxObi3JycpbMttym/9/V0FT32UlTIjML3AW8454adc3PAs8BDxQlLpDxdG0sOD2Ta4acn4OfC0ASj03NrHieRcJwMRwEIhnP/QEgbGotxJTJNz6LVIYU4sKMZX1MtwVC0KMfLJN3O01dGlwxrBVPJPt2W+/f5ltx/Lb0MMseee6UoJLmHgQfNrMmSv8mPAueKE5ZIeUpO7GVOMke7/TgHJy9H1zzOhaEJxmfm2dZcx8lwlHhifSe96USZXh1SqHRtlr48PmhykUg4guEo25rrkqV5B8cXHusLR2itr+H2nS1AcgnjwV0tC7EsLIPMYcy9khQy5v4i8AwQBF5KHet4keISKUurTewd3uejym71OFeTTs4ffLCbydk456+Nr/GK5a+PUlddxaE9bet63Wp6Aj5eG5pgdGrtM4/1ujgyyej0HB98sBtYerYSDEW4P+BbMpHbE/DTH46SSDj13LMoaKGnc+43nXN3Oufucc590Dk3U6zARMrNXDzB0PhM1iTTUl/DHbvbchpm6QtF2NZcx/t69iZvr7PH3BeKcG9XO/U11et63WrSZwHBy8Xvvac/8N5zuJNdbfULP6Px2ByvXh9fWMK4OJbR6TkujkwsXHG6O8cx90rh7VX8IptoaHwG51avb9IT8OU0zBIMR+gJ+Ahsa2JHSz3965iInZmP89LVUXoCvpxfk4vDXckzj/XEkqtgOEJ7Yy0HdrRwtNu/MKl66vIoCceKuYP07WAoykB0Gn9TLY11xfsg8wIld5EiGcyheFVPwM/4zDwXhrIPs0QmZ7k4PElPtz811u1bV8/95YExZucTRZtMTWuur+HO3W0bMu7eF0p+mFVVJcf2r0SmGRqLEQxHMIP7l31QHdjRTHtjLcFwJLkMcpWaMpVKyV2kSAZSwwN7Vhn7TQ8vrLbqpP/y0tUhR7v9hG5MMTKR26jnwuqSIk2mLna025/XBO9qRqfnuDA0sdDeheGfcIS+UISDO1tpW7YvavJDwEdfKJ3cNSSznJK7SJHk0nPv3t7Etua6Vde794UiVFcZh7t8wKJkl+NwSDAcYa+vkV0bMAbd0+3La4J3Nf2pM4H0B9+hPW3UVVdx4lKE/nBkoQ77ilhSS0svjUyuWse9Uim5ixTJ4GiMlvoaWhuyVxtMLynsX2VoIxiKcndn28IY8r1726mtNoKpde+rcc7RF4qsmIAslqOBZOmEfNbeZxMMR6my5GoigPqaau7taudzJwcYi81nHV5Kt3F6Lq5hmQyU3EWKJNf6Jj3dPi6OTHIztZfnYvPxBCcvR5dMhjbUVnP3nvaceu4DozGuj80UfTI1bd+2Rna01OV8FpGLYCjCHbvbaK6/dcF8T8C3MAyVbXgpvbQU1i71W4mU3EWKJNf6Juma4pl6769cG2d6Lr4ioR0N+Dl1JZqxINli6aR7tDu34mTrdWuj6eIk93jCcfJylKPLhl7SvXJfUy0HdjRnfG16ghdgd5t67ssVWhVSpKTEE47/9sWzXB+Lret1Ha31/Ma77y6oxvdANMbdnWtfNHRfl4+aKuMTXz7PM31LC4mlJ2VXLP3r9vHEv73Bf3jqBE2rLPm7MDRBQ20Vd3ZuTP2XZCx+vnL2Oj/1lyeoylBmYbH3Ht7DD97bmfXxV6+PMzGzcuhlYXI14M9YyuFWLD7ODo6p556Bkrt4yrnBMZ78xiW6/I2rJsHFpmbjXIlM876eroW6Jes1O59gZGKG3TkMyzTWVfOj3xHgmxdv8PrwyiqL77q3ky7/0p7o2968g6PdfgZTl9pnU2XwY2/dT+0GbkTx9rt38fenB3ljZHLV5w2Oxrh0Y2rV5L5QJmFZct/Z1sDjb9nH9965esG0H+np4sbELHt1deoKSu7iKelVKH/9U2/N+Q/++liM7/id5+kLRfJO7ukzhT05Tuz91mP3rOv4vqY6PvszpVGX70BHC//359625vP+4LlX+eOvXmBiZp6W+syppi8UYXtzHd3bm1Y89rs/ct+a73Ek4OeTHyi86qUXacxdPCUYjrCrrX5dRaTSGzgUMo48kF4GqeGBBT3dfhIOTq1SKK0/HF24WEuKS8ldPCW9DHC9yaKn21/QCpB0fRNdTHPL/ft8mK2sz552Y2KGN0Ymi34lrSQpuYtnFFLDvCfgY3A0ttADX6902Vmtt76lvbGW23e2ZD0j6k+t29+oNfmVTsldPKOQGuZHF13yno9rozHaGmqWrNWW5M81GIqQyFCuoC8coabKuK+rfQsi8z4ld/GMQmqY39XZRkNtVd47DQ1EY6vWlKlURwJ+xmLzGfdeDYYiHNrTRkOtqjluBCV38YxCapjXVldxX9f6qi8uNjg6ndMyyEqTPiNaPu4+F09w6kqUIxpv3zBK7uIJxahh3hPwc3ZglNhcfN2vVdnZzLLtvfrK4DixuYTG2zeQkrt4QrqGeSHJ4mi3n7m446Wro+t6XWwuzs3JWe3hmUG2vVf7QjcBTaZuJCV38YSFGuYFnOYfSfX617skUnt4ri7T3qvBcJTdbQ2ap9hABSV3M/OZ2TNm9oqZnTOztxYrMJH1CIYjdPkb2VlADfMdLfXs3960aq31TG4tg1TPPZNMe69uZFliSSq05/5HwJecc3cCh4FzhYcksj7pGubFuBgmWfEwinO57zQ0GNUFTKtZvvfq9bEYV6PTC2dKsjHyXpRrZm3AdwE/DuCcmwVWFqgW2WDpGubF6An2dPt5tv8qv/75MzTkuOrmdGqMXhOqmTXX13BXZxtfODXA1Gx84UxHPfeNVcgVFweAYeAvzOww0Ad81Dm3pFScmR0DjgEEAoEC3k4ks9eHkmuo79xdeJnbRw520NFaz98Fr67rdQ/ctm1h5yRZ6T2H9/DHz1/g6W+FATi4q4VDe3Tx0kay9Zx+LnmhWS/wAvCwc+5FM/sjYMw59+vZXtPb2+tOnDiRX6QiWfzDS4P8p88E+dLPf+fC5g0iXmJmfc653vW8ppAx9yvAFefci6nbzwA9BRxPJC8TsXmAVfcuFak0eSd359w14LKZ3ZG661HgbFGiElmHsVhyiV22muEilajQv4afAz5jZnXAReAnCg9JZH0mZpI9dyV3kVsK+mtwzp0E1jUOJFJs47F5muuqqa7Shg8iabpCVcreRGyelgb12kUWU3KXsjcxM6/JVJFllNyl7I3F5jTeLrKMkruUvWTPXcldZDEldyl7EzEld5HllNyl7I3H5jUsI7KMkruUPU2oiqyk5C5lLZ5wTMyo5y6ynJK7lLXJ2XRdGSV3kcWU3KWs3SoapuQuspiSu5S18Vi6rozG3EUWU3KXsjYxk6wIqZ67yFJK7lLWxtI9dyV3kSWU3KWsLYy5a7WMyBJK7lLW0rXctc5dZCkldylr4+ldmDQsI7KEkruUtYnYPGbQVFu91aGIlBQldylr46mrU6u0C5PIEkruUtbGY/OaTBXJoODkbmbVZtZvZl8sRkAi65Es96vJVJHlitFz/yhwrgjHEVm38Zk5TaaKZFDQX4WZdQHvAn4b+MWiRCSrOnN1lKvR6YXbRwI+drY2bMp7xxOOf31thNhcfMVjBjz4pu205diLds7x0tVR7uvyFRTTRGweX1NdQccQ8aJCuzx/CPwK0JrtCWZ2DDgGEAgECny7yjYxM8/7/vQbzMYTC/e9/e5dHP+x3k15/+fOXuOnPx3M+vhPPnwbv/Geu3M61j+fH+Ynnvw2f/NTb+WB27blHdP4zDz7tjXl/XoRr8o7uZvZu4Eh51yfmX13tuc5544DxwF6e3tdvu8ncOpylNl4gt/54Xs5vK+dP/nqa7xw8QbOOcw2frXIt96I0FBbxTM//RDL3+7XP3eGb1+6mfOxXnjjRuqYNwpL7tpiTySjQv4qHgbea2bvBBqANjP7tHPuA8UJTZbrC0Uwg3fd10l7Yy2PHOzgH89c442RSQ50tGz8+4cj3Nfl45697Ssee+hNO/jk115nanaeprq1f62CoUjymKmv+ZrQFnsiGeU9oeqc+5hzrss5tx94HPiqEvvGCoYj3L6zhfbG5Lh2T7c/dX90w987Nhfn7MAoPQF/xsd7un3EE47TV0bXPNbsfGLhef2XoyQS+Z3QzccTTM/FtVpGJAOtcy8TiYQjGIpwtPtWcn1zRwutDTUF935z8dLVUebibsn7L3ZkX/L+XGI5OzjGzHyCRw52EJ2a4+LIZF4xpevKqOcuslJRkrtz7p+dc+8uxrEks4sjE4zF5jmyqOdcVWUcCfjpD298ck8PoxwJ+DI+7m+u40BHc06xpI/1H7/zQPJ2nvGPaxcmkazUcy8T6R7x8p7z0YCf89fHGUsV0NrI99+/vYkdLfVZn3M04CcYjuLc6sMsfeEIe9obeOhN22lvrF1I9uul5C6SnZJ7mQiGoviaajmwo3nJ/T3dPpxLrqTZKM45guFo1vH2W7H4uTk5y6UbU6s+rz8Uoafbnzrz8OXdc781LKMxd5HllNzLRF84Qk/Av2LJ4/37fJgVvupkNZdvTjMyMbMwgZtN+qxitVgGR6cZGI0tfFAcDfh59foEo9PrP/PQFnsi2Sm5l4HRqTleG5qgJ8N4d2tDLXfsat3QFTPpnvVaPff0BO9qPfFgKArc+iBIf2CczOPMY1xb7IlkpeReBoKXU8k1S8+5pzs5qZrvksK19IUiNNdVc8furBciA7cmeFcbQ+8LRaivqeKuzjYADu/zUZXnmce4ttgTyUrJvQz0hyJUGRzOUoelJ+BnPDbPa8MTG/L+wXCE+wM+qnOomd4T8HH++vjCDkmZjnW4y0ddTfJXr6W+hjt2t+W14ufWhKrG3EWWU3IvA33hCHfubqM5Sw81PVyzEePukzPznBscW3NI5lYsfpzLPMwSm4vz8sAoR7p9S+4/2u2jPxwlvs4zj4mZOaqrjIZa/RqLLKfz2RIXTzhOhqO8r6cr63Nu29GMv6mWr50f5t4MpQEKcXZwjITLPiS03P2B5ATv/zt7Hf+yao0XhsaTF0It+6DoCfj59Athnjt7nS5/46rH72xvYHtqOeZEqq7MZtTVESk3Su4l7vy1cSZn4/Qs6+0uZma8Zf82vvTyNb708rWix1BTZRzZl/39F2trqOWu3W089c0QT30ztOLx6ipb8UHxlv3JwmE//em+NY+/19fIv/7q92BmjKuujEhW+ssocemVJ0cDq1dO/O0fvpd/dzR7774Qne2N66qZ/mcfPMq5wbGMj+1qa1hxIdS+bU189mce4sbEzKrH/cbrN3jyG5e4fHOawPamhf1TRWQl/WWUuGAowo6WOvZtW324oqO1nrcf2r1JUa1u37amdddYz1azZvlxn/zGJfrCNwlsb2IiNp/z5iAilUYzUSUumOXipUp0cFcrLfU1C2vltcWeSHZK7iVsZGKGSzemcurVVoLqKuP+fb6FVUGq5S6SnZJ7CetPXXWa60qVStAT8PHKtTEmZ+a1C5PIKpTcS1hfKEJttRV9eWM56+n2k0gVShufmdewjEgWSu4lLBiOcPeedhpqq7c6lJKR3hTkhYs3mJ1PaEJVJAsl9xI1F09w+kp0xQU/la69qZbbd7bwtQsjgHZhEslGyb1EnRscIzaXWPXipUrVE/Bz+koUUHIXyUbJvUQFs+y8JMmfSXqzJ02oimSm5F6i+sJROtsb6Gxf/eKlSrT4bEYTqiKZ5Z3czWyfmf2TmZ0zs5fN7KPFDKzSBVNb0clKB3a00N6YnEht1RZ7IhkV0nOfB37JOXcX8CDws2Z2d3HCqmzXx2JcjU7nXGa30qT3XgUNy4hkk3dyd84NOueCqe/HgXPA3mIFVsnS4+2ZttWTpHQlyXQPXkSWKkq3x8z2A0eAFzM8dgw4BhAIBIrxdp6X3oru0B5dvJTNjz+0n3v2tuNvzr1apUglKXhC1cxagM8CP++cW1Hn1Tl33DnX65zr7ejoKPTtKkIwHOG+rvaFrehkpeb6Gh45qN8nkWwKyh5mVksysX/GOfdscUKqbDPzcc5czX1bOxGRTApZLWPAp4BzzrnfL15Ile3M1TFm4wmOKLmLSAEK6bk/DHwQ+F4zO5n6984ixVWxFiZTdWWqiBQg7wlV59y/AtpBosiC4Qj7tjWys7Vhq0MRkTKmGbsS4pyjLxRRsTARKZiSewm5Gp1maHxGV6aKSMGU3EtI38LFS0ruIlIYJfcS0h+O0lRXzZ27W7c6FBEpc0ruJaQvlLx4qaZa/y0iUhhlkRIxNTvP2cEx1W8XkaJQci8Rp6+MEk84jbeLSFEouZeIYDg5maorU0WkGJTcS0QwFOHAjma2qcqhiBSBknsJcM4RDEe1vl1EikbJvQRcujHFzclZjbeLSNEouZeAdLEwrZQRkWJRci8BfeEIrfU13L6zZatDERGPUHIvAcFQhPsDPqqqVGRTRIpDyX2LjcfmePX6uMbbRaSolNy32KnLoyScxttFpLiU3LdYMBzBDO4P+LY6FBHxECX3LdYXinBwZyttDbVbHYqIeIiS+xZKJBz94Yj2SxWRoisouZvZO8zsvJm9Zma/VqygKsXrwxOMxeY1mSoiRZd3cjezauB/AT8I3A2838zuLlZglSBdLExlB0Sk2GoKeO0DwGvOuYsAZvZXwGPA2WIEttgfP3+BL5waKPZht9zIxAy+ploO7Gje6lBExGMKSe57gcuLbl8BvmP5k8zsGHAMIBAI5PVGHa313L7Le1dv3r6rhbe9uQMzXbwkIsVVSHLPlJHcijucOw4cB+jt7V3xeC4efyDA4w/k98EgIlKJCplQvQLsW3S7C/De2ImISBkqJLl/G7jdzG4zszrgceALxQlLREQKkfewjHNu3sz+M/BloBp4wjn3ctEiExGRvBUy5o5z7h+AfyhSLCIiUiS6QlVExIOU3EVEPEjJXUTEg5TcRUQ8yJzL67qi/N7MbBgI5fnyHcBIEcMpN5XcfrW9clVy+xe3vds517GeF29qci+EmZ1wzvVudRxbpZLbr7ZXZtuhsttfaNs1LCMi4kFK7iIiHlROyf34VgewxSq5/Wp75ark9hfU9rIZcxcRkdyVU89dRERypOQuIuJBZZHcK2kjbjPbZ2b/ZGbnzOxlM/to6v5tZvacmV1IffXsxqtmVm1m/Wb2xdTtSmq7z8yeMbNXUr8Db62U9pvZL6R+58+Y2dNm1uDltpvZE2Y2ZGZnFt2Xtb1m9rFUDjxvZj+w1vFLPrlX4Ebc88AvOefuAh4EfjbV3l8DnnfO3Q48n7rtVR8Fzi26XUlt/yPgS865O4HDJH8Onm+/me0FPgL0OufuIVlG/HG83fYngXcsuy9je1M54HHgUOo1f5rKjVmVfHJn0UbczrlZIL0Rtyc55wadc8HU9+Mk/7j3kmzzU6mnPQX80JYEuMHMrAt4F/Dni+6ulLa3Ad8FfArAOTfrnItSIe0nWYK80cxqgCaSO7t5tu3Oua8DN5fdna29jwF/5Zybcc69AbxGMjdmVQ7JPdNG3Hu3KJZNZWb7gSPAi8Au59wgJD8AgJ1bGNpG+kPgV4DEovsqpe0HgGHgL1LDUn9uZs1UQPudc1eB3wPCwCAw6pz7ChXQ9mWytXfdebAckntOG3F7jZm1AJ8Fft45N7bV8WwGM3s3MOSc69vqWLZIDdADfNI5dwSYxFvDEFmlxpYfA24D9gDNZvaBrY2qpKw7D5ZDcq+4jbjNrJZkYv+Mc+7Z1N3Xzawz9XgnMLRV8W2gh4H3mtklksNv32tmn6Yy2g7J3/UrzrkXU7efIZnsK6H93we84Zwbds7NAc8CD1EZbV8sW3vXnQfLIblX1EbcZmYkx1zPOed+f9FDXwA+lPr+Q8DnNzu2jeac+5hzrss5t5/k//NXnXMfoALaDuCcuwZcNrM7Unc9CpylMtofBh40s6bU38CjJOebKqHti2Vr7xeAx82s3sxuA24HvrXqkZxzJf8PeCfwKvA68PGtjmeD2/o2kqdbp4GTqX/vBLaTnD2/kPq6batj3eCfw3cDX0x9XzFtB+4HTqT+/z8H+Cul/cBvAa8AZ4C/BOq93HbgaZLzC3Mke+YfXq29wMdTOfA88INrHV/lB0REPKgchmVERGSdlNxFRDxIyV1ExIOU3EVEPEjJXUTEg5TcRUQ8SMldRMSD/j/iDG8nrGBK2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(no_jobs_waiting)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9261957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Waiting Time:  15.087110649455678\n",
      "Mean Process Time:  4.372976303100586\n",
      "Mean time taken: 6.6663384437561035\n"
     ]
    }
   ],
   "source": [
    "process_time  = []\n",
    "waiting_time = []\n",
    "for job in job_queue:\n",
    "    if job_queue[job]['status_flag'][0]:\n",
    "        waiting_time.append(job_queue[job]['entry_time'][0] - job_queue[job]['arrival_time'][0])\n",
    "        process_time.append(job_queue[job]['exit_time'][0] - job_queue[job]['entry_time'][0])\n",
    "\n",
    "print(\"Mean Waiting Time: \", np.mean(waiting_time))\n",
    "print(\"Mean Process Time: \", np.mean(process_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ee22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bec966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
