{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca834d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch import optim, nn\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Copy of Inceptionv3, slightly modified for recording intermeridates\n",
    "sys.path.append('/home/cshetty2/sct/pytorch')\n",
    "import reformated_models.inception_modified as inception_modified\n",
    "\n",
    "## Modified Alexnet, with a'factor' by which it can be made 'fat' \n",
    "import dummyModels as dm\n",
    "\n",
    "\n",
    "######## For profiler (some experiments. Not required) #################\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "## Placer libs of baechi\n",
    "sys.path.append('/home/cshetty2/sct')\n",
    "from placer.placer_lib import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## For debug purposes ONLY ########\n",
    "import ctypes, gc\n",
    "import psutil, os\n",
    "\n",
    "### From https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "#########################################\n",
    "\n",
    "def b2mb(x):\n",
    "    return round(x/1024**2,8)\n",
    "\n",
    "def b2gb(x):\n",
    "    return round(x/1024**3,8)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units_bigbrain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0e18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print memory of all available GPU's\n",
    "def print_gpu_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        #print(torch.cuda.get_device_name(i))\n",
    "        print(\"GPU:\", i)\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,8), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,8), 'GB')\n",
    "        #print(\"-----------------\")\n",
    "        #GPUtil.showUtilization()\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca08b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "def b2gb(x): return round(x/2**30,8)\n",
    "class TorchTracemalloc():\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        torch.cuda.reset_max_memory_allocated() # reset the peak gauge to zero\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.end  = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used   = b2gb(self.end-self.begin)\n",
    "        self.peaked = b2gb(self.peak-self.begin)\n",
    "        print(f\"delta used/peak {self.used}/{self.peaked}\")\n",
    "\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451db3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bb8da",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694df43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ParallelModelThreeLayerSplit\"\n",
    "batch_size = 32\n",
    "fct = 6\n",
    "\n",
    "Nrun = 3\n",
    "run_type = \"training\" \n",
    "repetable = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0962f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"ParallelModel\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 0\n",
    "    model = dm.parallelModel(fct).to(single_run_gpu)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelSplit\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 3\n",
    "    model = dm.parallelModelSplit(fct,[single_run_gpu,0], repetable)\n",
    "    opt_size = 512*fct\n",
    "\n",
    "if model_name == \"ParallelModelThreeLayer\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 0\n",
    "    model = dm.parallelModelThreeLayer(fct).to(single_run_gpu)\n",
    "    opt_size = 512*fct\n",
    "    \n",
    "if model_name == \"ParallelModelThreeLayerSplit\":\n",
    "    inp_size_single = (1, 512*fct)\n",
    "    single_run_gpu = 2\n",
    "    model = dm.parallelModelThreeLayerSplit(fct,[single_run_gpu,1], repetable)\n",
    "    opt_size = 512*fct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6481acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (batch_size,) + inp_size_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3d9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"forward\":\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for _ in range(Nrun):\n",
    "            #torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            if repetable == 1:\n",
    "                inp   = torch.ones(inp_size)\n",
    "            else:\n",
    "                inp   = torch.rand(inp_size)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu)\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0); torch.cuda.synchronize(1); torch.cuda.synchronize(2)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6213d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "inp   = torch.ones(inp_size).to(single_run_gpu)\n",
    "output = model(inp)\n",
    "last_gpu = output.get_device()\n",
    "\n",
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    #if 1:\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn((batch_size, opt_size)).to(last_gpu)\n",
    "            start = time.time()\n",
    "            inp = inp.to(single_run_gpu); \n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    prof.export_chrome_trace(\"trace_split2.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3843e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3e31f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.84384155 GB\n",
      "Cached:    1.2890625 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 2.46235704 GB\n",
      "Cached:    3.01757812 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model\n",
    "del inp\n",
    "del output\n",
    "try:\n",
    "    del labels\n",
    "    del optimizer\n",
    "    del loss\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd038674",
   "metadata": {},
   "source": [
    "## Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd37709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "fct = 6\n",
    "\n",
    "Nrun = 30 \n",
    "run_type = \"training\" \n",
    "repetable = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ba3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size_single = (1, 512*fct)\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "opt_size = 512*fct\n",
    "\n",
    "model1 = dm.parallelModelThreeLayerSplit(fct,[1,1], 0)\n",
    "model2 = dm.parallelModelThreeLayerSplit(fct,[2,2], 0)\n",
    "model3 = dm.parallelModelThreeLayerSplit(fct,[2,1], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b521e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken: 86.8112564086914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_gpu1 = 1\n",
    "last_gpu2 = 2\n",
    "last_gpu3 = 2\n",
    "\n",
    "if run_type == \"training\":\n",
    "\n",
    "    optimizer1 = optim.SGD(model1.parameters(), lr = 0.0001); \n",
    "    optimizer2 = optim.SGD(model2.parameters(), lr = 0.0001); \n",
    "    optimizer3 = optim.SGD(model3.parameters(), lr = 0.0001); \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            \n",
    "            labels1 = torch.randn((batch_size, opt_size)).to(last_gpu1)\n",
    "            labels2 = torch.randn((batch_size, opt_size)).to(last_gpu2)\n",
    "            labels3 = torch.randn((batch_size, opt_size)).to(last_gpu3)\n",
    "            \n",
    "            start = time.time()\n",
    "            optimizer1.zero_grad();optimizer2.zero_grad();optimizer3.zero_grad()\n",
    "            \n",
    "            output1 = model1(inp)\n",
    "            output2 = model2(inp)\n",
    "            output3 = model3(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss1 = criterion(output1, labels1 )\n",
    "            loss2 = criterion(output2, labels2 )\n",
    "            loss3 = criterion(output3, labels3 )\n",
    "            ##################################################################################\n",
    "            loss1.backward(loss1)\n",
    "            loss2.backward(loss2)\n",
    "            loss3.backward(loss3)\n",
    "            \n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db761b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fdaed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 4.14967394 GB\n",
      "Cached:    4.703125 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 5.76782322 GB\n",
      "Cached:    6.32421875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "del model1, model2, model3\n",
    "del inp\n",
    "del output1, output2, output3\n",
    "try:\n",
    "    del labels1, labels2, labels3\n",
    "    del optimizer1, optimizer2, optimizer3\n",
    "    del loss1, loss2, loss3\n",
    "except: pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e722127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93834e3d",
   "metadata": {},
   "source": [
    "# With Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1243df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2cc233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model_split, fct, batch_size, Nrun, done_flag ): \n",
    "    inp_size_single = (1, 512*fct)\n",
    "    model = dm.parallelModelThreeLayerSplit(fct,model_split, 0)\n",
    "    \n",
    "    inp_size = (batch_size,) + inp_size_single\n",
    "    inp   = torch.ones(inp_size)\n",
    "    output = model(inp)\n",
    "    last_gpu = output.get_device()\n",
    "    opt_size = tuple(output.size())[1]\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.0001); \n",
    "    criterion = nn.MSELoss()\n",
    "    dataset = torchvision.datasets.FakeData(\n",
    "        size= Nrun * batch_size,\n",
    "        image_size=inp_size_single,\n",
    "        num_classes=opt_size,\n",
    "        transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "\n",
    "    times = []\n",
    "    if 1:\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        for batch_idx, (inp, oup) in enumerate(data_loader):\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            labels = torch.randn((batch_size, opt_size)).to(last_gpu)\n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inp)\n",
    "            #torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            ######################### loss compute ################################################\n",
    "            loss = criterion(output, labels )\n",
    "            ##################################################################################\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize(0);torch.cuda.synchronize(1);torch.cuda.synchronize(2);torch.cuda.synchronize(3)\n",
    "            end = time.time()\n",
    "            times.append(1000*(end-start))\n",
    "    #prof.export_chrome_trace(\"trace_singlegpu.json\")\n",
    "    gpu_time = np.mean(times[10:])\n",
    "    print(\"Mean time taken:\", gpu_time)\n",
    "    print()\n",
    "\n",
    "    del model\n",
    "    del inp\n",
    "    del output\n",
    "    try:\n",
    "        del labels\n",
    "        del optimizer\n",
    "        del loss\n",
    "    except: pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "    done_flag[0] = 1\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6118d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "fct = 6\n",
    "Nrun = 50 \n",
    "\n",
    "done_flag1 = [0]\n",
    "done_flag2 = [0]\n",
    "done_flag3 = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73851536",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = threading.Thread(target=run_train, args=([1,1], fct, batch_size, Nrun,done_flag1,))\n",
    "run2 = threading.Thread(target=run_train, args=([2,2], fct, batch_size, Nrun,done_flag2,))\n",
    "run3 = threading.Thread(target=run_train, args=([1,2], fct, batch_size, Nrun,done_flag3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce499f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1.start(); time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3cb5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.start(); time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25980c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbbba2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "[0] [0] [0]\n",
      "Mean time taken: 73.59336614608765\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.46528673 GB\n",
      "Cached:    2.59375 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 4.15443468 GB\n",
      "Cached:    4.296875 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "[1] [0] [0]\n",
      "Mean time taken: 72.16576337814331\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 2.68098497 GB\n",
      "Cached:    2.8046875 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.98812866 GB\n",
      "Cached:    1.13085938 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "Mean time taken: 65.5152976512909\n",
      "\n",
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-43f56d31c917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_flag1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_flag2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_flag3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    print(done_flag1, done_flag2, done_flag3)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4668f1",
   "metadata": {},
   "source": [
    "## Memory measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "fct = 6\n",
    "\n",
    "Nrun = 3\n",
    "run_type = \"training\" \n",
    "repetable = 0\n",
    "\n",
    "\n",
    "inp_size_single = (1, 512*fct)\n",
    "single_run_gpu = 0\n",
    "model = dm.parallelModelThreeLayer(fct).to(single_run_gpu)\n",
    "opt_size = 512*fct\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "out_size = (batch_size,opt_size) \n",
    "\n",
    "inp   = torch.rand(inp_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TorchTracemalloc() as tt:\n",
    "    with torch.no_grad():\n",
    "    #if 1:\n",
    "        inp = inp.to(single_run_gpu)\n",
    "        out =  model(inp)\n",
    "print(tt.used)\n",
    "print(tt.peaked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del inp\n",
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b55e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units_bigbrain import *\n",
    "\n",
    "batch_size = 128\n",
    "fct = 1\n",
    "\n",
    "Nrun = 3\n",
    "run_type = \"training\" \n",
    "repetable = 0\n",
    "\n",
    "\n",
    "inp_size_single = (1, 512*fct)\n",
    "single_run_gpu = 0\n",
    "model = dm.parallelModelThreeLayer(fct)\n",
    "opt_size = 512*fct\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "out_size = (batch_size,opt_size) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec06fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Profiling(model,batch_size, single_run_gpu, 40, input_size = inp_size_single)\n",
    "final_output = tester.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a39037",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_res = 0\n",
    "for node_id in tester.sub_module_nodes:\n",
    "    node = tester.sub_module_nodes[node_id]\n",
    "    print(node.input_memory)\n",
    "    print(node.persistent_memory)\n",
    "    print(node.temporary_memory)\n",
    "    \n",
    "    #curr_res_usage =  node.persistent_memory + node.temporary_memory \\\n",
    "    #                + node.output_memory \n",
    "    \n",
    "    curr_res_usage =  node.persistent_memory\n",
    "    net_res += curr_res_usage\n",
    "    print(\"layer:\", node.module)\n",
    "    print(\"resource:\", curr_res_usage)\n",
    "    print('-'*20)\n",
    "print(net_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9765cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665b26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "fct = 1\n",
    "single_run_gpu = 0\n",
    "model = dm.parallelModelThreeLayer(fct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_model_size(model, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk   = torch.rand((1,1)).to(single_run_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dm.parallelModelThreeLayer(1).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1147-1085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa90fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = dm.parallelModelThreeLayer(fct).to(single_run_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "1189-1147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baechi_units_bigbrain import *\n",
    "\n",
    "batch_size = 32\n",
    "fct = 6\n",
    "\n",
    "Nrun = 3\n",
    "run_type = \"training\" \n",
    "repetable = 0\n",
    "\n",
    "\n",
    "inp_size_single = (1, 512*fct)\n",
    "single_run_gpu = 0\n",
    "\n",
    "opt_size = 512*fct\n",
    "inp_size = (batch_size,) + inp_size_single\n",
    "out_size = (batch_size,opt_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d70775",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "#with TorchTracemalloc() as tt:\n",
    "    model = dm.parallelModelThreeLayer(fct).to(single_run_gpu)\n",
    "    inp   = torch.rand(inp_size).to(single_run_gpu)\n",
    "    labels = torch.randn(out_size).to(single_run_gpu)\n",
    "\n",
    "    inp.requires_grad = True\n",
    "    #optimizer = optim.SGD(model.parameters(), lr = 0.0001); optimizer.zero_grad()\n",
    "    #criterion = nn.MSELoss()\n",
    "\n",
    "    output = model(inp)\n",
    "    #loss = criterion(output, labels)\n",
    "    #loss.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "163-85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83525909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8727ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3976e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baechiTest_dummyModels as dm\n",
    "factor = 1\n",
    "inp_size_single = (1, int(512*factor))\n",
    "model = dm.parallelThreeLayer(factor, 1)\n",
    "opt_size = 512*factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_original_methods={}\n",
    "def recur_function(module):\n",
    "    sub_modules = module.__dict__['_modules']\n",
    "    for name, sub_module in sub_modules.items():\n",
    "        # sub modules of sub_module, if there are more than 1, we need further recursion\n",
    "        sub_sub_modules = sub_module.__dict__['_modules']\n",
    "        if len(sub_sub_modules) > 0:\n",
    "            recur_function(sub_module)\n",
    "            continue\n",
    "\n",
    "        def _calculate_time_and_memory(function, *input):\n",
    "            with TorchTracemalloc() as tt:\n",
    "                torch.cuda.synchronize(0)\n",
    "                start_time = time.time()\n",
    "                result = function(*input)\n",
    "                torch.cuda.synchronize(0)\n",
    "                stop_time = time.time()\n",
    "            return (stop_time - start_time) * 1000, tt.used, tt.peaked , result\n",
    "\n",
    "        def forward_wrapper(cur_module, *input):\n",
    "            \"\"\"\n",
    "            use this wrapper to replace the original forward function in submodules\n",
    "            :param cur_module: the input submodule\n",
    "            \"\"\"\n",
    "\n",
    "            ## collect relevant information of cur module\n",
    "            function = forward_original_methods[cur_module]\n",
    "            forward_time, used_mem, peak_mem, result = _calculate_time_and_memory(function, *input)\n",
    "\n",
    "            ## Input size in bytes\n",
    "            input_size = 0\n",
    "            for inp in input:\n",
    "                input_size = input_size + estimate_tensor_size(inp, 'B')\n",
    "\n",
    "            ## Model size in bytes\n",
    "            persistent_memory = estimate_model_size(cur_module,'B', False)\n",
    "\n",
    "            output_memory = estimate_tensor_size(result, 'B')\n",
    "            \n",
    "            temporary_memory = peak_mem - used_mem\n",
    "            \n",
    "            print(\"Module:\", cur_module)\n",
    "            print(\"Input memory:\", b2mb(input_size))\n",
    "            print(\"Persistent memory:\", b2mb(persistent_memory) )\n",
    "            print(\"Temporary memory:\", b2mb(temporary_memory) )\n",
    "            print(\"*\"*20)\n",
    "\n",
    "            return result\n",
    "        if sub_module in forward_original_methods:\n",
    "                # only record the original forward functions once\n",
    "                continue\n",
    "\n",
    "        forward_original_methods[sub_module] = sub_module.forward\n",
    "        sub_module.forward = forward_wrapper.__get__(sub_module, sub_module.__class__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f945a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_function(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "inp_size = (batch_size,) +  inp_size_single\n",
    "inp = torch.randn(inp_size)* (0.000001); \n",
    "\n",
    "output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b81c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = dm.parallelThreeLayer(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16469a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = model5.to(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1111-1085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "1135-1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "1159-1135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcf62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4902c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import baechiTest_dummyModels as dm\n",
    "factor = 1\n",
    "inp_size_single = (1, int(512*factor))\n",
    "model = dm.parallelThreeLayer(factor, 1)\n",
    "opt_size = 512*factor\n",
    "batch_size = 32\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018188a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "return_graph, tester = build_graph(model, batch_size,args.prof_gpu_id, args.prof_rounds, inp_size = inp_size_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39680d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.04692078 GB\n",
      "Cached:    0.0703125 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a81ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 09:24:32,096 - m_sct_v1:157 - INFO - Start LP solver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer started.\n",
      "Presolve started.\n",
      "Linear dependency checker started.\n",
      "Linear dependency checker terminated.\n",
      "Eliminator started.\n",
      "Freed constraints in eliminator : 5\n",
      "Eliminator terminated.\n",
      "Eliminator - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
      "Lin. dep.  - number                 : 0               \n",
      "Presolve terminated. Time: 0.01    \n",
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 78              \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 24              \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 0               \n",
      "\n",
      "Optimizer  - threads                : 16              \n",
      "Optimizer  - solved problem         : the dual        \n",
      "Optimizer  - Constraints            : 11\n",
      "Optimizer  - Cones                  : 0\n",
      "Optimizer  - Scalar variables       : 22                conic                  : 0               \n",
      "Optimizer  - Semi-definite variables: 0                 scalarized             : 0               \n",
      "Factor     - setup time             : 0.00              dense det. time        : 0.00            \n",
      "Factor     - ML order time          : 0.00              GP order time          : 0.00            \n",
      "Factor     - nonzeros before factor : 35                after factor           : 42              \n",
      "Factor     - dense dim.             : 0                 flops                  : 5.88e+02        \n",
      "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
      "0   1.3e+00  3.0e+00  1.5e+00  1.00e+00   1.448988914e-01   -3.264117048e-01  1.0e+00  0.01  \n",
      "1   3.8e+00  7.5e-01  1.6e+00  0.00e+00   3.899563910e-01   3.440407249e-01   3.8e+00  0.02  \n",
      "2   1.5e+00  3.0e-01  6.4e-01  4.71e-01   5.038537369e-01   4.899302769e-01   1.5e+00  0.03  \n",
      "3   5.6e-02  1.1e-02  2.3e-02  9.20e-01   6.006133930e-01   6.000093282e-01   5.5e-02  0.03  \n",
      "4   1.6e-04  3.2e-05  6.8e-05  9.92e-01   6.015958710e-01   6.015905867e-01   1.6e-04  0.03  \n",
      "5   1.6e-08  3.2e-09  6.8e-09  1.00e+00   6.016082529e-01   6.016082524e-01   1.6e-08  0.03  \n",
      "Basis identification started.\n",
      "Basis identification terminated. Time: 0.00\n",
      "Optimizer terminated. Time: 0.05    \n",
      "\n",
      "\n",
      "Interior-point solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 6.0160825293e-01    nrm: 2e+00    Viol.  con: 2e-09    var: 0e+00  \n",
      "  Dual.    obj: 6.0160825140e-01    nrm: 1e+00    Viol.  con: 2e-16    var: 3e-10  \n",
      "\n",
      "Basic solution summary\n",
      "  Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "  Solution status : OPTIMAL\n",
      "  Primal.  obj: 6.0160825293e-01    nrm: 2e+00    Viol.  con: 3e-09    var: 0e+00  \n",
      "  Dual.    obj: 6.0160825417e-01    nrm: 1e+00    Viol.  con: 3e-18    var: 6e-17  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 09:24:32,171 - m_sct_v1:162 - INFO - LP solver finished. Relaxed makespan soultion: 0.601608\n",
      "2021-12-15 09:24:32,173 - m_sct_v1:140 - INFO - Favorite child round threshold: 0.5\n",
      "2021-12-15 09:24:32,175 - m_sct:143 - INFO - # favorite child: 7\n",
      "2021-12-15 09:24:32,178 - m_sct:144 - INFO - # favorite child changes: 0\n",
      "2021-12-15 09:24:32,184 - m_sct:172 - INFO - SCT estimated runtime: 0.000001\n"
     ]
    }
   ],
   "source": [
    "placed_op_graph = m_sct(return_graph, DEVICE_GRAPH_MULTIPLE)\n",
    "copy_p(return_graph, tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0895ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.04692078 GB\n",
      "Cached:    0.0703125 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108025fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del return_graph\n",
    "del placed_op_graph\n",
    "del tester\n",
    "gc.collect()              ## To clean any circular references\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c408f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 1\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 2\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n",
      "GPU: 3\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8696a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cshetty2/anaconda3/envs/baechi/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:151: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "source": [
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5650e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baechiTest_dummyModels as dm\n",
    "factor = 6\n",
    "inp_size_single = (1, int(512*factor))\n",
    "model = dm.tallParallelModel(factor)\n",
    "opt_size = 512*factor\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63518fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_graph, tester = build_graph(model, batch_size,args.prof_gpu_id, args.prof_rounds, inp_size = inp_size_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac04cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
