\begin{abstract}

With the rising popularity of large scale Deep Learning (DL) workload, GPU
clusters have become a necessity. Yet, the tools to manage them are far from
ideal. On one hand the deep learning engineers are burdened with task of
estimating resource management usage of their models and routinely face
Out-of-memory (OOM) errors. On the other hand, typical GPU cluster utilization
are low, often below 30\%. But with recent developments in GPU virtualization
and drawing from our learnings building Beachi - a fast model splitting system -
we explore ways to improve GPU cluster management for DL. Model Parallelism (MP)
techniques have so far been seen as a need, when the models are too big.
However, we (plan to) demonstrate MP as an essential tool in building efficient
DL workflows.

\end{abstract}
